{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4766ebb-432c-4bdb-8050-23df797098be",
   "metadata": {},
   "source": [
    "<!-- # Langchain은 다양한 LLM(대규모 언어 모델)을 지원한다\n",
    "-\t대규모 언어 모델(LLM, Large Language Model)을 개발하는 회사들은 사용자가 자신의 애플리케이션에서 LLM을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "-\t하지만 각 LLM은 고유한 API 호출 라이브러리(Library)를 제공하기 때문에, 개발자는 동일한 작업을 수행하더라도 LLM에 따라 다른 코드를 작성해야 하는 번거로움이 있다.\n",
    "-\tLangchain은 이러한 문제를 해결하기 위해 다양한 LLM의 API를 통합적으로 지원한다.\n",
    "-\t여러 LLM을 동일한 인터페이스(interface)로 호출할 수 있게 하여 특정 모델에 종속되지 않도록 하고, 필요에 따라 쉽게 다른 모델로 전환할 수 있다.\n",
    "-\tLangchain이 지원하는 주요 LLM 목록\n",
    "    - https://python.langchain.com/docs/integrations/chat/#featured-providers -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3b65a-1b4e-4fcc-99ac-ec7624dac9ad",
   "metadata": {},
   "source": [
    "## 설치\n",
    "```bash\n",
    "pip install langchain langchain-community  -qU\n",
    "pip install python-dotenv -qU \n",
    "pip install ipywidgets -qU\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9e5829-972a-4345-858d-e479dab7c320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.25'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9025ef1-ede0-4f3b-b8fe-1b9e348391ba",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "\n",
    "2. Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다.\n",
    "\n",
    "## 사용량 확인\n",
    "- profile/설정 -> Usage 에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f99e0-d15b-4769-8d2c-b6e0a9a26237",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 로그인 -> Dashboard -> API Keys -> Create New Secreat Key\n",
    "> Settings -> API Keys\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 값을 읽은 뒤 환경변수로 등록한다.\n",
    "- **주의**\n",
    "  - 생성된 API Key는 노출되면 안된다.\n",
    "  - API Key가 저장된 파일(코드나 설정파일)이 github에 올라가 공개되서는 안된다.\n",
    "\n",
    "## 사용 비용 확인\n",
    "- settings -> Usage 에서 확인\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  OpenAI LLM 모델: https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://platform.openai.com/docs/pricing\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a12d-8e7d-4a29-ac91-edb62c56bfe7",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-openai -qU\n",
    "```\n",
    "\n",
    "- OpenAI 자체 라이브러리 설치\n",
    "    - `pip install openai -qU`\n",
    "    - langchain-openai를 설치하면 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29772e79-1e31-421d-b5c1-625dc029c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486e5a1-6c59-499d-a275-41060c8b0a8a",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9bb769-eb14-43d5-9c26-98bb77153b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "flag = load_dotenv()\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7711ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat.errors import messages\n",
    "\n",
    "\n",
    "client = OpenAI()\t# 환경 변수에 OPENAI_API_KEY 이름으로 등록되어있으면 apikey는 생략!\n",
    "message = [\n",
    "    {\n",
    "        \"role\" : \"user\",\t# 누가 말하고 있는지 (발화자 - user, assistant, system)\n",
    "        \"content\" : \"OpenAI의 LLM 모델은 무엇이 있나용 ??\"\t# 질의 내용 -> 프롬포트\n",
    "\t}\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\t# 요청할 gpt 모델 종류\n",
    "    messages = message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "479fe830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Betl9MydYDzsGGAsmzk5SRTCKIlY3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI의 LLM(대규모 언어 모델)으로는 다음과 같은 모델들이 있습니다:\\n\\n1. **GPT-3**: 1750억 개의 매개변수를 가진 모델로, 다양한 자연어 처리 작업을 수행할 수 있습니다. 텍스트 생성, 번역, 질문 응답 등 여러 응용 분야에서 활용됩니다.\\n\\n2. **GPT-3.5**: GPT-3의 개선된 버전으로, 문맥 이해 능력과 생성 품질이 향상되었습니다. 문장 생성, 대화형 AI 등에서 더욱 효과적인 성능을 보입니다.\\n\\n3. **GPT-4**: GPT-4는 GPT-3.5의 후속 모델로, 더욱 향상된 이해력과 생성 능력을 제공하며, 다양한 형식의 데이터를 처리할 수 있는 기능이 강화되었습니다.\\n\\n이 외에도 OpenAI는 특정 용도나 응용 프로그램에 맞춘 다양한 모델을 연구하고 개발하고 있으며, 지속적으로 발전하고 있습니다. OpenAI의 모델들은 API 형태로 제공되어 다양한 어플리케이션에서 사용될 수 있습니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749087159, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_34a54ae93c', usage=CompletionUsage(completion_tokens=231, prompt_tokens=20, total_tokens=251, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "851be9d2-58e5-464b-87cb-52f09a9a146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대규모 언어 모델)으로는 다음과 같은 모델들이 있습니다:\n",
      "\n",
      "1. **GPT-3**: 1750억 개의 매개변수를 가진 모델로, 다양한 자연어 처리 작업을 수행할 수 있습니다. 텍스트 생성, 번역, 질문 응답 등 여러 응용 분야에서 활용됩니다.\n",
      "\n",
      "2. **GPT-3.5**: GPT-3의 개선된 버전으로, 문맥 이해 능력과 생성 품질이 향상되었습니다. 문장 생성, 대화형 AI 등에서 더욱 효과적인 성능을 보입니다.\n",
      "\n",
      "3. **GPT-4**: GPT-4는 GPT-3.5의 후속 모델로, 더욱 향상된 이해력과 생성 능력을 제공하며, 다양한 형식의 데이터를 처리할 수 있는 기능이 강화되었습니다.\n",
      "\n",
      "이 외에도 OpenAI는 특정 용도나 응용 프로그램에 맞춘 다양한 모델을 연구하고 개발하고 있으며, 지속적으로 발전하고 있습니다. OpenAI의 모델들은 API 형태로 제공되어 다양한 어플리케이션에서 사용될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c7056dd-ddc5-4c0f-8653-47be64287d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI의 LLM(대규모 언어 모델)으로는 다음과 같은 모델들이 있습니다:\n",
       "\n",
       "1. **GPT-3**: 1750억 개의 매개변수를 가진 모델로, 다양한 자연어 처리 작업을 수행할 수 있습니다. 텍스트 생성, 번역, 질문 응답 등 여러 응용 분야에서 활용됩니다.\n",
       "\n",
       "2. **GPT-3.5**: GPT-3의 개선된 버전으로, 문맥 이해 능력과 생성 품질이 향상되었습니다. 문장 생성, 대화형 AI 등에서 더욱 효과적인 성능을 보입니다.\n",
       "\n",
       "3. **GPT-4**: GPT-4는 GPT-3.5의 후속 모델로, 더욱 향상된 이해력과 생성 능력을 제공하며, 다양한 형식의 데이터를 처리할 수 있는 기능이 강화되었습니다.\n",
       "\n",
       "이 외에도 OpenAI는 특정 용도나 응용 프로그램에 맞춘 다양한 모델을 연구하고 개발하고 있으며, 지속적으로 발전하고 있습니다. OpenAI의 모델들은 API 형태로 제공되어 다양한 어플리케이션에서 사용될 수 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77416bf-be44-421b-bed1-c1da8512e983",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "> - **OpenAI**\n",
    ">     - 문장 완성 모델. (text completion) model\n",
    ">     - Default로 gpt-3.5-turbo-instruct 사용\n",
    ">       - instruct 모델만 사용가능\n",
    ">     - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124af8a3-d624-4399-8062-35a6d60c63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1948856-b1e4-4da0-9311-11f798b66a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "response = model.invoke(\"OpenAI LLM 모델 종류를 알려줘잉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afed81fb-5249-4bd3-91b0-4f86a0a75f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI는 다양한 대규모 언어 모델(LLM)을 개발해왔으며, 그 중 몇 가지 주요 모델은 다음과 같습니다:\\n\\n1. **GPT-2**: 2019년에 발표된 모델로, 대화, 텍스트 생성, 번역 등 다양한 작업을 수행할 수 있는 자연어 처리 모델입니다.\\n\\n2. **GPT-3**: 2020년에 출시된 모델로, 1750억 개의 매개변수를 가지고 있으며, 텍스트 생성, 질문 응답, 요약 등 다양한 작업에서 뛰어난 성능을 보여주었습니다.\\n\\n3. **GPT-3.5**: GPT-3의 후속 버전으로, 여러 개선된 점들이 포함되어 다양한 리소스를 활용하여 더 나은 성능을 발휘합니다.\\n\\n4. **GPT-4**: 2023년에 출시된 최신 모델로, 이전 버전보다 더 향상된 이해력과 생성 능력을 가지고 있습니다. 다양한 분야에서 활용될 수 있도록 다방면으로 개선되었습니다.\\n\\n이 외에도 OpenAI는 특정 용도에 맞춘 다양한 모델과 API를 개발해왔으며, 사용자 요구에 맞춘 다양한 기능을 제공하고 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 251, 'prompt_tokens': 18, 'total_tokens': 269, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BetuJuUOI9SH3m5cRywYc0FPSxQyx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--08e416a4-6f6c-4dd8-b643-87a98a185d32-0', usage_metadata={'input_tokens': 18, 'output_tokens': 251, 'total_tokens': 269, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fadc486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI는 다양한 대규모 언어 모델(LLM)을 개발해왔으며, 그 중 몇 가지 주요 모델은 다음과 같습니다:\n",
      "\n",
      "1. **GPT-2**: 2019년에 발표된 모델로, 대화, 텍스트 생성, 번역 등 다양한 작업을 수행할 수 있는 자연어 처리 모델입니다.\n",
      "\n",
      "2. **GPT-3**: 2020년에 출시된 모델로, 1750억 개의 매개변수를 가지고 있으며, 텍스트 생성, 질문 응답, 요약 등 다양한 작업에서 뛰어난 성능을 보여주었습니다.\n",
      "\n",
      "3. **GPT-3.5**: GPT-3의 후속 버전으로, 여러 개선된 점들이 포함되어 다양한 리소스를 활용하여 더 나은 성능을 발휘합니다.\n",
      "\n",
      "4. **GPT-4**: 2023년에 출시된 최신 모델로, 이전 버전보다 더 향상된 이해력과 생성 능력을 가지고 있습니다. 다양한 분야에서 활용될 수 있도록 다방면으로 개선되었습니다.\n",
      "\n",
      "이 외에도 OpenAI는 특정 용도에 맞춘 다양한 모델과 API를 개발해왔으며, 사용자 요구에 맞춘 다양한 기능을 제공하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd3c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='저는 이름이 없지만, 여러분이 원하는 정보를 제공하기 위해 여기 있습니다! 햄스터 이름을 정하는 데 도움을 줄 수 있어요. 어떤 종류의 이름을 원하세요? 귀엽고 사랑스러운 이름, 혹은 독특하고 멋진 이름 중에 어떤 게 좋으신가요? 몇 가지 예시를 드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 27, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BetxKWVRs0EB9LlzCilvMdcVKatdU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--db05618e-ae6f-4f41-ac19-aa43c8195771-0', usage_metadata={'input_tokens': 27, 'output_tokens': 77, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"너 이름이 뭐니 ? 내 햄부기 정해줄 수 있겐니 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83999895",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\t# gpt-4.1-mini, gpt-4.1\n",
    "    temperature = 0,\t\t# 무작위성, 0 이상 실수 설정 (0~1), 클수록 무작위성이 커짐\n",
    "    # max_token = 100\t\t\t# 응답 토큰 수 제한\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66855f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들의 이름을 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "<답변 형식>\n",
    "목록 형식으로 답변해줘.\n",
    "- 한글이름(영어이름) : 행성에 대한 간단한 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53b56d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury) : 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "- 금성(Venus) : 지구와 비슷한 크기를 가진 행성이지만, 두꺼운 대기와 강한 온실 효과로 매우 높은 온도를 유지합니다.\n",
      "- 지구(Earth) : 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 갖추고 있습니다.\n",
      "- 화성(Mars) : 붉은색의 표면을 가진 행성으로, 과거에 물이 존재했을 가능성이 있는 곳입니다.\n",
      "- 목성(Jupiter) : 태양계에서 가장 큰 행성으로, 강력한 자기장과 많은 위성을 가지고 있습니다.\n",
      "- 토성(Saturn) : 아름다운 고리로 유명한 행성으로, 가스 행성 중 하나입니다.\n",
      "- 천왕성(Uranus) : 독특하게 옆으로 기울어진 자전축을 가진 행성으로, 푸른색의 대기를 가지고 있습니다.\n",
      "- 해왕성(Neptune) : 태양계에서 가장 먼 행성으로, 강한 바람과 푸른색의 대기를 특징으로 합니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c4590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury) : 태양에서 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "- 금성(Venus) : 두 번째로 태양에 가까운 행성으로, 두꺼운 대기로 인해 온실효과가 심해 고온 환경을 가지고 있습니다.\n",
      "- 지구(Earth) : 생명체가 존재하는 유일한 행성으로, 대기와 물이 존재하며 다양한 기후를 가지고 있습니다.\n",
      "- 화성(Mars) : 붉은색 표면을 가진 '붉은 행성'으로, 과거에 물이 존재했을 가능성이 있는 행성입니다.\n",
      "- 목성(Jupiter) : 태양계에서 가장 큰 행성으로, 거대한 기체 행성이며 많은 위성을 가지고 있습니다.\n",
      "- 토성(Saturn) : 아름다운 고리로 유명한 행성으로, 목성처럼 기체로 이루어져 있습니다.\n",
      "- 천왕성(Uranus) : 측면으로 기울어져 자전하는 특이한 행성으로, 푸른색의 메탄 대기를 가지고 있습니다.\n",
      "- 해왕성(Neptune) : 태양계의 가장 바깥쪽에 위치한 행성으로, 강한 바람과 푸른색 대기를 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)\t# temperature : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cbd0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 형태 프롬포트 - role과 내용을 묶어서 전달.\n",
    "prompt = [\n",
    "    (\"user\", \"막걸리 제조법을 알려줘\")\t# (\"role\", \"대화내용\")\n",
    "]\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5431a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='막걸리는 한국의 전통 발효주로, 쌀과 물, 누룩을 주재료로 사용하여 만듭니다. 아래는 기본적인 막걸리 제조법입니다.\\n\\n### 재료\\n- 쌀: 1kg (찹쌀 또는 일반 쌀)\\n- 물: 1.5~2리터\\n- 누룩: 100g (막걸리용 누룩)\\n- 설탕: 선택 사항 (단맛을 원할 경우)\\n\\n### 제조 과정\\n\\n1. **쌀 씻기**: 쌀을 깨끗이 씻어 불순물을 제거합니다. 여러 번 물을 갈아주며 씻어주세요.\\n\\n2. **쌀 불리기**: 씻은 쌀을 물에 담가 4~6시간 정도 불립니다. 찹쌀을 사용할 경우, 더 부드럽고 쫄깃한 막걸리가 됩니다.\\n\\n3. **쌀 찌기**: 불린 쌀을 체에 받쳐 물기를 빼고, 찜통에 넣어 30~40분 정도 쪄줍니다. 쌀이 고루 익도록 중간에 한 번 저어주는 것이 좋습니다.\\n\\n4. **식히기**: 찐 쌀을 넓은 그릇에 옮겨 담고, 상온에서 식힙니다. 온도가 30도 정도로 식어야 합니다.\\n\\n5. **누룩 섞기**: 식은 쌀에 누룩을 고루 섞어줍니다. 누룩이 잘 섞이도록 손으로 부드럽게 비벼주세요.\\n\\n6. **발효**: 혼합한 쌀과 누룩을 깨끗한 발효 용기에 담고, 물을 추가합니다. 뚜껑을 덮되, 완전히 밀봉하지 않고 공기가 통할 수 있도록 합니다. 1~2일 동안 실온에서 발효시킵니다. 발효가 진행되면 거품이 생기고, 향이 나기 시작합니다.\\n\\n7. **여과**: 발효가 끝나면, 체나 면포를 이용해 막걸리를 여과합니다. 이때, 남은 찌꺼기는 버리거나 다른 요리에 활용할 수 있습니다.\\n\\n8. **병입 및 저장**: 여과한 막걸리를 깨끗한 병에 담고, 냉장고에 보관합니다. 이때, 설탕을 추가하여 단맛을 조절할 수 있습니다.\\n\\n9. **숙성**: 막걸리는 냉장고에서 며칠 숙성시키면 더욱 맛이 좋아집니다. \\n\\n이제 막걸리를 즐길 준비가 완료되었습니다! 막걸리는 차갑게 해서 마시는 것이 일반적이며, 다양한 안주와 함께 즐기면 좋습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 619, 'prompt_tokens': 15, 'total_tokens': 634, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BeuXQnob2zaffitkt1uhVzwJAbLHH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--ea03661d-f29f-43e2-b5cf-5ce709f2e3a8-0' usage_metadata={'input_tokens': 15, 'output_tokens': 619, 'total_tokens': 634, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}} 막걸리는 한국의 전통 발효주로, 쌀과 물, 누룩을 주재료로 사용하여 만듭니다. 아래는 기본적인 막걸리 제조법입니다.\n",
      "\n",
      "### 재료\n",
      "- 쌀: 1kg (찹쌀 또는 일반 쌀)\n",
      "- 물: 1.5~2리터\n",
      "- 누룩: 100g (막걸리용 누룩)\n",
      "- 설탕: 선택 사항 (단맛을 원할 경우)\n",
      "\n",
      "### 제조 과정\n",
      "\n",
      "1. **쌀 씻기**: 쌀을 깨끗이 씻어 불순물을 제거합니다. 여러 번 물을 갈아주며 씻어주세요.\n",
      "\n",
      "2. **쌀 불리기**: 씻은 쌀을 물에 담가 4~6시간 정도 불립니다. 찹쌀을 사용할 경우, 더 부드럽고 쫄깃한 막걸리가 됩니다.\n",
      "\n",
      "3. **쌀 찌기**: 불린 쌀을 체에 받쳐 물기를 빼고, 찜통에 넣어 30~40분 정도 쪄줍니다. 쌀이 고루 익도록 중간에 한 번 저어주는 것이 좋습니다.\n",
      "\n",
      "4. **식히기**: 찐 쌀을 넓은 그릇에 옮겨 담고, 상온에서 식힙니다. 온도가 30도 정도로 식어야 합니다.\n",
      "\n",
      "5. **누룩 섞기**: 식은 쌀에 누룩을 고루 섞어줍니다. 누룩이 잘 섞이도록 손으로 부드럽게 비벼주세요.\n",
      "\n",
      "6. **발효**: 혼합한 쌀과 누룩을 깨끗한 발효 용기에 담고, 물을 추가합니다. 뚜껑을 덮되, 완전히 밀봉하지 않고 공기가 통할 수 있도록 합니다. 1~2일 동안 실온에서 발효시킵니다. 발효가 진행되면 거품이 생기고, 향이 나기 시작합니다.\n",
      "\n",
      "7. **여과**: 발효가 끝나면, 체나 면포를 이용해 막걸리를 여과합니다. 이때, 남은 찌꺼기는 버리거나 다른 요리에 활용할 수 있습니다.\n",
      "\n",
      "8. **병입 및 저장**: 여과한 막걸리를 깨끗한 병에 담고, 냉장고에 보관합니다. 이때, 설탕을 추가하여 단맛을 조절할 수 있습니다.\n",
      "\n",
      "9. **숙성**: 막걸리는 냉장고에서 며칠 숙성시키면 더욱 맛이 좋아집니다. \n",
      "\n",
      "이제 막걸리를 즐길 준비가 완료되었습니다! 막걸리는 차갑게 해서 마시는 것이 일반적이며, 다양한 안주와 함께 즐기면 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res, res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09fc1a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 619,\n",
       "  'prompt_tokens': 15,\n",
       "  'total_tokens': 634,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_34a54ae93c',\n",
       " 'id': 'chatcmpl-BeuXQnob2zaffitkt1uhVzwJAbLHH',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71861354-39b6-4a7d-842d-8939d3a3e5bc",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-huggingface -qU\n",
    "```\n",
    "- nvidia GPU가 있는 경우 `torch cuda`  버전을 먼저 설치하고 `langchain-huggingface`를 설치 해야 한다. 아니면 `torch cpu` 버전이 설치된다.\n",
    "- `transformers`, `tokenizers`, `huggingface-hub` 등 huggingface 관련 package들이 같이 설치된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a6c8929-6c61-4e85-95b9-40857fd6bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\t\n",
    "# 1b : 파라미터수(10억), it : instruction training(질문/지시시-응답 형식으로 파인튜닝한 모델)\n",
    "model_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id = model_id,\n",
    "    task = \"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\" : 50}\t# transformers.pipeline()의 설정을 하는 파라미터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7562fc5a-6c5f-456b-aafd-ec8dbf9ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_hf.invoke(\"한국의 수도는 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdeb797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 ?\\n\\n정답: 서울\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653f31b-82aa-462f-a140-545b5c57d485",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://docs.anthropic.com/en/docs/about-claude/pricing\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "1. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "2. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "\n",
    "## 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing \n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8174de-b6a1-423b-8cce-271917ae8dc6",
   "metadata": {},
   "source": [
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8193ce-fa8b-4208-a7d9-af794044e61c",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic -qU\n",
    "```\n",
    "- `anthropic`package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3a23f-c491-4245-9013-83c5706fd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298545f-64b9-41aa-a375-7296030bb108",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba755283-5d24-464d-8c81-21d496100256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic# , Anthropic 지원하는 모델이 다른 것 같다.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model=\"claude-sonnet-4-20250514\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb8db47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic의 주요 LLM 모델은 다음과 같습니다:\n",
      "\n",
      "1. Claude\n",
      "- 가장 최신 대화형 AI 모델\n",
      "- 높은 윤리성과 안전성 강조\n",
      "- 정확하고 유연한 대화 능력\n",
      "- 현재 Claude 3 버전 출시\n",
      "\n",
      "2. Claude 2\n",
      "- Claude의 이전 버전\n",
      "- 더 넓은 컨텍스트 이해 능력\n",
      "- 복잡한 작업 처리에 강점\n",
      "\n",
      "3. Claude Instant\n",
      "- 더 가벼운 버전의 모델\n",
      "- 빠른 응답 속도\n",
      "- 간단한 작업에 최적화\n",
      "\n",
      "이 중 Claude 3가 현재 가장 최신이고 성능이 우수한 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1684d4b-a5f9-4e24-8263-b30d997833b0",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install langchain-ollama`\n",
    "  - `ollama` package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8e5fe",
   "metadata": {},
   "source": [
    "GUI 환경 : open_webui 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2254-7dd3-48b1-9de4-d25e278a2266",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0410c58d-5d0d-42a8-a2d6-0dfa0f7772f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model = \"qwen3:0.6b\"\n",
    ")\n",
    "\n",
    "response = model.invoke(\"한국의 수도를 알려줘잉~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ffd2dbcc-d3f6-4e30-86dd-f33db4f69a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user is asking for the capital of the country. I need to check which country they're referring to. Since they said \"한국\", which means \"the Korean Republic\" in Korean, I should mention South Korea. Let me confirm the capital of South Korea. It's Seoul. I should make sure there's no confusion with other countries. Also, maybe add some context to explain why it's the capital, like its significance or history. Keep it straightforward and positive.\n",
       "</think>\n",
       "\n",
       "한국의 수도는 **서울**입니다. 서울은 대한민체제의 핵심 도시로, 대한민국의 핵심 정치적, 경제적, 문화적 중심지입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "832b1cd0-e201-478f-85ae-f7ab40c778dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user is asking for the name of the Korean capital. I need to confirm that the capital is Seoul. Let me think. Seoul is the capital city of South Korea, right? It's known for its urban layout and modern infrastructure. I should mention that it's the main city in the country. Also, maybe add some context about its significance, like being the economic hub. Oh, and make sure to state that it's the capital city. I think that's all.\n",
       "</think>\n",
       "\n",
       "The Korean capital is **Seoul**. It is the largest city in South Korea and serves as the political, economic, and cultural center of the nation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(model.invoke(\"What is name of Korean capital\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b1d44",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "- 모델: https://ai.google.dev/gemini-api/docs/models?hl=ko\n",
    "- 가격정책: https://ai.google.dev/gemini-api/docs/pricing?hl=ko\n",
    "\n",
    "## API Key 생성\n",
    "\n",
    "1. https://aistudio.google.com/\n",
    "    - 연결 후 로그인(구글계정)\n",
    "2. Get API Key 클릭\n",
    "   \n",
    "    ![img](figures/gemini_api1.png)\n",
    "\n",
    "3. `API Key 만들기` 선택\n",
    "4. 프로젝트 선택 후 `기존 프로젝트에서 API 키 만들기` 선택\n",
    "\n",
    "## 환경변수\n",
    "- `GOOGLE_API_KEY` 환경변수에 생성된 API Key를 등록한다.\n",
    "## 설치\n",
    "- `pip install langchain_google_genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b2e8fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18500ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\"\n",
    ")\n",
    "\n",
    "response = model.invoke(\"gemini와 gemma의 차이를 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66a32b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini와 Gemma는 모두 Google에서 개발한 인공지능(AI) 모델이지만, **목적, 규모, 접근 방식, 그리고 사용처에서 중요한 차이**가 있습니다.\n",
       "\n",
       "핵심적인 차이점을 요약하자면 다음과 같습니다:\n",
       "\n",
       "*   **Gemini (제미니):** Google의 **최첨단, 대규모, 다중 모달(텍스트, 이미지, 오디오, 비디오 등)** AI 모델입니다. Google의 핵심 AI 기술을 대표하며 주로 API를 통해 접근하거나 Bard(현재 Gemini)와 같은 Google 제품에 통합되어 사용됩니다.\n",
       "*   **Gemma (젬마):** Gemini 개발에 사용된 연구 및 기술을 기반으로 만들어진 **경량(lightweight), 오픈 소스** 모델 제품군입니다. 개발자와 연구자가 로컬 환경에서 실행하고 미세 조정(fine-tuning)하기 쉽도록 설계되었습니다.\n",
       "\n",
       "아래 표에서 두 모델의 주요 차이점을 비교해 보세요:\n",
       "\n",
       "| 구분             | Gemini (제미니)                                     | Gemma (젬마)                                                  |\n",
       "| :--------------- | :-------------------------------------------------- | :------------------------------------------------------------ |\n",
       "| **목적**         | - Google의 핵심, 최첨단 AI 기술 시연 및 상업적 활용 | - AI 연구 및 개발 커뮤니티를 위한 **오픈 소스 경량 모델** 제공 |\n",
       "|                  | - 광범위한 고성능 AI 애플리케이션 구축             | - 로컬 실행, 미세 조정, 효율적인 AI 개발 지원                 |\n",
       "| **규모/성능**    | - **매우 크고 강력함** (Ultra, Pro, Nano 등 다양한 버전) | - **상대적으로 작고 효율적임** (2B, 7B 등 다양한 크기)        |\n",
       "|                  | - 다양한 벤치마크에서 최고 수준의 성능             | - 특정 크기 대비 매우 우수한 성능을 보이지만, Gemini Ultra만큼 강력하진 않음 |\n",
       "| **모달리티**     | - **다중 모달(Multimodal)**: 텍스트, 코드, 이미지, 오디오, 비디오 등 다양한 유형의 데이터 이해 및 생성 | - 주로 **텍스트 기반** (현재까지). 향후 확장 가능성 있음       |\n",
       "| **접근성/라이선스** | - **폐쇄형(Proprietary)**: Google의 API(Vertex AI, Google AI Studio)를 통해 접근하거나 Google 제품에 통합되어 사용 | - **오픈 소스(Open Source)**: 모델 가중치(weights)가 공개되어 누구나 다운로드하여 상업적 용도로도 사용 및 수정 가능 (Apache 2.0 라이선스) |\n",
       "| **주요 사용처**  | - Bard/Gemini 챗봇, Google Workspace, Google Cloud의 AI 서비스, 복잡한 AI 애플리케이션 개발 | - 개인 개발자, 연구자, 스타트업, 로컬 환경에서 AI 모델 실행, 특정 도메인에 특화된 AI 모델 미세 조정 |\n",
       "| **특징**         | - Google의 AI 역량을 집약한 **플래그십 모델**      | - Gemini의 연구 기반 위에 구축된 **'작은 버전' 또는 '파생 모델'** |\n",
       "\n",
       "### 두 모델의 관계\n",
       "\n",
       "Gemma는 Gemini의 '작은 버전' 또는 '오픈 소스 파생 모델'이라고 이해할 수 있습니다. Google은 Gemini를 개발하면서 얻은 최첨단 AI 기술과 안전성 연구를 Gemma에도 적용했습니다. 즉, Gemma는 Gemini의 핵심 기술 일부를 계승하면서도, 더 많은 개발자와 연구자가 AI 기술에 접근하고 활용할 수 있도록 '오픈 소스'와 '경량화'에 초점을 맞춘 모델 제품군입니다.\n",
       "\n",
       "**요약하자면,**\n",
       "\n",
       "*   **Gemini**는 최첨단 성능과 다중 모달 기능을 필요로 하는 대규모 애플리케이션 및 서비스에 적합한 Google의 핵심 모델입니다.\n",
       "*   **Gemma**는 개발자와 연구자가 로컬 환경에서 효율적으로 실행하고 미세 조정할 수 있는 오픈 소스 경량 모델로서, AI 생태계의 접근성을 높이는 데 기여합니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917ce34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
