{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fef342",
   "metadata": {},
   "source": [
    "### ❓부분 코드 채워 넣기 ! \n",
    "- 나머지 코드 cell은 주석 읽어본 뒤 실행, 결과 확인하기 !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83929e84",
   "metadata": {},
   "source": [
    "# 자연어 처리 모델링 프로세스\n",
    "\n",
    "1. **데이터 수집**\n",
    "    - **Corpus(말뭉치)**: 자연어 학습을 위해 수집한 언어 표본 집합을 \"말뭉치\" 또는 \"Corpus\"라고 한다.\n",
    "    - **수집 방법**\n",
    "        - 공개 데이터 사용\n",
    "        - 데이터 구매\n",
    "        - 웹 크롤링\n",
    "\n",
    "2. **텍스트 전처리**\n",
    "     - 분석 목적에 맞게 텍스트를 전처리한다.\n",
    "     - **정제 (Cleaning)**\n",
    "       - 문서 내 노이즈(불필요한 문자, 기호, 빈도가 낮은 단어 등)를 제거한다.\n",
    "       - 불용어(Stop word) 제거: 분석에 불필요한 단어(예: \"의\", \"에\", \"은\" 등)를 삭제하여 모델 성능을 향상시킨다.\n",
    "     - **정규화 (Normalization)**\n",
    "       - 같은 의미의 단어들을 하나의 형태로 통일한다. (예: \"말하다\", \"말하면\", \"말하기\" → \"말\")\n",
    "       - **주요 기법**\n",
    "         - 어간 추출 (Stemming), 원형 복원 (Lemmatization), 형태소 분석\n",
    "\n",
    "3. **텍스트 토큰화**\n",
    "    - 문서의 텍스트를 분석하기 위해 최소 단위로 나누는 작업이다.\n",
    "    - 보통 단어 단위나 글자 단위로 나누며, 토큰을 기계가 이해할 수 있도록 정수 형태로 변환한다.\n",
    "\n",
    "4. **임베딩**\n",
    "    - 각 토큰(단어)의 의미나 특성을 보다 잘 표현할 수 있도록 단어를 고차원 벡터로 변환한다.\n",
    "    - Feature Extraction(특성 추출)과정이라고 볼 수 있다.\n",
    "    - 빈도수 기반 통계적 방식과 뉴럴 네트워크를 이용한 학습 방식이 있다.\n",
    "    - **주요 기법**\n",
    "      - TF-IDF, Word2Vec, FastText\n",
    "\n",
    "5. **모델링**\n",
    "    - 임베딩된 데이터를 입력으로 받아 자연어 관련 문제를 해결하는 머신러닝 또는 딥러닝 모델을 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97dfbfa",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "- Natural Language ToolKit\n",
    "- https://www.nltk.org/\n",
    "- 자연어 처리를 위한 대표적인 파이썬 패키지. 한국어는 지원하지 않는다.\n",
    "\n",
    "## NLTK 설치\n",
    "- nltk 패키지 설치\n",
    "    - `pip install nltk`\n",
    "- NLTK 추가 패키지 설치\n",
    "```python\n",
    "import nltk\n",
    "nltk.download() # 설치 GUI 프로그램 실행을 실행해 다운로드\n",
    "nltk.download('패키지명')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f520132",
   "metadata": {},
   "source": [
    "## NLTK 주요기능\n",
    "\n",
    "- ### 텍스트 토큰화/정규화/전처리등 처리를 위한 기능 제공\n",
    "    - 토큰화(Tokenization)\n",
    "    - Stop word(불용어) 제공\n",
    "    - 형태소 분석\n",
    "        - 형태소\n",
    "            - 의미가 있는 가장 작은 말의 단위\n",
    "        - 형태소 분석\n",
    "            - 말뭉치에서 의미있는(분석시 필요한) 형태소들만 추출하는 것           \n",
    "        - 어간추출(Stemming)\n",
    "        - 원형복원(Lemmatization)\n",
    "        - 품사부착(POS tagging - Part Of Speech)\n",
    "        \n",
    "- ### 텍스트 분석 기능을 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845add2d",
   "metadata": {},
   "source": [
    "# NLTK 텍스트 정규화 기본 문법\n",
    "\n",
    "## Tokenization (토큰화)\n",
    "\n",
    "- 문자열을 분석의 최소단위(Token)으로 나누는 작업.\n",
    "- 글자단위, 단어단위, 형태소 단위, 문장단위 등 다양 방식으로 나눌 수있다.\n",
    "- 분석을 위해 문서를 작은 단위로 나누는 작업.\n",
    "- **주요 Tokenizer (함수)**\n",
    "    - **sent_tokenize()** : 문장단위로 나눠준다.\n",
    "    - **word_tokenize()** : 단어단위로 나눠준다.\n",
    "    - **regexp_tokenize()** : 토큰의 단위를 정규표현식으로 지정\n",
    "    - return: 분리된 토큰들을 원소로 하는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 실행하시고 시작하시길!\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\t# WordNetLemmatizer에서 지정할 수 있는 품사 목록\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144c5c5",
   "metadata": {},
   "source": [
    "### 1. word_tokenize\n",
    "- 내부적으로 **PunktSentenceTokenizer + TreebankWordTokenizer** 를 사용.\n",
    "- 이 토크나이저는 유니코드 구두점 문자(예: `’`) 도 구두점으로 인식해서 따로 분리함.\n",
    "- 그래서 **'Life’s' → 'Life', ’, 's'** 로 나뉨.\n",
    "\n",
    "### 2. TreebankWordTokenizer\n",
    "- 규칙 기반의 전통적인 영어 토크나이저.\n",
    "- 하지만 `’` 같은 유니코드 스마트 따옴표는 일반 ASCII `'`처럼 인식하지 못해서,\n",
    "- **'Life’s'** 를 통째로 하나의 단어로 봄.\n",
    "<br>\n",
    "\n",
    "\n",
    "|`'`|`＇`|`’`|\n",
    "|-|-|-|\n",
    "|U+0027|U+FF07|U+2019\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떻게 다르게 토큰이 나눠지는지 출력 보기\n",
    "txt = \"Life’s wonderfull, It can't disagree.\"\n",
    "word_tokens1 = nltk.word_tokenize(txt)\n",
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "word_tokens2 = tokenizer.tokenize(txt)\n",
    "\n",
    "len(word_tokens1), len(word_tokens2), word_tokens1, word_tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    전달받은 text 토큰화해 반환하는 함수\n",
    "    문장별로 단어 리스트(의미를 파악하는데 중요한 단어들)를 2차원 배열 형태로 반환\n",
    "       1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    구두점/특수문자, 숫자, 불용어(stop words)들은 모두 제거한다.\n",
    "    parameters:\n",
    "        text: string - 변환하려는 전체문장\n",
    "    return:\n",
    "        2차원 리스트. 1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    \"\"\"\n",
    "    # cleaning\n",
    "    ## 1. 소문자로 변환 : 대소문자를 구분해버리면 같은 단어라도 다르게 분류되니 통일!\n",
    "\t# low,,, up,, lower,, upper,,,\n",
    "    text = text.?\n",
    "    ## 2. 문장 단위 토큰화 \n",
    "\t# 문장이니까 sent, 토큰화니까 tokenize가 들어가면 좋겠죠~\n",
    "    sent_list = nltk.?(text)\n",
    "    ## 3. stopword(불용어) 사전 생성 stopwords.words(\"english\")\n",
    "    stopword_list = ?\n",
    "    ## 4. stopword 사전에 불용어 추가(옵션)\n",
    "    # [\"although\", \"nuless\", \"may\"]를 추가 해볼까요 ?\n",
    "    stopword_list.extend([?])\n",
    "\t# 특수 문자도 추가해봅시다 string.punctuation = \"!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "    stopword_list.extend(list(?))\t# list 형태로 넣어야 차곡차곡 들어감\n",
    "    \n",
    "    # cleaning + 토큰화\n",
    "    result_list = []\n",
    "    for sent in sent_list:  # 문장 단위로 토큰화 + 전처리할려고 문장을 하나씩 가져오는 루프\n",
    "        word_list = nltk.?(sent) \t# 문장 단위로 가져왔으니 이제 단어(word) 단위 토큰화(tokenize)를 해줘야겠죠 ?\n",
    "        # 불용어 제거\n",
    "        # 컴프리핸션이 익숙치 않을 수도 있지만 읽어보면 읽힌답니다 ? 저도 방금 읽음\n",
    "        # w를 word_list에서 가져오는데 그게 if 불용어 사전에 없으면 word_list로 선언\n",
    "        word_list = [w for w in word_list if w not in stopword_list]\n",
    "        # 선언 해주고 하나씩 append하면 list가 쨘하고 나오겠죠~? \n",
    "        result_list.append(word_list)\n",
    "        \n",
    "\t\t\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a03985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얘를 넣어서 함수가 잘 만들어졌는지 확인해 봅시당\n",
    "text_sample = \"\"\"Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense.\n",
    "Readability counts.\n",
    "Special cases aren't special enough to break the rules.\n",
    "Although practicality beats purity.\n",
    "Errors should never pass silently.\n",
    "Unless explicitly silenced.\n",
    "In the face of ambiguity, refuse the temptation to guess.\n",
    "There should be one-- and preferably only one --obvious way to do it.\n",
    "Although that way may not be obvious at first unless you're Dutch.\n",
    "Now is better than never.\n",
    "Although never is often better than *right* now.\n",
    "If the implementation is hard to explain, it's a bad idea.\n",
    "If the implementation is easy to explain, it may be a good idea.\n",
    "Namespaces are one honking great idea -- let's do more of those!\"\"\"\n",
    "\n",
    "tokenize_text(text_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14fd5c3",
   "metadata": {},
   "source": [
    "## 한국어에서 토큰화가 어려운 이유\n",
    "- 영어는 띄어쓰기(공백)을 기준으로 토큰화를 진행해도 큰 문제가 없다.\n",
    "- 한국어는 교착어이기 때문에 띄어쓰기를 기준으로 토큰화를 하면 같은 단어가 다른 토큰으로 인식되어 여러개 추출되는 문제가 발생한다.\n",
    "    - 예) \"그가\", \"그는\", \"그의\", \"그와\" 모두 \"그\"를 나타내지만 붙은 조사가 달라 다 다른 토큰으로 추출되게 된다.\n",
    "- 그래서 한국어는 어절 단위 토큰화는 하지 않도록 한다.\n",
    "- 대신 형태소에 기반한 토큰화를 하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c842f62",
   "metadata": {},
   "source": [
    "## 형태소(morpheme) 분석\n",
    "- 형태소\n",
    "    - 일정한 의미가 있는 가장 작은 말의 단위\n",
    "- 형태소 분석  \n",
    "    - 말뭉치에서 의미있는(분석에 필요한) 형태소들만 추출하는 것\n",
    "    - 보통 단어로 부터 어근, 접두사, 접미사, 품사등 언어적 속성을 파악하여 처리한다. \n",
    "- 형태소 분석을 위한 기법\n",
    "    - 어간추출(Stemming)\n",
    "    - 원형(기본형) 복원 (Lemmatization)\n",
    "    - 품사부착 (POS tagging - Part Of Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800bbb4",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming)\n",
    "- 어간: 활용어에서 변하지 않는 부분\n",
    "    - painted, paint, painting → 어간: paint\n",
    "    - 보다, 보니, 보고 → 어간: `보`\n",
    "- 어간 추출 목적\n",
    "    - 같은 의미를 가지는 단어의 여러가지 활용이 있을 경우 다른 단어로 카운트 되는 문제점을 해결한다.\n",
    "        - flower, flowers 가 두개의 단어로 카운트 되는 것을 flower로 통일한다.        \n",
    "- nltk의 주요 어간 추출 알고리즘\n",
    "    - Porter Stemmer\n",
    "    - Lancaster Stemmer\n",
    "    - Snowball Stemmer\n",
    "- 메소드\n",
    "    - `stemmer객체.stem(단어)`\n",
    "- stemming의 문제\n",
    "    - 완벽하지 않다는 것이 문제이다.        \n",
    "        - ex) new와 news는 다른 단어 인데 둘다 new로 처리한다.\n",
    "    - 처리후 눈으로 직접 확인해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20e83a",
   "metadata": {},
   "source": [
    " ### **어간과 형태소의 차이**\n",
    "-  **형태소** : 의미(뜻)을 가진 가장 작은 언어의 단위, 더 이상 나눌 수 없는 언어의 조각 단위. 형태소는 그 자체로 의미를 가지며 단어를 형성하거나 변형 시키는데 사용한다.\n",
    "\t- 자립 형태소:  명사, 동사, 형용사 같이 **독립적**으로 사용될 수 있는 형태소\n",
    "\t\t- 나, 너, 택시, 가다, you, have\n",
    "\t- 의존 형태소: 조사, 접미사, 접두사 같이 다른 형태소와 **결합해서 사용** 되야 하는 형태소.\n",
    "\t\t- \\~의, \\~가, un\\~, \\~able\n",
    "-  **어간** :  어간은 접미사나 다른 변화 형태가 추가되기 전의 **단어 기본 형태**를 말한다. 즉 활용시 변하지 않는 부분을 말한다.\n",
    "\t- **view** + ing, **view** + er\n",
    "\t- **먹** + 습니다.  **먹** + 었다.  **먹** + 고\n",
    "\t- **예쁘**  + 다, **예쁘**+ 고, **예쁘** + 지만, **예쁘**+ 어서(예뻐서)\n",
    "\n",
    "- **어간**은 특정 단어에서 그 단어의 핵심 의미를 담은 부분. 형태소에서 주로 자립 형태소에 해당하고 어미와 같은 의존 형태소가 결합하여 문법적 기능이나 형태를 변화시킬 수 있다.\n",
    "-  **형태소**는 의미를 구성하는 기본 단위로서의 역할을 하며, **어간**은 특히 단어를 형성하고 변형 시키는 기반으로서의 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abc0cd",
   "metadata": {},
   "source": [
    "##  NLTK에서 지원하는 주요 Stemmer\n",
    "\n",
    "### 지원 Stemmer 목록\n",
    "\n",
    "| Stemmer            | 설명                                       | \n",
    "|-|-|\n",
    "| **PorterStemmer**   | 가장 고전적인 스테머 (Porter 알고리즘)       |\n",
    "| **LancasterStemmer**| 더 강력하고 과감한 알고리즘                  |\n",
    "| **SnowballStemmer** | Porter의 개선판, 다국어 지원                |\n",
    "| **RegexpStemmer**   | 정규표현식을 이용한 사용자 정의 스테머       | \n",
    "\n",
    "<br>\n",
    "\n",
    "###  NLTK 주요 Stemmer 비교표\n",
    "\n",
    "| Stemmer | 특징 | 장점 | 단점 | 추천 사용 경우 |\n",
    "|--------|------|------|------|----------------|\n",
    "| **PorterStemmer** | 고전적인 규칙 기반 스테머.<br> 비교적 보수적인 축약. | - 간단하고 가벼움<br>- 널리 사용됨<br>- 빠른 처리 속도 | - 일부 축약이 너무 약함<br>- 오래된 규칙 기반으로 최신 용도엔 제한 | - 고전적인 NLP 실험<br>- 빠른 전처리 필요할 때 |\n",
    "| **LancasterStemmer** | 매우 공격적인 어간 축약 방식 | - 처리 속도 빠름<br>- 희소성 극복에 유리<br>- 구현 간단 | - 과도한 축약으로 의미 손실 큼<br>- 축약 결과 해석이 어려움 | - 차원을 줄여 복잡도 감소와 같은 효과를 보기위해<br>- 리소스가 매우 제한된 환경 |\n",
    "| **SnowballStemmer** | Porter의 개선판. 더 정밀하고 다양한 언어 지원 | - 의미 보존력 우수<br>- 다국어 지원<br>- 규칙과 코드가 정교함 | - Porter보다 약간 느림<br>- 구현 복잡성 ↑ | - 정밀한 전처리가 필요한 경우<br>- 다국어 텍스트 처리 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 3개 stemmer 실행하면서 결과 비교해보기\n",
    "words = [\n",
    "    \"Working\",\n",
    "    \"works\",\n",
    "    \"worked\",\n",
    "    \"Painting\",\n",
    "    \"Painted\",\n",
    "    \"paints\",\n",
    "    \"Happy\",\n",
    "    \"happier\",\n",
    "    \"happiest\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer\n",
    "stemmer = ?()\n",
    "[stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79764de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LancasterStemmer\n",
    "stemmer = ?()\n",
    "[stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SnowballStemmer\n",
    "stemmer = ?(\"\") # 얘는 언어 설정 해줘야함 !\n",
    "[stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9a184",
   "metadata": {},
   "source": [
    "### 원형(기본형)복원(Lemmatization)\n",
    "- 단어의 원형(기본형)을 반환한다.\n",
    "    - ex) am, is, are => be\n",
    "- 단어의 품사를 지정하면 정확한 결과를 얻을 수 있다. \n",
    "- `WordNetLemmatizer객체.lemmatize(단어 [, pos=wordnet.(품사)])`\n",
    "\t- 품사 : NOUN(명사), ADJ(형용사), VERB(동사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "for word in words:\n",
    "\tprint(lemm.?(word, pos=wordnet.?))\t# lemmatize 메소드,\n",
    "\t   \t\t\t\t\t\t\t\t\t# 품사들을 넣어보며 결과값을 확인해봅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce71ce5",
   "metadata": {},
   "source": [
    "### 어간추출과 원형복원의 장단점\n",
    "\n",
    "- **원형 복원 (Lemmatization)**\n",
    "\n",
    "   - **장점**:\n",
    "      - 문맥과 품사를 고려하여 정확한 사전적 원형을 반환하므로 의미적으로 정확한 결과를 얻을 수 있다.\n",
    "      - 품사 정보를 유지하기 때문에 의미론적 분석에 유리하다.\n",
    "   - **단점**:\n",
    "      - 형태소 분석과 사전 조회 과정이 필요하기 때문에 어간 추출보다 처리속도가 느리다.\n",
    "      - 구현이 더 복잡하며, 품사 태깅 및 사전 데이터베이스가 필요하다.\n",
    "\n",
    "- **어간 추출 (Stemming)**\n",
    "\n",
    "   - **장점**:\n",
    "     - 규칙 기반 알고리즘으로 동작하므로 처리속도가 빠르다.\n",
    "     - 처리속도가 빠르므로 대량의 텍스트를 효율적으로 처리할 수 있다.\n",
    "     - 정보 검색이나 텍스트 분류 등 단순한 텍스트 정규화 작업에 적합하다\n",
    "   - **단점**:\n",
    "      - 문법적 차이를 고려하지 않고 기계적으로 자르기 때문에 의미 손실이 발생하거나 다른 단어들이 같은 어간으로 추출될 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61fcd50",
   "metadata": {},
   "source": [
    "### 품사부착-POS Tagging(Part-Of-Speech Tagging)\n",
    "- 형태소에 품사를 붙이는 작업.\n",
    "    - 품사의 구분이나 표현은 언어, 학자마다 다르다. \n",
    "- NLTK는 [펜 트리뱅크 태그세트](https://bluebreeze.co.kr/1357)(Penn Treebank Tagset) 이용\n",
    "    - **명사** : N으로 시작 (NN-일반명사, NNP-고유명사)\n",
    "    - **형용사** : J로 시작 (JJ, JJR-비교급, JJS-최상급)\n",
    "    - **동사**: V로 시작 (VB-동사원형, VBP-3인칭 아닌 현재형 동사)\n",
    "    - **부사**: R로 시작 (RB-부사)\n",
    "    - `nltk.help.upenn_tagset('키워드')` : 도움말\n",
    "- `pos_tag(단어_리스트)`    \n",
    "    - 단어와 품사를 튜플로 묶은 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagset 조회\n",
    "nltk.help.upenn_tagset() \t\t# 전체 tagset\n",
    "nltk.help.upenn_tagset(\"NN\") \t# 개별 tag에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Book\", \"car\", \"have\", \"Korea\", \"is\", 'well', 'can']\n",
    "?(words)\t# 품사(pos) 태깅(tagging)을 해봅시다 ! pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d725c",
   "metadata": {},
   "source": [
    "### 품사부착과 원형복원을 이용해 원형복원하기.\n",
    "- 품사부착으로 품사 조회\n",
    "    - pos_tag와 lemmatization이 사용하는 품사 형태 다르기 때문에 변환함수 만듬\n",
    "- lemmatization하기.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos-tag 에서 반환한 품사표기(펜 트리뱅크 태그세트)을 \n",
    "# WordNetLemmatizer의 품사표기로 변환 -> 원형 복원을 위해\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"\n",
    "    펜 트리뱅크 품사표기를 WordNetLemmatizer에서 사용하는 품사표기로 변환\n",
    "    형용사/동사/명사/부사 표기 변환\n",
    "    \"\"\"\n",
    "    if pos_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b6514",
   "metadata": {},
   "source": [
    "### 토큰화 + 품사 태깅 + 원형 복원 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebff0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "sent_tokens = tokenize_text(text_sample)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "lemm = WordNetLemmatizer()\n",
    "for sent in sent_tokens:\n",
    "\tpos = pos_tag(sent)\t\t# 품사 태깅\n",
    "\tl = []\n",
    "\tfor word, pos_t in pos:\n",
    "\t\ttag = get_wordnet_pos(pos_t)\t# 품사를 wordnet방식으로 변경.\n",
    "\t\tif tag is not None:\n",
    "\t\t\tl.append(lemm.lemmatize(word, pos=tag))\t# 원형 복원\n",
    "\tresult.append(l)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 개념도 추가해서 다시 함수 정의 해봅시다!\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    전달받은 text 토큰화해 반환하는 함수\n",
    "    문장별로 단어 리스트(의미를 파악하는데 중요한 단어들)를 2차원 배열 형태로 반환\n",
    "       1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    구두점/특수문자, 숫자, 불용어(stop words)들은 모두 제거한다.\n",
    "    parameters:\n",
    "        text: string - 변환하려는 전체문장\n",
    "    return:\n",
    "        2차원 리스트. 1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    \"\"\"\n",
    "    # cleaning\n",
    "    ## 1. 소문자로 변환 : 대소문자를 구분해버리면 같은 단어라도 다르게 분류되니 통일!\n",
    "\t# low,,, up,, lower,, upper,,,\n",
    "    text = text.?\n",
    "    ## 2. 문장 단위 토큰화 \n",
    "\t# 문장이니까 sent, 토큰화니까 tokenize가 들어가면 좋겠죠~\n",
    "    sent_list = nltk.?(text)\n",
    "    ## 3. stopword(불용어) 사전 생성 stopwords.words(\"english\")\n",
    "    stopword_list = ?\n",
    "    ## 4. stopword 사전에 불용어 추가(옵션)\n",
    "    # [\"although\", \"nuless\", \"may\"]를 추가 해볼까요 ?\n",
    "    stopword_list.extend([?])\n",
    "\t# 특수 문자도 추가해봅시다 string.punctuation = \"!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "    stopword_list.extend(list(?))\t# list 형태로 넣어야 차곡차곡 들어감\n",
    "    \n",
    "\t# Lemmatizer 생성\n",
    "    lemm = WordNetLemmatizer()\n",
    "    # 결과 저장 리스트 생성\n",
    "    result_list = []\n",
    "    \n",
    "    for sent in sent_list:  # 문장 단위로 토큰화 + 전처리할려고 문장을 하나씩 가져오는 루프\n",
    "        word_list = nltk.?(sent) \t# 문장 단위로 가져왔으니 이제 단어(word) 단위 토큰화(tokenize)를 해줘야겠죠 ?\n",
    "        # 불용어 제거\n",
    "        # w를 word_list에서 가져오는데 그게 if 불용어 사전에 없으면 word_list로 선언\n",
    "        word_list = [w for w in word_list if w not in stopword_list]\n",
    "        \n",
    "        \n",
    "        ## 품사 태깅\n",
    "        word_pos = ?(word_list)\t# 품사(pos) 태깅(tagging)이니까 pos_tag \n",
    "        \t\t\t\t\t\t# -> [(단어, 품사), (단어, 품사), ...] 형태로 나옴\n",
    "        word_pos = [\n",
    "            \t\t(word, ?(pos)) \t# None이 아닌 얘만 가져와서 (단어, 변환된 pos)튜플을 만들어줘야 합니다!\n",
    "                    for word, pos in word_pos if ?(pos) is not None\t\t# 위에 정의해준 get_wordnet_pos 함수에 넣어서 \n",
    "                    \t\t\t\t\t\t\t\t\t\t\t\t\t# 품사 태그가 None이 아닌 경우만 가져와야 합니다!\n",
    "                    ]\t\t\t# -> [(단어, 품사), (단어, 품사), ...] 형태로 나옴\n",
    "        ## 원형 복원\n",
    "        tokens = [lemm.?(word, pos = pos) for word, pos in word_pos] # Lemmatizr의 메소드 lemmatize를 사용해 원형 복원\n",
    "        result_list.append(tokens)\n",
    "\n",
    "        \n",
    "\t\t\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5607fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "tokenize_text(text_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682cb439",
   "metadata": {},
   "source": [
    "# NLTK의 분석을 위한 클래스들\n",
    "\n",
    "## Text클래스\n",
    "- 문서 분석에 유용한 여러 메소드 제공\n",
    "- **토큰 리스트**을 입력해 객체생성 후 제공되는 메소드를 이용해 분석한다.\n",
    "- ### 생성\n",
    "    - Text(토큰리스트, [name=이름])\n",
    "- ### 주요 메소드\n",
    "    - count(단어)\n",
    "        - 매개변수로 전달한 단어의 빈도수\n",
    "    - plot(N)\n",
    "        - 빈도수 상위 N개 단어를 선그래프로 시각화\n",
    "    - dispersion_plot(단어리스트)\n",
    "        - 매개변수로 전달한 단어들이 전체 말뭉치의 어느 부분에 나오는지 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef889622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메소드들을 활용할 기사 하나 load\n",
    "with open(\"news.txt\", \"rt\", encoding=\"utf-8\") as fr:\n",
    "\tnews_txt = fr.read()\n",
    "\n",
    "len(news_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16abe8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "news_tokens = tokenize_text(news_txt)\n",
    "news_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text 객체에 넣으려면 1차원 형태로 변경\n",
    "#[[w, w, w], [w, w, w]] -> [w, w, w, w, w, w]\n",
    "news_tokens_flatten = []\n",
    "for lst in news_tokens:\n",
    "\tnews_tokens_flatten += lst\n",
    "\n",
    "news_tokens_flatten[:10], len(news_tokens_flatten)\n",
    "# 위 출력 결과랑 비교했을 때 1차원 형태로 잘 변경된 모습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75114ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text 객체 생성\n",
    "news_text = ?(# Text 객체 생성이니 Text!\n",
    "\tnews_tokens_flatten,\t# 분석 대상 문서, 토큰리스트(1차원), str-내부적으로 글자단위 토큰화\n",
    "\tname = \"흥민쏜 기사\",\t# 생성한 Text객체의 이름을 지정\n",
    ")\n",
    "news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 토큰의 빈도수를 확인 (문서 안에서 몇번 나왔는지)\n",
    "news_text.count(\"min-kyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeaa580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['font.family'] = \"malgun gothic\"\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "news_text.plot(20)\t# 빈도수 상위 N개를 시각화 (생략하면 전체 토큰에 대해 시각화해버림)\n",
    "plt.title(\"top 20 ㅋㅋ\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00483e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text.dispersion_plot(\n",
    "\t['son', 'korean', 'football']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a45cbb",
   "metadata": {},
   "source": [
    "## FreqDist\n",
    "- document에서 사용된 토큰(단어)의 사용 빈도수와 관련 정보를 조회할 수있는 분석 클래스.\n",
    "    - 토큰(단어)를 key, 개수를 value로 가지는 딕셔너리 형태\n",
    "- 생성\n",
    "    - Text 객체의 vocab() 메소드로 조회한다.\n",
    "    - 생성자(Initializer)에 토큰 List를 직접 넣어 생성가능\n",
    "- 주요 메소드\n",
    "    - B(): 출연한 고유 단어의 개수\n",
    "        - [Apple, Apple] -> 1\n",
    "    - N(): 총 단어수 \n",
    "        - [Apple, Apple] -> 2\n",
    "    - get(단어) 또는 FreqDist['단어'] : 특정 단어의 출연 빈도수\n",
    "    - freq(단어): 총 단어수 대비 특정단어의 출연비율\n",
    "    - most_common() : 빈도수 순서로 정렬하여 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq = news_text.vocab()\t# 객체 생성\n",
    "freq2 = FreqDist(news_tokens_flatten) # 직접 빈도 정보 계산\n",
    "freq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e488502",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.?() # 출연한 고유 단어의 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83173fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.?() # 총 단어수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80706b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"min-kyu\"\n",
    "\n",
    "freq.?(word), ?[word]\t# 특정 단어의 출연수 메소드(.get) or FreqDist 객체에 단어를 넣어서 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.?(word)\t# 특정 단어의 출연 비율 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.get(word) / freq.N()\t# 위와 같은걸 알 수 있다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c24800",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.?()\t# 빈도수 상위(숫자로 제한 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d39bde",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "\n",
    "- 텍스트의 단어의 빈도수를 시각적으로 표현하는 도구로 **문서의 주요 키워드들을 시각적으로 확인할 때 사용한다.**\n",
    "     - 빈도수가 높은 단어는 크고 굵게, 빈도수가 낮은 단어는 작고 얇게 표현한다.\n",
    "- wordcloud 패키지 사용\n",
    "     - 설치: `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# 1. wordcloud 객체 생성 : 어떻게 그릴지 설정\n",
    "wc = WordCloud(\n",
    "\tmax_words=200,\t\t\t# 최대 몇개 단어를 사용해서 그릴지,\n",
    "\t\t\t\t\t\t\t# (입력 단어(토큰)이 지정한 개수보다 많을 경우 빈도수 많은 순서로 선택)\n",
    "\tprefer_horizontal=0.5,\t# 가로 방향으로 쓴 단어의 비율\n",
    "\tmin_font_size = 1,\t\t# 시작 폰트 크기\n",
    "\trelative_scaling=0.5,\t# 빈도수가 증가할 때 마다 폰트크기를 얼마나 키울지 (0.5 -> 50%)\n",
    "\twidth = 400,\n",
    "\theight = 300,\n",
    "\tbackground_color = \"white\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. wordcloud 객체에 data를 넣어서 graph 생성\n",
    "wc_img = wc.generate_from_frequencies(freq)\t# {단어 : 빈도수, 단어2 : 빈도수, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ef98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장\n",
    "wc_img.to_file(\"news_cloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wc_img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
