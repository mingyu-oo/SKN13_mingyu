{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7980dcb6",
   "metadata": {},
   "source": [
    "# 1. Count-Based Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f64c5",
   "metadata": {},
   "source": [
    "## 01. Bag of Words(BoW)\n",
    ": **단어의 순서를 고려하지 않고** **출현 빈도**에만 집중해 단어를 표현(word representation)하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994f187",
   "metadata": {},
   "source": [
    "### 01-1. DTM/TDM \n",
    ": 문서를 구성하는 단어들이 몇번 나오는지 표현하는 행렬로 Vector화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53723d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"나는 야구를 좋아합니다. 나는 축구를 좋아하지 않습니다.\", \n",
    "        \"친구는 야구와 축구를 좋아합니다. 야구보다 축구를 더 좋아합니다.\", \n",
    "        \"나는 축구를 더 좋아합니다. 영국 축구 리그보다 이탈리아 축구 리그를 더 좋아합니다.\", \n",
    "        \"나는 농구를 좋아하지 않습니다. 배구도 좋아하지 않습니다.\", \n",
    "        \"나는 액션영화를 좋아합니다.\", \n",
    "        \"어제 비빕밥을 먹었습니다.\"]\n",
    "\n",
    "stop_words = list('은는이가을를도.,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f3f605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '야구', '를', '좋아합니다', '.', '나', '는', '축구', '를', '좋아하지', '않습니다', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okt 모델 활용\n",
    "# 1) import\n",
    "from konlpy.tag import Okt\n",
    "# 2) Okt 모델을 불러와서 okt변수로 지정\n",
    "okt = Okt()  # 함수로 불러오기\n",
    "# 3) 토크나이저(tokenizer) 함수 정의하기\n",
    "# tokenizer라는 함수를 만들고 doc를 매개변수(parameter)로 지정\n",
    "def tokenizer (doc):\n",
    "    return okt.morphs(doc)  # 형태소 단위로 토큰화 - 메소드 지정 \n",
    "\n",
    "\n",
    "tokenizer(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리기 객체 생성\n",
    "# 1) import\n",
    "from sklearn.feature_extraction.text import CountVectorizer   # Stopword를 한번에 처리해주는 기능이 있음.\n",
    "import pandas as pd\n",
    "# 2) CountVectorizer 전처리기 생성\n",
    "cv = CountVectorizer(\n",
    "    tokenizer=_________, # 앞에서 만들어둔 함수 불러오기 # 문서를 받아서 토큰화 처리하는 callable 전달.\n",
    "    stop_words = __________,  # 불용어는 앞에 stop_words 변수에 넣음.\n",
    "    token_pattern=None\n",
    ")\n",
    "# 3) 전처리기 학습 \n",
    "# 앞에 만들어둔 text 넣기\n",
    "cv._____(____)   # 학습할 때 쓰는 메소드(fit? transform? 뭘 써야 할까여)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e16ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1) 어휘사전\n",
    "cv._________   # 단어(vocabulary)와 그에 따른 index를 dictionary 형태로 반봔\n",
    "\n",
    "# 4-2) 어휘사전 정렬\n",
    "# vocabulary_.items()는 (단어, 인덱스) 형태의 튜플들로 구성된 iterable 객체를 반환\n",
    "# key함수: 자료구조 원소를 받아서 정렬할 때 사용할 값을 반환\n",
    "# key값은 lambda 활용해서 튜플 형태 중 인덱스 값 호출 (값, 인덱스)\n",
    "dict(sorted(cv.vocabulary_.items(), key=______ x: x[_])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) 변환(transform)+ array 형태로 바꾸기\n",
    "cv._______(text)_______()\n",
    "\n",
    "# 5-2) feature 이름(토큰화된 단어) 조회하기\n",
    "cv._______________() \n",
    "\n",
    "#5-3) dataframe으로 만들어보기\n",
    "DTM_df = pd.DataFrame(\n",
    "    cv.transform(text).toarray(),\n",
    "    columns= cv.get_feature_names_out(),\n",
    "    index= [f\"문서{i+1}\" for i in range(len(text))]\n",
    ")  \n",
    "DTM_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fe5e6",
   "metadata": {},
   "source": [
    "> #### n-gram\n",
    ">\n",
    "> -   N 개의 단어(token)을 묶어서 하나의 토큰으로 처리하는 방식을 n-gram이라고 한다. (n은 몇개 토큰을 하나의 단위로 묶을지 개수)\n",
    ">     -   uni-gram (n=1), bi-gram (n=2), tri-gram (n=3), 4개부터는 n-gram으로 표기(4-gram, 5-gram, ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "##n-gram 적용\n",
    "#######################\n",
    "cv = CountVectorizer(\n",
    "    tokenizer=tokenizer,\n",
    "    stop_words=stop_words,\n",
    "    token_pattern=None,\n",
    "    ngram_range=(1,3)  #ngram: 1, 2, 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "cv.fit(text) # 어휘사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cadda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names_out()   \n",
    "\n",
    "# 여기서 질문! n_gram을 적용하지 않을 때와 토큰에 어떤 차이가 있나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a26f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe으로 만들어보기\n",
    "ngram_df = pd.DataFrame(\n",
    "    cv.transform(text).toarray(),\n",
    "    columns=cv.get_feature_names_out()\n",
    ")\n",
    "ngram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224f3a4",
   "metadata": {},
   "source": [
    "### 01-2. TF-IDF\n",
    "\n",
    ": 개별 문서에 많이 나오는 단어가 높은 값을 가지도록 하되 동시에 여러 문서에 자주 나오는 단어에는 페널티를 주는 방식\n",
    "\n",
    "> 문서에서 나온 횟수가 같더라도, 다른 문서에서 더 많이 나왔을 때, 그 가중치를 곱해줌\n",
    "\n",
    "-   TF (Term Frequency) 정의: 해당 단어가 **해당 문서에** 몇번 나오는지를 나타내는 지표\n",
    "-   DF (Document Frequency) 정의: 해당 단어가 **몇개의 문서에** 나오는지를 나타내는 지표\n",
    "-   IDF (Inverse Document Frequency) 정의: DF에 역수로 $\\cfrac{\\text{전체 문서수}}{\\text{해당 단어가 나오는 문서수}}$\n",
    "-   TF-IDF 정의: $TF * \\left(\\log \\cfrac{\\text{전체 문서수}}{\\text{해당 단어가 나오는 문서수}} \\right)$\n",
    "\n",
    "   -   log는 전체 문서의 수가 많으면 값의 단위가 너무 커지므로 log를 취한다.\n",
    "    -   scikit-learn의 경우 분모가 0이 되는 것을 방지하기 위해 **분모에 1을 더하고** $\\log(0)$도 계산이 안되므로 **분자에도 1을 더했으며** 그 계산 결과에 **1을 더하여 계산**함.\n",
    "        -   $TF * \\left(\\log \\cfrac{\\text{전체 문서수 + 1}}{\\text{해당 단어가 나오는 문서수 + 1}} + 1\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56244fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TfidfVectorizer import\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#2. TfidfVectorizer 전처리기 생성\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=tokenizer,\n",
    "    stop_words=stop_words,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "# 3. fit & transform\n",
    "#입력: [문서1, 문서2, ...]\n",
    "tfidf.fit(text)   # 어휘사전 생성\n",
    "result = tfidf.transform(text) # 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. dataframe으로 만들기\n",
    "tfidf_df = pd.DataFrame(\n",
    "    result.toarray(),\n",
    "    columns= tfidf.get_feature_names_out(),\n",
    "    index= [f\"문서{i+1}\" for i in range(len(text))]\n",
    ")\n",
    "tfidf_df.iloc[:2]   \n",
    "# 여러 문서에 나올수록 값이 더 작아짐. \n",
    "# 즉 횟수가 더 많은 값이 나올수록 영향을 적게 미치게됨. \n",
    "# ex. 좋아합니다: 6번, 0.287558 / 축구: 5번, 0.33557"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db213fd",
   "metadata": {},
   "source": [
    "# 2. Word Embedding\n",
    ": 앞선 Count-based는 출현 빈도만 나타내기 때문에 문장을 모두 파악하는데 정보가 부족하다.<br>\n",
    "빈도수 이외에 의미의 유사성이 반영될 수 있도록 비슷한 의미를 가지는 단어는 비슷한 값들로 구성되도록 하기 위해 워드 임베딩 사용\n",
    "\n",
    "**단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)** 이라고 한다. 워드 임베딩 결과로 나온 밀집 벡터를 **임베딩 벡터(embedding vector)** 라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cbd11",
   "metadata": {},
   "source": [
    "## 02. Word2Vec\n",
    ": 딥러닝 기반 word embedding 방식. 딥러닝 모델은 `입력층-은닉층-출력층` 의 단순한 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109ee16",
   "metadata": {},
   "source": [
    "### 02-1. CBOW(Continous Bag of Words)\n",
    ":  **주변 단어들**로부터 중심 단어를 예측하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecdf38",
   "metadata": {},
   "source": [
    "#### Dataset 구성\n",
    "- CBOW/Skip-gram 모델 학습 데이터셋은 모두 token들이 one-hot encoding 되 있어야 한다.  \n",
    "\n",
    "#### Window size 설정\n",
    "- Window size를 설정 한 뒤 문장을 중심 단어를 뒤로 이동 시키면서 중심단어와 주변단어들을 추출해 데이터셋을 만든다.\n",
    "- 지정한 개수(window) 만큼씩 이동하면서 어떤 작업을 진행하는 것을 **Sliding window 방식** 이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb35ea",
   "metadata": {},
   "source": [
    "<img src=\"figures/word2vec_cbow.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a5826",
   "metadata": {},
   "source": [
    "### 02-2. Skip-gram\n",
    "\n",
    "중심단어를 이용해 그 주변단어를 예측하는 모델을 구성한다.\n",
    "\n",
    "<img src='figures/word2vec_skipgram.png' width=\"800\">\n",
    "\n",
    "- 모델의 입력으로 중심단어의 one-hot vector가 들어가고 모델을 주변단어를 추론한다. window size 가 2라면 총 4개의 주변단어를 추론한다.\n",
    "- 학습 할 때는 각 주변단어들에 대한 개별 loss를 계산하고 그 합계를 최종 loss하여 $W_{in}$과 $W_{out}$ 을 update 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28ae60",
   "metadata": {},
   "source": [
    "| 항목                | **CBOW (Continuous Bag of Words)** | **Skip-Gram**                             |\n",
    "| ----------------- | ---------------------------------- | ----------------------------------------- |\n",
    "| 🔁 **예측 방향**      | 주변 단어 → 중심 단어                      | 중심 단어 → 주변 단어                             |\n",
    "| 🧾 **입력(Input)**  | 주변 단어들 (Context Words)             | 중심 단어 (Target Word)                       |\n",
    "| 🎯 **출력(Output)** | 중심 단어 (Target Word)                | 주변 단어들 (Context Words)                    |\n",
    "| ⚙️ **학습 방식**      | 주변 단어 벡터들의 평균을 통해 예측               | 하나의 중심 단어로 여러 주변 단어 예측                    |\n",
    "| ⏱ **학습 속도**       | 상대적으로 빠름                           | 상대적으로 느림                                  |\n",
    "| 🔍 **희귀 단어 표현**   | 상대적으로 부정확                          | 희귀 단어 표현에 강함                              |\n",
    "| 📦 **말뭉치 크기 적합성** | 작은 말뭉치에 유리                         | 큰 말뭉치에 유리                                 |\n",
    "| 🎯 **적합한 사용 목적**  | 일반적인 단어 표현 학습                      | 정교한 단어 표현 학습                              |\n",
    "| 💬 **문맥 정보 처리**   | 주변 단어 평균 처리 (정보 손실 가능성 있음)         | 각 주변 단어를 독립적으로 처리                         |\n",
    "| 🛠 **주요 활용 기법**   | Hierarchical Softmax 등             | Negative Sampling, Hierarchical Softmax 등 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd0970",
   "metadata": {},
   "source": [
    "- 입력 단어는 **one-hot vector** 다. 그래서 단어 index만 1이고 나머진 모두 0으로 구성된다.\n",
    "이  입력 one-hot vector와 가중치 행렬 $W_{in}$이 가중합(행렬곱)을 계산하면 가중치 행렬에서 그 단어 index의 행(one-hot vector의 1의 index의 행)  행을 가져오는 것이 된다. 그래서 word2vec의 hidden layer를 계산하는 작업($X \\cdot W_{in}$)은 **가중치 행렬 $W_{in}$에서 해당 단어에 해당하는 행을 찾는(lookup) 작업**을 하는 것이 된다.<br>\n",
    "> (1,4)@     (4@3)-> 가중치값인데 결국, 1에 해당되는 가중치값이  임베딩 vector가 된다. <br>\n",
    "> 위의 예시에서는 (1,10000)@(10000,100)이고,그 임베딩 vector의 값은 0번 : 가중치에서 가장 첫번째 줄\n",
    "    - ex)\n",
    "\\begin{align} \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    0 & 0& 1 & 0\n",
    "\\end{matrix}\n",
    "\\right] \\cdot \\left[\n",
    "\\begin{matrix}\n",
    "0.1 & 0.1 & 0.1 \\\\\n",
    "0.2 & 0.2 & 0.2 \\\\\n",
    "0.3 & 0.3 & 0.3 \\\\\n",
    "0.4 & 0.4 & 0.4\n",
    "\\end{matrix}\n",
    "\\right] = \\left[\n",
    "\\begin{matrix}\n",
    "0.3 & 0.3 & 0.3\n",
    "\\end{matrix}\n",
    "\\right] \\\\\n",
    "\\text{행: 단어(4), 열: embedding 차원(3)}\n",
    "\\end{align}    \n",
    "- Word2Vec의 학습은 **가중치 행렬의 각 행들이 단어들의 word embedding vector**가 되도록 **주변단어와 중심단어의 관계로 학습**하는 것이다.\n",
    "- 학습이 완료 되면 $W_{in}$ 이나 $W_{out}$ 파라미터를 word embeding vector로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c5b0b",
   "metadata": {},
   "source": [
    "## ✅ 임베딩 백터화 정리\n",
    "\n",
    "다음은 one-hot 벡터와 가중치 행렬 $W_{in}$ 간의 행렬곱으로 임베딩 벡터를 얻는 과정\n",
    "\n",
    "### 📌 입력 벡터 (one-hot)\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "0 & 0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 📌 가중치 행렬 $W_{in}$\n",
    "\n",
    "$$\n",
    "W_{in} =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.1 & 0.1 \\\\\n",
    "0.2 & 0.2 & 0.2 \\\\\n",
    "0.3 & 0.3 & 0.3 \\\\\n",
    "0.4 & 0.4 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 📌 행렬곱 결과 (임베딩 벡터 $h$)\n",
    "\n",
    "$$\n",
    "h = x \\cdot W_{in}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.3 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "즉, 입력된 단어가 **\"cherry\"** (인덱스 2)일 경우,\n",
    "가중치 행렬 $W_{in}$의 2번째 행을 그대로 가져와서\n",
    "**임베딩 벡터로 사용하는 것과 동일합니다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322c161",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
