{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "주요파라미터의 뜻을 써주세요.\n",
    "\n",
    "sentences : [  ] \n",
    "\n",
    "vector_size : [  ]\n",
    "\n",
    "window : [  ]\n",
    "\n",
    "min_count : [  ]\n",
    "\n",
    "sg: 0: [  ] , 1 : [  ]\n",
    "\n",
    "epoch : [  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q : Word2Vec에서 유사도 계산 함수는 무엇을 쓸까요? \n",
    "A : [  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745cdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 문장 \n",
    "sentences = [\"All Gensim source code is hosted on Github under the GNU LGPL license, maintained by its open source community.\",\n",
    "    \"For commercial arrangements, see Business Support.\",\n",
    "    \"Gensim can process arbitrarily large corpora, using data-streamed algorithms.\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1 단계  # 소문자로 모두 변환\n",
    "[doc.[  ]() for doc in sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2 - 1 단계  # 공백도 빈문자열이 됨 \n",
    "import re\n",
    "[re.sub([    ], [     ],  doc.lower()) for doc in sentences] # 공백을 빈문자열로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89394354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2 - 2 단계  # 알파벳, 숫자, _를 제외한 모든 문자들을 제거 \n",
    "[re.sub([      ], \"\",  doc.lower()) for doc in sentences] # 공백은 빼고 제거해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4953468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 단계 # 단어(어절) 단위 토큰화\n",
    "import re \n",
    "import nltk\n",
    "[[    ](re.sub(r\"[^\\w\\s]\", \"\",  doc.lower())) for doc in sentences] # 2차원 형태로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35eec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd83b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "# 위의 문장 리스트를 토큰화해서 Word2Vec 모델을 학습하기 \n",
    "# 벡터사이즈 = 10, 주변 단어 고려 2개, 등장빈도 낮은 단어 제거 기준 1, 학습 반복 횟수 10\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "model1 = Word2Vec(\n",
    "    sentences=tokens, # 학습 시킬 데이터\n",
    "    [    ] = 10, # embedding vector의 차원 (한 개 단어에서 몇 개 feature을 추출할지.)\n",
    "    [    ] = 2,       # window size 설정, 주변 단어의 개수 = 2\n",
    "    [    ] = 1,    # 최소 단어 등장 횟수 1\n",
    "    epochs = 10,\n",
    "    workers = os.cpu_count()        # 병렬처리 개수. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: vector_size를 너무 작게 설정했을 때 발생할 수 있는 문제는? \n",
    "A: [    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94625495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KeyedVectors 조회 \n",
    "[    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding Vector 조회 \n",
    "[    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eececf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 목록 조회 \n",
    "# 인덱스에서 key\n",
    "model1.wv.[    ] # vocab : token_id -> token (단어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daaf2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key에서 인덱스\n",
    "model1.wv.[    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 단어의 embedding vector를 조회\n",
    "model1.wv['commercial'] # 넘파이배열로 학습된 결과를 벡터값으로 알려줌. 모델에다가는 얘를 넣는 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90937b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 key 값, 특정 값 있는지 알고싶다면?\n",
    "'flowers' [    ] model1.wv, 'code' [    ] model1.wv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487dcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 검사 \n",
    "model1.wv.[    ](\"gensim\") # trees 과 가장 유사한 단어를 순서대로 10개를 반환 \n",
    "# (단어, 유사도) 유사도 -1 ~ 1\n",
    "# 어떤 임베딩 벡터가 제일 비슷해? \n",
    "# 코사인 유사도 / l2 distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 가장 비슷한 단어 3개를 출력\n",
    "model1.wv.most_similar(\"code\", [    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어간의 유사도\n",
    "model1.wv.[    ](\"gensim\", \"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1aa489",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q. Skip-gram과 CBOW의 차이를 쓰시오\n",
    "(예시) \"나는 커피를 마신다\"\n",
    "A:\n",
    "- Skip-gram : [    ]\n",
    "- CBOW : [    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q. keyedVectors 저장 시 binary=True 와, binary=False 의 차이를 쓰세요.\n",
    "A:\n",
    "- binary=True : [    ]\n",
    "- binary=False : [    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: 기존 Word2Vec이 동음이의어를 잘 처리하지 못하는 이유를 설명\n",
    "A: [    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: Contextualized Embedding이 같은 'apple'을 어떻게 처리하는지 서술하시오.\n",
    " - A. I bought an apple phone.\n",
    " - B. I ate a red apple.\n",
    "A: [    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: Hidden state 를 설명하세요\n",
    "A: [    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['I', 'bought', 'an' 'apple', 'phone']\n",
    "Q: output과 hidden의 차이\n",
    "A: [    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input data의 shape은\n",
    "(Sequence_legnth, batch_size, feature_shape)이다. \n",
    "\n",
    "default 값은 ? [    ]\n",
    "\n",
    "`batch_first=[    ]`로 설정하면 (**batch_size**, seq_len, feature_shape) 순이 된다.\n",
    "\n",
    "\n",
    "ex)  주가 데이터\n",
    "            - feature: 시가, 종가, 최고가, 최저가 (4개)\n",
    "            - sequence: 100일치( 100개의 feature가 하나의 입력이 된다.)\n",
    "            - batch size: 30 (100일치 데이터 30개)\n",
    "            \n",
    "            - [    ]\n",
    "            - 나중에 feature 4가 embedding size가 됨 \n",
    "            - batch size 가 false 일 땐, [    ]가 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output shape의 shape은 \n",
    "(Sequence length, batch_size, hidden_size * D)이다.\n",
    "\n",
    "D: 양방향(bidirectional) 이면 [2] 아니면 [1]\n",
    "\n",
    "default 값은 ? [    ]\n",
    "\n",
    "batch_first= [    ]로 설정하면 (**batch_size**, seq_len, hidden_size * D) 순이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb32337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8868bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 수 : 1, 단방향\n",
    "rnn1 = nn.RNN(\n",
    "    input_size = 4,         # 개별 timestep의 feature 수 \n",
    "    hidden_size = 256,      # 개별 timestep별로 추출할 feature의 개수. Unit/Node의 개수\n",
    "    num_layers = 1,         # 멀티레이어 RNN -> 쌓을 layer의 개수 \n",
    "    # batch_first = True    # (batch, seq_length, feature)\n",
    "    # nonlinearity = \"relu\" # 활성함수. default: \"tanh\"\n",
    ")\n",
    "\n",
    "output = rnn1(input_data)\n",
    "print(type(output)) #(output_data: 모든 timestep 의 hidden state, hidden_state: 마지막 timestep hiddenstate)\n",
    "out1, hidden1 = rnn1(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: 결과 값을 쓰세요 : \n",
    "# 모든 timestep의 결과\n",
    "print(out1.shape) : [torch.Size([ , , ])]\n",
    "# 마지막 timestep의 결과\n",
    "print(hidden1.shape): [torch.Size([ , , ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 수 : 1, 양방향\n",
    "rnn2= nn.RNN(\n",
    "    input_size = 4,         # 개별 timestep의 feature 수 \n",
    "    hidden_size = 256,      # 개별 timestep별로 추출할 feature의 개수. Unit/Node의 개수\n",
    "    num_layers = 1,         # 멀티레이어 RNN -> 쌓을 layer의 개수 \n",
    "    bidirectional = True    # 양방향 RNN 여부 (default: False)\n",
    ")\n",
    "\n",
    "out2, hidden2 = rnn2(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: 결과 값을 쓰세요 : \n",
    "# 모든 timestep의 결과\n",
    "print(out2.shape) : [torch.Size([ , , ])]\n",
    "# 마지막 timestep의 결과\n",
    "print(hidden2.shape) : [torch.Size([ , , ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39146c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 수 : 3, 단방향\n",
    "rnn3= nn.RNN(\n",
    "    input_size = 4,         # 개별 timestep의 feature 수 \n",
    "    hidden_size = 256,      # 개별 timestep별로 추출할 feature의 개수. Unit/Node의 개수\n",
    "    num_layers = 3,         # 멀티레이어 RNN -> 쌓을 layer의 개수 \n",
    "    bidirectional = False    # 양방향 RNN 여부 (default: False) - 단방향이라 False \n",
    ")\n",
    "\n",
    "out3, hidden3 = rnn3(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: 결과 값을 쓰세요 : \n",
    "# 모든 timestep의 결과\n",
    "print(out3.shape) : [torch.Size([ , , ])]\n",
    "# 마지막 timestep의 결과\n",
    "print(hidden3.shape) : [torch.Size([ , , ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 수 : 3, 양방향\n",
    "rnn4 = nn.RNN(\n",
    "    input_size = 4,         # 개별 timestep의 feature 수 \n",
    "    hidden_size = 256,      # 개별 timestep별로 추출할 feature의 개수. Unit/Node의 개수\n",
    "    num_layers = 3,         # 멀티레이어 RNN -> 쌓을 layer의 개수 \n",
    "    bidirectional = True    # 양방향 RNN 여부 (default: False) - 양방향이라 True\n",
    ")\n",
    "\n",
    "out4, hidden4 = rnn4(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17329b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: 결과 값을 쓰세요 : \n",
    "# 모든 timestep의 결과\n",
    "print(out3.shape) : [torch.Size([ , , ])]\n",
    "# 마지막 timestep의 결과\n",
    "print(hidden3.shape) : [torch.Size([ , , ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1272b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
