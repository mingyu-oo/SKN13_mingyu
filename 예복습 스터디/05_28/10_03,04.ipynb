{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 공통 메소드\n",
    "# 아래 빈칸을 채워 주세요!\n",
    "# [      ] : 형태소 단위로 토큰화\n",
    "# [      ] : 명사만 추출하여 토큰화\n",
    "# [      ] : 품사 부착\n",
    "# [      ] : 형태소 분석기가 사용하는 품사태그 설명하는 속성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b9fbc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Okt' object has no attribute 'morpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m okt = Okt() \u001b[38;5;66;03m# 형태소 분석기 객체 생성 kkma = Kkma()\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 형태소 단위 토큰화\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokens = \u001b[43mokt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmorpus\u001b[49m(txt)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m토큰수:\u001b[39m\u001b[33m\"\u001b[39m, ______(tokens))\n",
      "\u001b[31mAttributeError\u001b[39m: 'Okt' object has no attribute 'morpus'"
     ]
    }
   ],
   "source": [
    "# Konlpy 와 nltk를 이용해 헌법 text 분석하기\n",
    "# _______로 되어있는 부분을 채워주세요!\n",
    "# 형태소 분석기 객체를 생성. 종류별로 클래스가 제공.\n",
    "from konlpy.tag import Okt, Kkma\n",
    "\n",
    "okt = Okt() # 형태소 분석기 객체 생성 kkma = Kkma()\n",
    "# 형태소 단위 토큰화\n",
    "tokens = okt.morpus(txt)\n",
    "\n",
    "print(\"토큰수:\", ______(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = okt.morphs(txt, _______) # ________: 원형복원 - Okt 의 기능.\n",
    "nouns = okt.______(txt) # 명사 토큰만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c041bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사부착\n",
    "pos_tag = okt.______(txt)\n",
    "pos_tag[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKT 기능 - 비속어 처리.\n",
    "sample_txt = \"반갑습니당.\"\n",
    "# sample_txt = \"이것도 되나욬ㅋㅋㅋ\"\n",
    "okt.morphs(sample_txt, ________) # ___________ -> 비속어 처리 -> 토큰화\n",
    "\n",
    "# 비속어를 처리한 문장을 반환.\n",
    "okt.normalize(sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헌법 corpus를 분석\n",
    "from nltk import Text, FreqDist \n",
    "from konlpy.tag import Okt\n",
    "from konlpy.corpus import kolaw\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 1 데이터 로딩\n",
    "with kolaw.open(\"constitution.txt\") as fr:\n",
    "    txt = fr.read()\n",
    "txt[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 생성 \n",
    "okt = __________\n",
    "\n",
    "# 2. 전처리 + 토큰화\n",
    "## 전처리: 명사, 동사만 추출\n",
    "pos_tag = okt.____(txt)\n",
    "# pos_tag[:5]\n",
    "\n",
    "tokens = [token for token, pos in pos_tag if pos in [____, _______] and len(token) > 1]\n",
    "    # 명사, 동사, 2글자 이상상\n",
    "print(len(tokens), tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special token의 종류에 대해서 써주세요.\n",
    "# [      ] : oov 토큰을 표시.\n",
    "# [      ] : 문장의 토큰수를 맞추기 위한 채우는 토큰.\n",
    "# [      ] : 문서의 시작을 표시 + 전체 문서의 의미를 저장하는 토큰을 사용.\n",
    "# [      ] : 문서의 시작.\n",
    "# [      ] : 문서의 끝.\n",
    "# [      ] : 문서가 여러 문장으로 구성 된 경우 문장 구분.\n",
    "# [      ] : 일부 토큰을 가리는 토큰.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216302e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE 방식\n",
    "# 아래 ____을 채워주세요!\n",
    "import time\n",
    "# Tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "# subword 알고리즘을 적용하기 전에 어떻게 나눠놓을 것인지.\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "# Trainer (학습)\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "## subword 알고리즘을 구현한 Tokenizer의 객체를 넣어 생성. (BPE)\n",
    "### unk_token: OOV(Unknow) 단어(토큰)을 처리할 토큰을 지정.\n",
    "tokenizer = Tokenizer(___(unk_token=______))\n",
    "# pre tokenizer를 등록\n",
    "tokenizer.pre_tokenizer = ____________ # 공백을 기준으로 미리 토큰화해 놓는다.\n",
    "\n",
    "# tokenzer를 학습하는 Trainer객체 -> Tokenizer알고리즘 별로 Trainer클래스가 제공됨.\n",
    "# initializer에 어떻게 학습시킬지 설정\n",
    "trainer = ________(\n",
    "    _________=10000,  # 어휘사전의 최대 크기(고유 토큰의 최대 개수.) - 30000\n",
    "    _________=[\"[UNK]\", \"[PAD]\"], \n",
    "    # 어휘사전에 추가할 특수(목적)토큰(Special token)들 지정. 이중 unk_token은 반드시 설정해야 함. \n",
    "    _________=10,   # 사전에 넣을 토큰의 **최소 출연 횟수**(빈도수)\n",
    "    continuing_subword_prefix=\"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31102592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word piece 방식\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "tokenizer2 = Tokenizer(_________(unk_token=_______))\n",
    "# Pre tokenizer \n",
    "tokenizer2.pre_tokenizer = ___________\n",
    "# Trainer 생성\n",
    "trainer = ___________(\n",
    "    _________=20000,\n",
    "    _________=[\"[UNK]\", \"[PAD]\", \"[SEP]\", \"[SOS]\", \"[EOS]\"],\n",
    "    _________=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unihtsm 방식\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "tokenizer3 = Tokenizer(_________())\n",
    "# Pre tokenizer \n",
    "tokenizer3.pre_tokenizer = _________\n",
    "# Trainer 생성\n",
    "trainer = UnigramTrainer(\n",
    "    _________=20000,\n",
    "    _________=[\"[UNK]\", \"[PAD]\", \"[SEP]\", \"[SOS]\", \"[EOS]\"],\n",
    "    _________=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
