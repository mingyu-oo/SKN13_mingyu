
import torch
import time

def test_multi_classification(dataloader, model, loss_fn, device="cpu") -> tuple:
    """
    ë‹¤ì¤‘ ë¶„ë¥˜ ê²€ì¦/í‰ê°€ í•¨ìˆ˜

    Args:
        dataloader: DataLoader - ê²€ì¦í•  ëŒ€ìƒ ë°ì´í„°ë¡œë”
        model: ê²€ì¦í•  ëª¨ë¸
        loss_fn: ëª¨ë¸ ì¶”ì •ê°’ê³¼ ì •ë‹µì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•  loss í•¨ìˆ˜.
        device: str - ì—°ì‚°ì„ ì²˜ë¦¬í•  ì¥ì¹˜. default-"cpu", gpu-"cuda"
    Returns:
        tuple: (loss, accuracy)
    """
    model.to(device)                    # ëª¨ë¸ì„ ì§€ì •ëœ ì¥ì¹˜(cpu/gpu)ë¡œ ì´ë™
    model.eval()                        # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (dropout ë“± ë¹„í™œì„±í™”)
    size = len(dataloader.dataset)      # ì „ì²´ ë°ì´í„° ìˆ˜
    num_steps = len(dataloader)         # ë°°ì¹˜ ìˆ˜

    test_loss, test_accuracy = 0., 0.   # ì´ˆê¸°í™”

    with torch.no_grad():               # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        for X, y in dataloader:         # ë°°ì¹˜ ë‹¨ìœ„ ë°˜ë³µ
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()

            ## ì •í™•ë„ ê³„ì‚°
            pred_label = torch.argmax(pred, axis=-1)    # ì˜ˆì¸¡ í´ë˜ìŠ¤
            test_accuracy += torch.sum(pred_label == y).item()

        test_loss /= num_steps          # í‰ê·  loss
        test_accuracy /= size           # ì „ì²´ ëŒ€ë¹„ ì •í™•ë„

    return test_loss, test_accuracy     # íŠœí”Œ ë°˜í™˜

def test_binary_classification(dataloader, model, loss_fn, device="cpu") -> tuple:
    """
    ì´ì§„ ë¶„ë¥˜ ê²€ì¦/í‰ê°€ í•¨ìˆ˜
    >>>> ë‹¤ì¤‘ë¶„ë¥˜ì™€ ê±°ì˜ ë™ì¼í•˜ë˜, ì •í™•ë„ ê³„ì‚°ë°©ì‹ë§Œ ë‹¤ë¦„

    Args:
        dataloader: DataLoader - ê²€ì¦í•  ëŒ€ìƒ ë°ì´í„°ë¡œë”
        model: ê²€ì¦í•  ëª¨ë¸
        loss_fn: ëª¨ë¸ ì¶”ì •ê°’ê³¼ ì •ë‹µì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•  loss í•¨ìˆ˜.
        device: str - ì—°ì‚°ì„ ì²˜ë¦¬í•  ì¥ì¹˜. default-"cpu", gpu-"cuda"
    Returns:
        tuple: (loss, accuracy)
    """
    model.to(device)
    model.eval()                    # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (dropout ë“± ë¹„í™œì„±í™”)
    size = len(dataloader.dataset)
    num_steps = len(dataloader)

    test_loss, test_accuracy = 0., 0.

    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()

            ## ì •í™•ë„ ê³„ì‚°
            pred_label = (pred >= 0.5).type(torch.int32)  # 0.5 ì´ìƒì´ë©´ 1ë¡œ ë¶„ë¥˜
            test_accuracy += (pred_label == y).sum().item() 

        test_loss /= num_steps
        test_accuracy /= size      #ì „ì²´ ê°œìˆ˜ë¡œ ë‚˜ëˆˆë‹¤.
    return test_loss, test_accuracy    

def train(dataloader, model, loss_fn, optimizer, device="cpu", mode:"binary or multi"='binary'):
    """
    ëª¨ë¸ì„ 1 epoch í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜

    Args:
        dataloader: DataLoader - í•™ìŠµë°ì´í„°ì…‹ì„ ì œê³µí•˜ëŠ” DataLoader
        model - í•™ìŠµëŒ€ìƒ ëª¨ë¸
        loss_fn: ëª¨ë¸ ì¶”ì •ê°’ê³¼ ì •ë‹µì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•  loss í•¨ìˆ˜.
        optimizer - ìµœì í™” í•¨ìˆ˜
        device: str - ì—°ì‚°ì„ ì²˜ë¦¬í•  ì¥ì¹˜. default-"cpu", gpu-"cuda"
        mode: str - ë¶„ë¥˜ ì¢…ë¥˜. binary ë˜ëŠ” multi 

    Returns:
        tuple: í•™ìŠµí›„ ê³„ì‚°í•œ Train setì— ëŒ€í•œ  train_loss, train_accuracy
    """
    model.train()                           # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    size = len(dataloader.dataset)          # ì „ì²´ í•™ìŠµ ë°ì´í„° ìˆ˜(ì´ ë°ì´í„°í¬ì¸íŠ¸ ê°œìˆ˜)

    for X, y in dataloader:                 # ë°°ì¹˜ ë‹¨ìœ„ ë°˜ë³µ
        X, y = X.to(device), y.to(device)   # 1. DEVICEë¡œ ì´ë™.
        pred = model(X)                     # 2. forward(ëª¨ë¸ ì¶”ì •)
        loss = loss_fn(pred, y)             # 3. loss ê³„ì‚°
        optimizer.zero_grad()               # 4. ê¸°ì¡´ gradient ì´ˆê¸°í™”
        loss.backward()                     # 5. ì—­ì „íŒŒë¡œ gradient ê³„ì‚°
        optimizer.step()                    # 6. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

    # í•™ìŠµì´ ëë‚œ í›„ì— ë™ì¼ train ë°ì´í„°ë¥¼ í‰ê°€    
    if mode == 'binary':
        train_loss, train_accuracy = test_binary_classification(dataloader, model, loss_fn, device)
    else:
        train_loss, train_accuracy = test_multi_classification(dataloader, model, loss_fn, device)
    return train_loss, train_accuracy



def fit(train_loader, val_loader, model, loss_fn, optimizer, epochs, save_best_model=True, 
        save_model_path=None, early_stopping=True, patience=10, device='cpu',  mode:"binary or multi"='binary',
        lr_scheduler=None):
    """
    ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜
    >>>>> ì „ì²´ í•™ìŠµ ë£¨í”„(Epoch ë‹¨ìœ„)

    Args:
        train_loader (Dataloader): Train dataloader
        test_loader (Dataloader): validation dataloader
        model (Module): í•™ìŠµì‹œí‚¬ ëª¨ë¸
        loss_fn (_Loss): Loss function
        optimizer (Optimizer): Optimizer
        epochs (int): epochìˆ˜
        save_best_model (bool, optional): í•™ìŠµë„ì¤‘ ì„±ëŠ¥ê°œì„ ì‹œ ëª¨ë¸ ì €ì¥ ì—¬ë¶€. Defaults to True.
        save_model_path (str, optional): save_best_model=Trueì¼ ë•Œ ëª¨ë¸ì €ì¥í•  íŒŒì¼ ê²½ë¡œ. Defaults to None.
        early_stopping (bool, optional): ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€. Defaults to True.
        patience (int, optional): ì¡°ê¸°ì¢…ë£Œ Trueì¼ ë•Œ ì¢…ë£Œì „ì— ì„±ëŠ¥ì´ ê°œì„ ë ì§€ ëª‡ epochê¹Œì§€ ê¸°ë‹¤ë¦´ì§€ epochìˆ˜. Defaults to 10.
        device (str, optional): device. Defaults to 'cpu'.
        mode(str, optinal): ë¶„ë¥˜ ì¢…ë¥˜. "binary(default) or multi
        lr_scheduler: Learning Rate Scheduler ê°ì²´. default: None, Epoch ë‹¨ìœ„ë¡œ LR ë¥¼ ë³€ê²½.

    Returns:
        tuple: ì—í­ ë³„ ì„±ëŠ¥ ë¦¬ìŠ¤íŠ¸. (train_loss_list, train_accuracy_list, validation_loss_list, validataion_accuracy_list)
    """

    train_loss_list = []                # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    train_accuracy_list = []
    val_loss_list = []
    val_accuracy_list = []


    if save_best_model:
        best_score_save = torch.inf     # ìµœê³  ì„±ëŠ¥ ì €ì¥ ì´ˆê¸°ê°’

    ############################
    # early stopping
    #############################
    if early_stopping:
        trigger_count = 0
        best_score_es = torch.inf       # early stopping ê¸°ì¤€ ì ìˆ˜

    # ëª¨ë¸ deviceë¡œ ì˜®ê¸°ê¸°
    model = model.to(device)          # ëª¨ë¸ ì¥ì¹˜ë¡œ ì´ë™
    s = time.time()                   # ì‹œê°„ ì¸¡ì • ì‹œì‘

    for epoch in range(epochs):       # ì—í­ ë°˜ë³µ
        # 1 epoch í•™ìŠµ
        train_loss, train_accuracy = train(train_loader, model, loss_fn, optimizer, device=device, mode=mode)

        ############ 1 epoch í•™ìŠµ ì¢…ë£Œ í›„-> LRë¥¼ ì¡°ì • (ìˆìœ¼ë©´) ########### 
        if lr_scheduler is not None: 
            current_lr = lr_scheduler.get_last_lr()[0]  # logìš©
            lr_scheduler.step()   ###### lr_scheduler.step(step) ì´ë ‡ê²Œ ë‘ë©´ step ë‹¨ìœ„ë¡œ lrì„ ë³€ê²½í•œë‹¤.
            new_lr = lr_scheduler.get_last_lr()[0] # logìš©
            if current_lr != new_lr: # LRê°€ ë³€ê²½ë˜ì—ˆìœ¼ë©´
                print(f">>>>>>Learning Rateê°€ {current_lr}ì—ì„œ {new_lr}ë¡œ ë³€ê²½ë¨<<<<<<")


        # ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€
        if mode == "binary":
            val_loss, val_accuracy = test_binary_classification(val_loader, model, loss_fn, device=device)
        else:
            val_loss, val_accuracy = test_multi_classification(val_loader, model, loss_fn, device=device)

        # ì—í­ë³„ ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
        train_loss_list.append(train_loss)
        train_accuracy_list.append(train_accuracy)
        val_loss_list.append(val_loss)
        val_accuracy_list.append(val_accuracy)

        print(f"Epoch[{epoch+1}/{epochs}] - Train loss: {train_loss:.5f} Train Accucracy: {train_accuracy:.5f} || Validation Loss: {val_loss:.5f} Validation Accuracy: {val_accuracy:.5f}")
        print('='*100)

        # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥
        if save_best_model:
            if val_loss < best_score_save:
                torch.save(model, save_model_path)
                print(f"ì €ì¥: {epoch+1} - ì´ì „ : {best_score_save}, í˜„ì¬: {val_loss}")
                best_score_save = val_loss

        # Early Stopping ì²˜ë¦¬            
        if early_stopping:
            if val_loss < best_score_es: 
                best_score_es = val_loss  
                trigger_count = 0

            else:
                trigger_count += 1                
                if patience == trigger_count:
                    print(f"Early stopping: Epoch - {epoch}")
                    break

    e = time.time()
    print(e-s, "ì´ˆ")
    return train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list

def train_caws(dataloader, model, loss_fn, optimizer, device="cpu", mode:'binary or multi'='binary', lr_scheduler=None, epoch=None):
    model.train()
    size = len(dataloader.dataset)
    steps_per_epoch = len(dataloader)

    for batch_idx, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)
        pred = model(X)
        loss = loss_fn(pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ğŸ¯ schedulerë¥¼ step ë‹¨ìœ„ë¡œ í˜¸ì¶œ
        if lr_scheduler is not None and epoch is not None:
            fractional_epoch = epoch + batch_idx / steps_per_epoch
            lr_scheduler.step(fractional_epoch)

    # í‰ê°€
    if mode == 'binary':
        train_loss, train_accuracy = test_binary_classification(dataloader, model, loss_fn, device)
    else:
        train_loss, train_accuracy = test_multi_classification(dataloader, model, loss_fn, device)
    return train_loss, train_accuracy


def fit_caws(train_loader, val_loader, model, loss_fn, optimizer, epochs,
             save_best_model=True, save_model_path=None, early_stopping=True,
             patience=10, device='cpu', mode="binary", lr_scheduler=None):
    """
    CosineAnnealingWarmRestartsë¥¼ step ë‹¨ìœ„ë¡œ ì ìš©í•˜ëŠ” í•™ìŠµ ë£¨í‹´.
    train_caws()ë¥¼ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ë©°, scheduler.step(fractional_epoch)ë¥¼ stepë§ˆë‹¤ í˜¸ì¶œí•¨.
    """
    train_loss_list = []
    train_accuracy_list = []
    val_loss_list = []
    val_accuracy_list = []

    if save_best_model:
        best_score_save = torch.inf

    if early_stopping:
        trigger_count = 0
        best_score_es = torch.inf

    model = model.to(device)
    import time
    s = time.time()

    for epoch in range(epochs):
        # ğŸ” CosineAnnealingWarmRestartsë¥¼ step ë‹¨ìœ„ë¡œ ì ìš©í•˜ëŠ” train_caws() í˜¸ì¶œ
        train_loss, train_accuracy = train_caws(
            train_loader, model, loss_fn, optimizer,
            device=device, mode=mode,
            lr_scheduler=lr_scheduler, epoch=epoch
        )

        # âœ… scheduler.step() í˜¸ì¶œ ì œê±°ë¨

        # ê²€ì¦
        if mode == "binary":
            val_loss, val_accuracy = test_binary_classification(val_loader, model, loss_fn, device=device)
        else:
            val_loss, val_accuracy = test_multi_classification(val_loader, model, loss_fn, device=device)

        # ë¡œê·¸ ë° ì €ì¥
        train_loss_list.append(train_loss)
        train_accuracy_list.append(train_accuracy)
        val_loss_list.append(val_loss)
        val_accuracy_list.append(val_accuracy)

        print(f"Epoch[{epoch+1}/{epochs}] - Train loss: {train_loss:.5f} Train Acc: {train_accuracy:.5f} || Val loss: {val_loss:.5f} Val Acc: {val_accuracy:.5f}")
        print('='*100)

        if save_best_model and val_loss < best_score_save:
            torch.save(model, save_model_path)
            print(f"ì €ì¥: {epoch+1} - ì´ì „: {best_score_save:.5f}, í˜„ì¬: {val_loss:.5f}")
            best_score_save = val_loss

        if early_stopping:
            if val_loss < best_score_es:
                best_score_es = val_loss
                trigger_count = 0
            else:
                trigger_count += 1
                if trigger_count >= patience:
                    print(f"Early stopping: Epoch - {epoch}")
                    break

    e = time.time()
    print(e - s, "ì´ˆ")
    return train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list

