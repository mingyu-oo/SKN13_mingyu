{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abaeb157",
   "metadata": {},
   "source": [
    "## 🔁 GRU(Gated Recurrent Unit) 구조와 동작 원리\n",
    "\n",
    "### ✅ LSTM의 장점을 가져온 GRU\n",
    "\n",
    "GRU(Gated Recurrent Unit)는 **LSTM의 장점을 유지하면서 더 단순한 구조**로 만든 순환 신경망(RNN) <br>\n",
    "LSTM의 핵심 기능인 **장기 기억 유지**는 그대로 가져오되, 내부 구조를 더 간소화해서 **연산량을 줄이고 학습 속도를 개선**\n",
    "\n",
    "\n",
    "### 🔍 GRU의 핵심 특징\n",
    "\n",
    "- LSTM에서 사용되던 3개의 게이트 (Forget, Input, Output)를  \n",
    "  → 2개의 게이트 (Update, Reset)로 축소\n",
    "- LSTM의 **Cell State** $C_t$를 제거하고,  \n",
    "  → 하나의 **Hidden State** $h_t$가 모든 기억을 담당\n",
    "- 구조는 단순하지만, 성능은 LSTM에 근접하거나 유사\n",
    "\n",
    "\n",
    "### 📦 GRU의 구조 개요\n",
    "\n",
    "- GRU는 은닉 상태 $h_{t-1}$에서 다음 상태 $h_t$로 연결될 때  \n",
    "  **두 갈래로 분기되었다가 다시 합쳐지는 구조(장기기억과 단기기억)** 를 가짐! (LSTM이랑 비슷)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984124d4",
   "metadata": {},
   "source": [
    "![gru_cell](figures/rnn/23_gru_cell.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de16c15",
   "metadata": {},
   "source": [
    "###  🧠 GRU (Gated Recurrent Unit) 계산 흐름\n",
    "##### ✅ Step 0. 입력 정보\n",
    "- $x_t$: 현재 시점의 입력 벡터 (예: 단어 임베딩)\n",
    "- $h_{t-1}$: 이전 시점의 은닉 상태 (과거 정보의 요약)\n",
    "#### 🧩 Step 1. Reset Gate 계산\n",
    "**→ 과거 정보를 얼마나 '초기화'할지 결정**\n",
    "\n",
    "$r_t = \\sigma(W_r x_t + U_r h_{t-1})$\n",
    "- $\\sigma$: sigmoid 함수 (출력값 0~1)\n",
    "- $W_r$: 입력 $x_t$에 대한 가중치\n",
    "- $U_r$: 이전 상태 $h_{t-1}$에 대한 가중치\n",
    "- 👉 $r_t$가 작을수록 과거 정보를 무시하고 새 정보를 더 반영\n",
    "\n",
    "🧠 **의도**: 문맥이 바뀔 때 이전 기억을 지워야 할 경우 사용됨\n",
    "\n",
    "#### 🧩 Step 2. Update Gate 계산\n",
    "**→ 현재 상태에 과거를 얼마나 유지할지 결정**\n",
    "\n",
    "$z_t = \\sigma(W_z x_t + U_z h_{t-1})$\n",
    "- $z_t \\approx 1$: 이전 상태 $h_{t-1}$를 거의 그대로 유지  \n",
    "- $z_t \\approx 0$: 새로 계산된 정보를 더 반영\n",
    "\n",
    "🧠 **의도**: 장기 기억이 필요한 경우, 과거 상태를 그대로 유지\n",
    "\n",
    "#### 🧪 Step 3. Candidate Hidden State 계산\n",
    "**→ 현재 입력과 선택된 과거 정보로 새로운 상태 후보 생성**\n",
    "\n",
    "$\\tilde{h}_t = \\tanh(W x_t + U (r_t \\cdot h_{t-1}))$\n",
    "- $r_t \\cdot h_{t-1}$: reset gate로 조절된 과거 정보\n",
    "- $\\tanh$: 비선형 활성화 함수 (출력 범위 -1 ~ 1)\n",
    "\n",
    "🧠 **의도**: 현재 입력과 일부 과거 정보를 이용해 새 정보를 생성\n",
    "\n",
    "#### ⚙️ Step 4. 최종 Hidden State 계산\n",
    "**→ 새 정보와 과거 정보를 비율에 따라 혼합**\n",
    "\n",
    "$h_t = z_t \\cdot h_{t-1} + (1 - z_t) \\cdot \\tilde{h}_t$\n",
    "- $z_t$: 이전 상태를 유지할 정도\n",
    "- $1 - z_t$: 새로운 상태 후보를 반영할 정도\n",
    "\n",
    "🧠 **의도**: 필요에 따라 '기억 유지' 또는 '기억 갱신'을 자동으로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795ee38",
   "metadata": {},
   "source": [
    " ### 📘Pytorch GRU\n",
    "- `nn.GRU` 클래스 이용\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "- **입력**\n",
    "    - **input**: (seq_length, batch, hidden_size) shape의 tensor. (batch_first=False), batch_first=True이면 `seq_length`와 `batch` 위치가 바뀐다.\n",
    "    - **hidden**: (D * num_layers, batch, hidden_size) shape의 Tensor. D(양방향:2, 단방향:1), hidden은 생략하면 0이 입력됨.\n",
    "- **출력** - output과 hidden state가 반환된다.\n",
    "    - **output**\n",
    "        - 모든 sequence의 처리결과들을 모아서 제공.\n",
    "        - shape: (seq_length, batch, D * hidden_size) : D(양방향:2, 단방향:1), batch_first=True이면 `seq_length`와 `batch` 위치가 바뀐다.\n",
    "    - **hidden**\n",
    "        - 마지막 time step 처리결과\n",
    "        - shape: (D * num_layers, batch, hidden) : D(양방향:2, 단방향:1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402a5c6",
   "metadata": {},
   "source": [
    "📝 다음 코드의 빈칸을 채우고 주석을 참고하여 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b15627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU 입출력  확인\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# dummy data -> (seq len-20, batch-2, 개별 timestep의 입력 feature수-10으로 설정해주세요!)\n",
    "input_data = torch.randn((20,2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce34fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 256])\n",
      "torch.Size([1, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 단방향, layer 수 : 1\n",
    "gru1 = nn.GRU(  # GRU 계층을 입력하세요!\n",
    "    input_size=10,   # 개별 timestep의 feature 수(embedding.dim)\n",
    "    hidden_size=256, # 각 timestep별로 256개의 특성을 추출(unit)\n",
    "    num_layers=1, # 몇층(layer)를 쌓을 지. (default: 1)\n",
    "    bidirectional=False # 양방향 여부 (default: False)\n",
    ")\n",
    "out1, hidden1 = gru1(input_data)\n",
    "#모든 timestep의 hidden state값을 묶어서 반환.[20:seq len, 2:batch, 256:hidden_size] \n",
    "# 256개의 값으로 구성되어 있는 20개의 seq 값.\n",
    "print(out1.shape) \n",
    "# 마지막 timestep 처리 hidden state값 [1: seq len, 2, 256]\n",
    "print(hidden1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71ff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 512])\n",
      "torch.Size([2, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 양방향, layer 수: 1\n",
    "gru2 = nn.GRU(  # GRU 계층을 입력하세요!\n",
    "    input_size=10,\n",
    "    hidden_size=256, \n",
    "    num_layers=1, # 몇층(layer)를 쌓을 지. (default: 1)\n",
    "    bidirectional=True# 양방향 여부 (default: False)\n",
    ")\n",
    "out2, hidden2 = gru2(input_data)\n",
    "\n",
    "# [20: seq_len, 2: batch, 512:hidden_size * 2]  양방향(정/역방향) hidden state를 합쳐서(concat) 반환.\n",
    "print(out2.shape)\n",
    "# [2:정/역방향 두개, 2:batch, 256:hidden size]\n",
    "print(hidden2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f440ce",
   "metadata": {},
   "source": [
    "# Seq2Seq 를 이용한 Chatbot 모델 구현\n",
    "- Encoder를 이용해 질문의 특성을 추출하고 Decoder를 이용해 답변을 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd6b28",
   "metadata": {},
   "source": [
    "### 📘데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ce25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests 모듈로 받기\n",
    "import requests\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/refs/heads/master/ChatbotData.csv\"\n",
    "res = requests.get(url)\n",
    "if res.status_code == 200:\n",
    "    with open(\"data/chatbot_data.csv\", \"wt\", encoding=\"utf-8\") as fw:\n",
    "        fw.write(res.text)\n",
    "else:\n",
    "    print(f\"불러오지 못함: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed997b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/chatbot_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 제거\n",
    "df.drop(columns='label', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22770231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A\n",
       "0           12시 땡!   하루가 또 가네요.\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "4          PPL 심하네   눈살이 찌푸려지죠."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877defd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q    0\n",
       "A    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.isna().sum()  # 결측치 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1dacac",
   "metadata": {},
   "source": [
    "### 📘Dataset, DataLoader 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e64f82",
   "metadata": {},
   "source": [
    "### 📘 Tokenization\n",
    "\n",
    "### Subword방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873e859",
   "metadata": {},
   "source": [
    "📝 다음 코드의 빈칸을 채우고 주석을 참고하여 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq에서는 정규화 같은 전처리가 필요하지 않다. 부정과 긍정을 판단하는 RSTM 같은 것들은 전처리나 클랜징을 필요로 함. \n",
    "# token 학습 -> vocab 사전 생성.\n",
    "## 질문들 + 답변들 합쳐서 학습.\n",
    "question_texts = df['Q']\n",
    "answer_texts = df['A']\n",
    "all_texts = list(question_texts + \" \"+answer_texts) # 같은 index끼리 합치기 => list로 변환\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe85a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27524257",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a60367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2325f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10_000   # 총 어휘 수\n",
    "min_frequency = 5   # 어휘사전에 등록될 단어(토큰)의 최소 빈도수.\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))  # unk 처리\n",
    "tokenizer.pre_tokenizer = Whitespace()  # 음절 단위로 쪼갬.\n",
    "trainer = BpeTrainer(      \n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    continuing_subword_prefix='##', # 연결 subword 앞에 붙일 접두어지정. \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\"] # [SOS]: 문장의 시작을 의미하는 토큰.\n",
    ")\n",
    "# tokenizer: token + ##izer\n",
    "## 학습\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer) # 리스트로 부터 학습\n",
    "## tokenizer.train(\"파일경로\") # 파일에 있는 text를 학습."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"총 어휘수:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc43c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "encode = tokenizer.encode(\"오늘 날씨가 너무 좋습니다. 즐거운 하루 되세요. 쿄쿄쿜ㅋ\")    # id와 토큰을 생성해줌. \n",
    "print(encode.ids)   # 토큰 id\n",
    "print(encode.tokens)  # 토큰 (단어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b83b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(1296), tokenizer.token_to_id('##삿')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76d751",
   "metadata": {},
   "source": [
    "### 📘 Tokenizer 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da495989",
   "metadata": {},
   "source": [
    "### 📘 Dataset, DataLoader 정의\n",
    "\n",
    "\n",
    "### Dataset 정의 및 생성\n",
    "- 모든 문장의 토큰 수는 동일하게 맞춰준다.\n",
    "    - DataLoader는 batch 를 구성할 때 batch에 포함되는 데이터들의 shape이 같아야 한다. 그래야 하나의 batch로 묶을 수 있다.\n",
    "    - 문장의 최대 길이를 정해주고 **최대 길이보다 짧은 문장은 `<PAD>` 토큰을 추가**하고 **최대길이보다 긴 문장은 최대 길이에 맞춰 짤라준다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c5752",
   "metadata": {},
   "source": [
    "📝 다음 코드의 빈칸을 채우고 주석을 참고하여 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9660a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ChatbotDataset\n",
    "    parameter:\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "        max_length: 개별 문장의 token 개수. 모든 문장의 토큰수를 max_length에 맞춘다.\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int 총단어수\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, question_texts, answer_texts, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        parameter\n",
    "            question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "            answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "            max_length: 개별 문장의 token 개수. 모든 문장의 토큰수를 max_length에 맞춘다.\n",
    "            tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.question_texts = [self.__process_sequence(q) for q in question_texts]\n",
    "        self.answer_texts = [self.__process_sequence(a) for a in answer_texts]  # 받아온 q/a 문장 리스트를 max_length의 token list로 변환해서 attribute로 저장.\n",
    "    \n",
    "    def __pad_token_sequence(self, token_sequence): \n",
    "        \"\"\"\n",
    "        max_length 길이에 맞춰 token_id 리스트를 구성한다.\n",
    "        max_length 보다 길면 뒤에를 자르고 max_length 보다 짧으면 [PAD] 토큰을 추가한다.\n",
    "        \n",
    "        Parameter\n",
    "            token_sentence: list[int] - 길이를 맞출 한 문장 token_id 목록\n",
    "        Return\n",
    "            list[int] - length가 max_length인 token_id 목록\n",
    "        \"\"\"\n",
    "        pad_token = self.tokenizer.token_to_id('[PAD]')   # [PAD] 토큰 id를 조회 (0)\n",
    "        seq_len = len(token_sequence) # 입력 문장의 토큰수\n",
    "        if seq_len > self.max_length: # 문장 최대 토큰수 보다 길다면.\n",
    "            return token_sequence[:self.max_length]\n",
    "        else:\n",
    "            return token_sequence + ([pad_token] * (self.max_length - seq_len))\n",
    "    \n",
    "    def __process_sequence(self, text): \n",
    "        \"\"\"\n",
    "        한 문장(str)을 받아서 padding이 추가된 token_id 리스트로 변환 후 반환\n",
    "        Parameter\n",
    "            text: str - token_id 리스트로 변환할 한 문장\n",
    "        Return\n",
    "            list[int] - 입력받은 문장에 대한 token_id 리스트\n",
    "        \"\"\"\n",
    "        # encoding\n",
    "        encode = self.tokenizer.encode(text) # \"........\" => [. , . , .]\n",
    "        # max_length 크기에 맞춘다.\n",
    "        token_ids = self.__pad_token_sequence(encode.ids) #[3400, 20, 6, 0, 0, 0 ..]\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_texts)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # return  index의 (질문토큰들,  답변토큰들)\n",
    "        q = self.question_texts[index]  # List\n",
    "        a = self.answer_texts[index]\n",
    "        # List->LongTensor. nn.Embedding()의 입력(정수타입)으로 들어간다. \n",
    "        return torch.tensor(q, dtype=torch.int64), torch.tensor(a, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61dc8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적당한 max_length 값: 전체 문장 총 토큰수의 9분위수\n",
    "import numpy as np\n",
    "a = [len(tokenizer.encode(s).ids) for s in all_texts]\n",
    "# a[:5]\n",
    "np.quantile(a, q=[0.9, 0.95, 0.97, 1.0])\n",
    "# max_length=20\n",
    "a  # 각 문장에 대한 토큰 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9913d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(a, 30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Dataset 셍상\n",
    "MAX_LENGTH = 20\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, MAX_LENGTH, tokenizer)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]  \n",
    "# (tensor([  10, 1815, 1348,  368,    3,    0,    0,    0,    0,    0,    0,    0,\n",
    "#            0,    0,    0,    0,    0,    0,    0,    0]),  -> 질문 \n",
    "# tensor([6118,  378,   47, 2252,    8,    0,    0,    0,    0,    0,    0,    0,\n",
    "#            0,    0,    0,    0,    0,    0,    0,    0])) -> 답"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2062d7",
   "metadata": {},
   "source": [
    "### 📘 Trainset / Testset 나누기\n",
    "train : test = .95 : .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(dataset) * 0.95)\n",
    "len(dataset) - int(len(dataset)*0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset)*0.95)   # trainset의 개수\n",
    "test_size = len(dataset) - train_size  # testset의 개수\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f69526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_split()이용해서 분리\n",
    "###shuffle(섞는다) 후 개수에 맞게 나눔  -> 순서대로 나누지 않음.\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size]) \n",
    "# random_split(Dataset객체, [나눌 개수, ...])\n",
    "# ex. random_split(dataset, [10,20,30,40,50]) # 5개로 나눔. 각각 지정한 개수별로."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc004437",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset), type(train_set)  # subset (부분 집합)의 형태로 나옴. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5592260",
   "metadata": {},
   "source": [
    "### 📘 DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)  #drop_last= True 남은거는 쓰지 않겠다.\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a85bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader) # step 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db7917",
   "metadata": {},
   "source": [
    "### 📘 모델 정의\n",
    "\n",
    "## Seq2Seq 모델 정의\n",
    "- Seq2Seq 모델은 Encoder와 Decoder의 입력 Sequence의 **길이**와 **순서**가 자유롭기 때문에 챗봇이나 번역에 이상적인 구조다.\n",
    "    - 단일 RNN은 각 timestep 마다 입력과 출력이 있기 때문에 입/출력 sequence의 개수가 같아야 한다.(ex. many to many)\n",
    "    - 챗봇의 질문/답변이나 번역의 대상/결과 문장의 경우는 사용하는 어절 수가 다른 경우가 많기 때문에 단일 RNN 모델은 좋은 성능을 내기 어렵다.\n",
    "    - Seq2Seq는 **입력처리(질문,번역대상)처리 RNN과 출력 처리(답변, 번역결과) RNN 이 각각 만들고 그 둘을 연결한 형태로 길이가 다르더라도 상관없다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a9386",
   "metadata": {},
   "source": [
    "## 📘 Encoder\n",
    "Encoder는 하나의 Vector를 생성하며 그 Vector는 **입력 문장의 의미**를 N 차원 공간 저장하고 있다. 이 Vector를 **Context Vector** 라고 한다.<br> \n",
    "LSTM과 동일(Many to one) -> LSTM에서 특징 추출해내는 거랑 똑같이 여기서도 입력 문장의 context의 특징을 추출해내는 것\n",
    "![encoder](figures/seq2seq_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c662",
   "metadata": {},
   "source": [
    "📝 다음 코드의 빈칸을 채우고 주석을 참고하여 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, \n",
    "                hidden_size, bidirectional=True, num_layers=1, dropout_rate=0.0):  #bidirectional : 양방향성.  문장 token에 대해서 왼쪽에서 뽑아낸거랑 오른른쪽에서 시작해서 뽑아낸거 둘다 고려.\n",
    "        super().__init__()\n",
    "        # Encoder는 context vector(문장의 feature)를 생성하는 것이 목적 (분류기는 생성안함.) 여기서는 그냥 특징을 decoder에 주는 역할\n",
    "        # Embedding Layer, GRU Layer를 생성.\n",
    "        self.vocab_size = vocab_size # 어휘사전의 총 어휘수(토큰수)\n",
    "        # 임베딩레이어\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\t\t# 총 어휘개수 (weight 행렬의 행)\n",
    "            embedding_dim,\t# embedding vector 차원수. (Weight 행렬의 열 수) weight 행렬의 shape: [vocab_size, embedding_dim]\n",
    "            padding_idx=0   # [PAD]  (패딩 토큰의 ID) - padding의 embedding vector는 학습이 안되도록 한다.(vector값이 0으로 구성)\n",
    "        )\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim,\t\t\t\t# 개별 토큰(time step)의 크기(feature 수).\n",
    "            hidden_size=hidden_size,\t# hidden state의 크기- 개별 토큰 별로 몇개의 feature를 추출할지. \n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0.0  # stacked rnn일 경우(layer가 여러개일 경우), dropout 적용.  \n",
    "        )\n",
    "    \n",
    "    def forward(self, X):   # 계산\n",
    "        # X shape: (batch, seq_len) 토큰값 하나씩\n",
    "        X = self.embedding(X) # (batch, seq_len, embedding_dim)\n",
    "        X = X.transpose(1, 0) # (seq_len, batch, embedding_dim)\n",
    "        out, hidden = self.gru(X)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2bb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding_idx 예시\n",
    "e_l = nn.Embedding(10,5, padding_idx=1)\n",
    "e_l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "encoder_model = Encoder(1000, 200, 256)   # vocab_size : 1000, emb차원: 200, hidden size: 256\n",
    "dummy_data = torch.zeros((64, 20), dtype=torch.int64)  #(batch:64, seq_len:20)\n",
    "summary(encoder_model, input_data=dummy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd2d3e",
   "metadata": {},
   "source": [
    "## 📘Decoder\n",
    "- Encoder의 출력(context vector)를 받아서 번역 결과 sequence를 출력한다.\n",
    "- Decoder는 매 time step의 입력으로 **이전 time step에서 예상한 단어와 hidden state값이** 입력된다.\n",
    "- Decoder의 처리결과 hidden state를 Estimator(Linear+Softmax)로 입력하여 **입력 단어에 대한 번역 단어가 출력된다.** (이 출력단어가 다음 step의 입력이 된다.)\n",
    "    - Decoder의 첫 time step 입력은 문장의 시작을 의미하는 <SOS>(start of string) 토큰이고 hidden state는 context vector(encoder 마지막 hidden state) 이다.\n",
    "\n",
    "![decoder](figures/seq2seq_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574585f",
   "metadata": {},
   "source": [
    "📝 다음 코드의 빈칸을 채우고 주석을 참고하여 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80810ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # auto regressive RNN 모델은 단방향만 가능\n",
    "    def __init__(self, vocab_size, embedding_dim, \n",
    "                 hidden_size, num_layers=1, bidirectional=False, dropout_rate=0.0):  #여기서는 역방향을 할 수 없음. 앞에서 생성이 되어야 하는데, 뒤로 하면 생성 안됨. \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size # 총 어휘사전 토큰 개수.\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # GRU \n",
    "        ## Auto Regressive RNN은 단방향만 가능. \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, \n",
    "                          num_layers=num_layers, dropout=dropout_rate if num_layers > 1 else 0.0)\n",
    "        # Dropout layer (feature랑 분류기 사이에 dropout layer 넣기)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 분류기 (다음 단어(토큰)를 추론)\n",
    "           # - 다중분류(단어사전의 단어들의 다음 단어일 확를)\n",
    "        self.lr = nn.Linear(\n",
    "            hidden_size,  # GRU 출력 값 중 마지막 hidden state값을 입력으로 받음.  # ev -> --- -> hidden state -> linnear에 넣기\n",
    "            vocab_size    # 출력: 다음 단어일 확률.   \n",
    "        )\n",
    "        \n",
    "    def forward(self, X, hidden):\n",
    "        # X: torch.LongTensor: shape - [batch] : 한 단어씩 입력을 받음.\n",
    "        # hidden: torch.FloatTensor: shape - [1, batch, hidden_size] (이전까지의 특성)   # sequence_length는 1이 됨 (단어가 한개)\n",
    "        \n",
    "        X = X.unsqueeze(1) # seq_len 축을 추가. [batch] -> [batch, 1] (Embedding Layer의 input shape)  1: sequance_length\n",
    "        X = self.embedding(X) # [batch, 1, embedding 차원]\n",
    "        X = X.transpose(1, 0) # [1, batch, embedding 차원]  #seq_len이랑 batch 축 바꿈. \n",
    "        \n",
    "        out, hidden = self.gru(X, hidden)\n",
    "        last_out = out[-1] # out: 전체 hidden state값-> 마지막 hidden state을 추출  # 근데 어차피 seq_len이 1이니까 한개임.\n",
    "        self.dropout(last_out)   # 과적합을 막아주기 위해서 dropout 진행\n",
    "        last_out = self.lr(last_out)\n",
    "\n",
    "        #last_out : 어휘 사전의 단어들에 대해 다음 단어일 확률. \n",
    "        return last_out, hidden # (hidden: 다음 timestep에 전달.)  # hidden도 같이 다음 꺼에 넣어야 함,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb315c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### summary\n",
    "\n",
    "decoder_model = Decoder(1000, 200, 256)\n",
    "\n",
    "dummy_input = torch.ones((64, ), dtype=torch.int64)\n",
    "dummy_hidden = torch.ones((1, 64, 256), dtype=torch.float32)\n",
    "\n",
    "summary(decoder_model, input_data=(dummy_input, dummy_hidden))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
