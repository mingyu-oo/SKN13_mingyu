{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55193d8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "\t- 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "\t- 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "\t- Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8677d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(\n",
    "\tnum_embeddings=20_000,\t# vocab size (단어 사전의 단어 수) -> 총 몇개의 단어에 대한 embedding vector를 만들지 정해줌.\n",
    "\tembedding_dim=200,\t\t# embedding vector의 차원수 -> 개별 단어를 몇개의 숫자(feature)로 표현할지.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92dcfee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 1.1805,  0.8355, -0.2973,  ...,  2.3843,  0.1475,  0.8621],\n",
       "         [ 0.6521, -0.6876,  0.3673,  ...,  0.2631,  0.5408,  0.2684],\n",
       "         [-0.9871,  0.5435,  0.1098,  ..., -0.0611,  1.1731, -0.6244],\n",
       "         ...,\n",
       "         [ 0.1002, -0.4966, -1.3286,  ..., -2.1819,  0.8691, -1.3715],\n",
       "         [-0.8155, -0.8340, -1.0334,  ..., -0.3444, -1.3056,  1.3306],\n",
       "         [-0.9277, -1.1484,  0.0081,  ...,  1.5903,  0.8394, -0.6785]],\n",
       "        requires_grad=True),\n",
       " torch.Size([20000, 200]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.weight, embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eafacf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 200])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding layer의 입력 : 문서를 구성하는 토큰들의 ID(int)를 1차원으로 묶어서 전달.\n",
    "\n",
    "# doc = 나는:30|어제:159|밥을:9000|먹었다:326\n",
    "doc = torch.tensor([[30, 158, 9000, 326],[30, 158, 9000, 326],[30, 158, 9000, 326]], dtype=torch.int64)\n",
    "embedding_vector = embed(doc)\n",
    "embedding_vector.shape\n",
    "\n",
    "# [3 : batch_size, 4 : seq_len, 200 : embedding vector 차원 수]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5441a",
   "metadata": {},
   "source": [
    "## 전처리된 dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8c4a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle로 저장한 전처리한 dataset 읽어오기\n",
    "with open(\"datasets/nsmc/preprocessing_trainset.pkl\", \"rb\") as fr:\n",
    "\ttrain_dict = pickle.load(fr)\n",
    "\n",
    "with open(\"datasets/nsmc/preprocessing_testset.pkl\", \"rb\") as fr:\n",
    "\ttest_dict = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d8d5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_dict[\"input\"]\n",
    "train_labels = train_dict[\"output\"]\n",
    "test_inputs = test_dict[\"input\"]\n",
    "test_labels = test_dict[\"output\"]\n",
    "\n",
    "all_inputs = train_inputs + test_inputs\t\t# vocab 만들 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007cab1",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "- Subword 방식 토큰화 적용\n",
    "- Byte Pair Encoding 방식으로 huggingface tokenizer 사용\n",
    "\t- BPE: 토큰을 글자 단위로 나눈뒤 가장 자주 등장하는 글자 쌍(byte paire)를 찾아 합친뒤 어휘사전에 추가한다.\n",
    "\t- https://huggingface.co/docs/tokenizers/quicktour\n",
    "\t- `pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60c1e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30_000\t\t# vocab의 최대 단어 수\n",
    "min_frequency = 5\t\t# 사전에 추가할 최소 빈도수\n",
    "tokenizer = Tokenizer(\n",
    "\tBPE(unk_token=\"[UNK]\")\n",
    ")\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "\tvocab_size=vocab_size,\n",
    "\tmin_frequency=min_frequency,\n",
    "\tspecial_tokens=[\"[UNK]\", \"[PAD]\"],\n",
    "\tcontinuing_subword_prefix=\"##\"\n",
    "\t# 단어의 중간에 나오는 subword일 경우 앞에 ## 붙여주기\n",
    "\t# \"시작하는\" -> \"시작\", \"하는\" => \"시작\",\"##하는\"\n",
    ")\n",
    "tokenizer.train_from_iterator(all_inputs, trainer=trainer)\t# vocab 생성 == tokenizer학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a95d5e",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "\tdef __init__(self, texts, labels, max_length, tokenizer):\n",
    "\t\t\"\"\"\n",
    "\t\ttexts: list - 댓글 목록. 리스트에 댓글들을 담아서 받는다. [\"댓글\", \"댓글\", ...]\n",
    "\t\tlabels: list - 댓글 감정 목록. \n",
    "\t\tmax_length: 개별 댓글의 최대 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.(Sequence 개수를 맞춘다)\n",
    "\t\ttokenizer: Tokenizer\n",
    "\t\t\"\"\"\n",
    "\t\tself.max_length = max_length\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.labels = labels\n",
    "\t\t# self.texts : 입력 댓글 - token id로 변환된 댓글(문서), 글자 수는 max_length에 맞춤\n",
    "\t\t#\t\t\t   max_length 보다 적으면 [PAD] 추가, max_length보다 많으면 잘라냄\n",
    "\t\tself.texts = [self.__pad_token_sequences(tokenizer.encode(txt).ids) for txt in texts]\n",
    "\n",
    "\t###########################################################################################\n",
    "\t# id로 구성된 개별 문장 tokenizer list를 받아서 패딩 추가 [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "\t# max_length에 token list의 개수를 맞춰주는 func\n",
    "\t############################################################################################\n",
    "\tdef __pad_token_sequences(self, token_sequences):\n",
    "\t\t\"\"\"\n",
    "\t\tid로 구성된 개별 문서(댓글)의 token_id list를 받아서 max_length 길이에 맞추는 메소드\n",
    "\t\tmax_length 보다 토큰수가 적으면 [PAD] 추가, 많으면 max_length 크기로 줄인다.\n",
    "\t\t\tex) max_length = 5, [PAD] token id가 0\n",
    "\t\t\t- [20, 2, 1] => [20, 2, 1, 0, 0]\n",
    "\t\t\t- [20, 30, 40, 50, 60, 70, 80][:5] -> [20, 30, 40, 50, 60]\n",
    "\t\t\"\"\"\n",
    "\t\tpad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
    "\t\tseq_len = len(token_sequences)\t# 입력받은 토큰 개수.\n",
    "\t\tresult = None\n",
    "\t\tif seq_len > self.max_length:\t\t# 잘라내기\n",
    "\t\t\tresult = token_sequences[:?]\t# self.max_length까지 잘라낼려면 ? \t\t(정답 : self.max_length)\n",
    "\t\telse:\n",
    "\t\t\tresult = token_sequences + ([pad_token_id] * (self.max_length - seq_len))\n",
    "\t\treturn result\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\t\t# 총 data개수 반환\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t\"\"\"\n",
    "\t\tidx 번째 text와 label을 학습 가능한 type으로 변환해서 반환\n",
    "\t\tParameter\n",
    "\t\t\tidx: int 조회할 index\n",
    "\t\tReturn\n",
    "\t\t\ttuple: (torch.LongTensor, torch.FloatTensor) - 댓글 토큰_id 리스트, 정답 Label\n",
    "\t\t\"\"\"\n",
    "\t\ttxt = self.texts[idx]\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\treturn (torch.tensor(txt, dtype=torch.int64), torch.tensor([label], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac39c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "trainset = NSMCDataset(train_inputs, train_labels, MAX_LENGTH, tokenizer)\n",
    "testset = NSMCDataset(test_inputs, test_labels, MAX_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ec20d",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f47aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model def\n",
    "class NSMCClassifier(nn.Module):\n",
    "\n",
    "\tdef __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, bidireational=True, dropout_rate=0.2):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tvocab_size(int) : 어휘사전의 총 어휘수\n",
    "\t\t\tembedding_dim(int) : (word) embedding vector의 차원수\n",
    "\t\t\thidden_size(int) : LSTM의 hidden state의 feature 수\n",
    "\t\t\tnum_layers(int) : LSTM의 layer의 개수\n",
    "\t\t\tbidireational(bool) : LSTM의 양방향 여부\n",
    "\t\t\tdropout_rate(float) : LSTM이 두 개 이상의 layer로 구성된 경우 적용할 dropout 비율\n",
    "\t\t\t\t\t\t\t\t  Dropout Layer의 dropout 비율\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\t# model을 구성할 Layer들을 정의 : Embedding, LSTM, Dropout, Linear(추론기기), Sigmoid\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(\n",
    "\t\t\tnum_embeddings=vocab_size,\t\t# 총 단어(토큰)수 -> tokenizer에 등록된 총 단어 수\n",
    "\t\t\tembedding_dim=embedding_dim,\t\t# embedding vector의 차원 수\n",
    "\t\t\tpadding_idx=0\t\t\t\t\t# [PAD]의 token ID(tokenizer.token_to_index(\"[PAD]\") 가 0인 것을 아니 그냥 0으로 넣어준것)\n",
    "\t\t\t\t\t\t\t\t\t\t\t# padding token은 학습하지 않는다.\n",
    "\t\t)\n",
    "\t\t# embedding layer의 출력 shape : (batch_size : 64, seq_len : 문서 토큰 수, embedding_dim)\n",
    "\n",
    "\t\tself.lstm= nn.LSTM(\n",
    "\t\t\tinput_size=embedding_dim,\t# 개별 토큰(단어)의 feature수(embedding -> LSTM)\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tnum_layers=num_layers,\n",
    "\t\t\tbidirectional=bidireational,\n",
    "\t\t\tdropout=dropout_rate if num_layers > 1 else 0\t# stacked rnn일 경우 설정.\n",
    "\t\t)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(dropout_rate)\t\t# LSTM과 Linear 사이에 과적합 방지를 위해서 사용\n",
    "\n",
    "\t\t# LSTM의 출력 : out, (hidden, cell)\n",
    "\t\t# out : 모든 timestep의 hidden state 값 [seq_len, batch, hidden * bidirectional ]\n",
    "\t\t# hidden : 마지막 timestep의 hidden state(단기 기억)\n",
    "\t\t# cell : 마지막 timestep의 cell state(장기 기억)\n",
    "\n",
    "\t\tinput_features = hidden_size*2 if bidireational else hidden_size\n",
    "\t\tself.classifier = nn.Linear(input_features, 1)\t# 출력 1: 이진분류 -> positive의 확률\n",
    "\t\tself.sigmoid = nn.Sigmoid()\t\t\t# classifier의 출력값을 확률(0 ~ 1)값으로 변환하는 func\n",
    "\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tX(tensor) : 입력 문서 토큰 list. shape : [batch_size, max_length : anstjxhzmstn] - [64, 30]\n",
    "\t\t\"\"\"\n",
    "\t\tembedding_vectors = self.embedding(X)\n",
    "\t\t# [batch, seq_len] -> embedding -> [batch_size, seq_len, embedding_dim]\n",
    "\t\t# LSTM - batch_first = False : 입력 shape - [seq_len, batch_size, embedding_dim]\\\n",
    "\t\t# embedding_vectors의 batch 축과 seq_len 축(값의 위치)을 바꿔준다!\n",
    "\t\tembedding_vectors = embedding_vectors.transpose(1,0)\t# 0번 축을 1번 축으로, 1번 축을 0번 축으로\n",
    "\t\tout, _ = self.lstm(embedding_vectors)\n",
    "\t\t# out.shape : [seq_len, batch_size, hidden_size * (2 if bidireational else 1)]\n",
    "\t\t# classifier(linear)에는 out의 마지막 index(마지막 seq) 값을 입력\n",
    "\t\toutput = self.dropout(out[-1])\n",
    "\t\toutput = self.classifier(output)\n",
    "\t\tlast_output = self.sigmoid(output)\n",
    "\n",
    "\t\treturn last_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ca46d",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 생성 전 변수들 먼저 선언\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\t\t# 총 어휘수\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# model의 복잡도 올린다 -> EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS를 크게!\n",
    "# Auto regressive model이 아니면 BIDIRECTIONAL=True (양방향)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NSMCClassifier(\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tembedding_dim=EMBEDDING_DIM,\n",
    "\thidden_size=HIDDEN_SIZE,\n",
    "\tnum_layers=NUM_LAYERS,\n",
    "\tbidireational=BIDIRECTIONAL,\n",
    "\tdropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cef336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "i = torch.randint(1, 10, (64, MAX_LENGTH))\t# int64 type의 dummy input data 생성\n",
    "# input shape : ...\n",
    "summary(model, input_data= i, device=device)\n",
    "# summary(model, input_shape) -> 내부적으로 inputdata(float32)를 생성해서 추론함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e5add",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14cc665",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc590bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch train하는 func\n",
    "def train(model, dataloader, loss_fn, optimizer, device=\"cpu\"):\n",
    "\t# 1. model을 train mode로 변환\n",
    "\tmodel.?()  # 학습 모드로 전환 \t\t\t\t(정답: train)\n",
    "\t# 2. model을 device로 이동\n",
    "\tmodel = model.to(device)\n",
    "\ttotal_loss= 0.0 \t# step별 loss를 누적\n",
    "\n",
    "\t# step 단위로 model train (batch)\n",
    "\tfor X, y in dataloader:\n",
    "\t\t# 1. X, y를 device로 이동\n",
    "\t\tX, y = X.to(device), y.to(device)\n",
    "\t\t# 2. predict\n",
    "\t\tpred = model(X)\n",
    "\t\t# 3. loss cal\n",
    "\t\tloss = loss_fn(pred, y)\n",
    "\t\t# 4. Gradient cal\n",
    "\t\tloss.?()  # 손실함수로부터 역전파 수행 \t\t\t\t\t(정답: backward)\n",
    "\t\t# 5. parametor update (w.data - w.grad * lr)\n",
    "\t\toptimizer.?()  # 파라미터 갱신 \t\t\t\t\t\t(정답: step)\n",
    "\t\t# 6. Gradient 초기화\n",
    "\t\toptimizer.?()  # 매 학습 반복마다 기울기 초기화 \t\t(정답: zero_grad)\n",
    "\t\t# loss 누적\n",
    "\t\ttotal_loss += loss.item()\n",
    "\t# 1 epoch train 완료\n",
    "\treturn total_loss / len(dataloader)\t\t# 1 epoch의 train loss return. (total loss / step 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b79f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch eval하는 func\n",
    "def test(model, dataloader, loss_fn, device=\"cpu\"):\n",
    "\t# 1. model을 eval mode로 변환\n",
    "\tmodel.?()  # 평가 모드로 전환 \t\t\t\t(정답: eval)\n",
    "\t# 2. model을 devie로 이동\n",
    "\tmodel = model.to(device)\n",
    "\n",
    "\t#loss, accracy\n",
    "\ttotal_loss = 0.0\n",
    "\ttotal_acc = 0.0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\t# step 단위로 model eval\n",
    "\t\tfor X, y in dataloader:\n",
    "\t\t\t# 1. X, y를 device로 이동\n",
    "\t\t\tX, y = X.to(device), y.to(device)\n",
    "\t\t\t# 2. predict\n",
    "\t\t\tpred_proba = model(X)\t# 양성일 확률\n",
    "\t\t\tpred_label = (pred_proba > 0.5).type(torch.int32)\n",
    "\t\t\ttotal_loss += loss_fn(pred_proba, y).item()\n",
    "\t\t\ttotal_acc += (pred_label == y).sum().item()\n",
    "\n",
    "\t\t# loss, acc 값을 return\n",
    "\t\treturn total_loss / len(dataloader) , total_acc / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36117f7d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "EPOCHS = 3\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = ?  # Adam 옵티마이저 정의           (정답: torch.optim.Adam(model.parameters(), lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb895ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "for epoch in range(EPOCHS):\n",
    "\ttrain_loss = train(model, train_loader, loss_fn, optimizer, device)\n",
    "\tval_loss, val_acc = test(model, test_loader, loss_fn, device)\n",
    "\ttrain_loss_list.append(train_loss)\n",
    "\tval_loss_list.append(val_loss)\n",
    "\tval_acc_list.append(val_acc)\n",
    "\tprint(f\"[{epoch}/{EPOCHS}] train loss : {train_loss}, val loss : {val_loss}, val acc : {val_acc}\")\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ba402",
   "metadata": {},
   "source": [
    "## 모델 save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "torch.save(model, \"saved_models/nsmc/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# load\n",
    "load_model = torch.load(\"model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d45d9",
   "metadata": {},
   "source": [
    "## 전처리 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "morph_tokenizer = Okt()\n",
    "\n",
    "def text_preprocessing(text):\n",
    "\t\n",
    "\ttext = text.lower()\n",
    "\ttext = re.sub(f\"[{string.punctuation}]+\", ' ', text)\n",
    "\treturn ' '.join(morph_tokenizer.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_sequences(token_sequences, max_length):\n",
    "\t\"\"\"padding 처리 메소드.\"\"\"\n",
    "\tpad_token = tokenizer.token_to_id('[PAD]')  \n",
    "\tseq_length = len(token_sequences)           \n",
    "\tresult = None\n",
    "\tif seq_length > max_length:                 \n",
    "\t\tresult = token_sequences[:max_length]\n",
    "\telse:                                            \n",
    "\t\tresult = token_sequences + ([pad_token] * (max_length - seq_length))\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499afe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_preprocessing(text_list):\n",
    "\t\"\"\"\n",
    "\t모델에 입력할 수있는 input data를 생성\n",
    "\tParameter:\n",
    "\t\ttext_list: list - 추론할 댓글리스트\n",
    "\tReturn\n",
    "\t\ttorch.LongTensor - 댓글 token_id tensor\n",
    "\t\"\"\"\n",
    "\n",
    "\t# cleansing + 정규화 \n",
    "\ttext_list = [text_preprocessing(txt) for txt in text_list]\n",
    "\t# text -> 토큰화\n",
    "\ttoken_list = [tokenizer.encode(txt).ids for txt in text_list]\n",
    "\t# token list의 size를 max_length에 맞추기\n",
    "\ttoken_list = [pad_token_sequences(token, MAX_LENGTH) for token in token_list]\n",
    "\n",
    "\treturn torch.tensor(token_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42c1f3",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "comment_list = [\"아 진짜 재미없다.\", \"여기 식당 먹을만 해요\", \"이걸 영화라고 만들었냐?\", \"기대 안하고 봐서 그런지 괜찮은데.\", \"이걸 영화라고 만들었나?\", \"아! 뭐야 진짜.\", \"재미있는데.\", \"연기 짱 좋아. 한번 더 볼 의향도 있다.\", \"뭐 그럭저럭\"]\n",
    "input_tensor = predict_data_preprocessing(comment_list)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, comment_list:list[str], input_tensor:torch.tensor, device=\"cpu\"):\n",
    "\t\"\"\"\n",
    "\tmodel로 input_tensor를 추론해서 긍정/부정적인 댓글인지 출력\n",
    "\t출력 형식\n",
    "\t\tcomment(댓글) label 확률\n",
    "\t\t\"아 노잼\"\t   부정\t 0.9\t(부정일 확률)\n",
    "\t\t\"꿀잼 ㅋ\"\t   긍정  0.87\t(긍정일 확률)\n",
    "\t\"\"\"\n",
    "\n",
    "\t# 1. model을 eval mode로 변환\n",
    "\tmodel.?()  # 평가 모드로 전환 (정답: eval)\n",
    "\t# 2. model을 device로 이동\n",
    "\tmodel = model.to(device)\n",
    "\t# input_tensor = input_tensor.to(device)\n",
    "\n",
    "\t\n",
    "\twith torch.no_grad():\t# 추론 과정이니까\n",
    "\t\tpred = model(input_tensor)\t# shape : (batch, 1) -> Positive일 확률값\n",
    "\t\tprint(pred)\n",
    "\t\tfor txt, pos_proba in zip(comment_list, pred):\n",
    "\t\t\tlabel = \"긍정적\" if pos_proba.item() > 0.5 else \"부정적\"\n",
    "\t\t\tproba =\tpos_proba.item() if pos_proba.item() > 0.5 else 1-pos_proba.item()\t\t# 확률\n",
    "\t\t\tprint(txt, label, round(proba, 3), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5a5b55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4584],\n",
      "        [0.5452],\n",
      "        [0.4386],\n",
      "        [0.5529],\n",
      "        [0.4386],\n",
      "        [0.4592],\n",
      "        [0.5762],\n",
      "        [0.8015],\n",
      "        [0.5042]])\n",
      "아 진짜 재미없다.\t부정적\t0.542\n",
      "여기 식당 먹을만 해요\t긍정적\t0.545\n",
      "이걸 영화라고 만들었냐?\t부정적\t0.561\n",
      "기대 안하고 봐서 그런지 괜찮은데.\t긍정적\t0.553\n",
      "이걸 영화라고 만들었나?\t부정적\t0.561\n",
      "아! 뭐야 진짜.\t부정적\t0.541\n",
      "재미있는데.\t긍정적\t0.576\n",
      "연기 짱 좋아. 한번 더 볼 의향도 있다.\t긍정적\t0.802\n",
      "뭐 그럭저럭\t긍정적\t0.504\n"
     ]
    }
   ],
   "source": [
    "predict(load_model, comment_list, input_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"분석하려는 댓글을 입력하세요. 종료하려면 '!quit'을 입력하세요.\")\n",
    "while True:\n",
    "\tcomment = input(\"댓글 : \")\n",
    "\tif comment == \"!quit\":\n",
    "\t\tprint(\"종료\")\n",
    "\t\t\n",
    "\t\tbreak\n",
    "\tcomment_list = [comment]\n",
    "\tinput_tensor = predict_data_preprocessing([comment])\n",
    "\tpredict(model, comment_list, input_tensor, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
