{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18aad08",
   "metadata": {},
   "source": [
    "## **Pretrained Model**\n",
    "- 이미 어떤 데이터로 학습(pretraining)되어 있는 모델\n",
    "- 즉, 처음부터 학습하지 않고도 특정 작업에 바로 사용하거나, 추가 학습(fine-tuning) 하여 쓸 수 있는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb2ca1",
   "metadata": {},
   "source": [
    "### **Pretrained model을 사용하는 이유**\n",
    "\n",
    "| 이유       | 설명                                     |\n",
    "| -------- | -------------------------------------- |\n",
    "| 시간 절약 | 수백 시간, 수천 GPU 시간이 드는 학습 과정을 생략할 수 있음   |\n",
    "| 비용 절감 | 대규모 데이터셋 없이도 좋은 성능                     |\n",
    "|    성능    | 이미 언어/이미지의 일반적인 패턴을 잘 이해하고 있음          |\n",
    "|   재사용성  | 여러 작업에 쉽게 전이 학습 가능 (transfer learning) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb8103",
   "metadata": {},
   "source": [
    "### **Pretrain model을 사용하는 방식**\n",
    " -   제로샷 전이학습(Zero shot transfer learning)\n",
    "        -   추가 학습없어 Pretrained 모델을 해결하려는 문제에 그대로 사용한다.\n",
    " -   전이학습 (Transfer learning)\n",
    "        -   Pretrained 모델의 일부분을 재학습 시킨다. 주로 출력 Layer를 학습시킨다.(classifier와 같은.)\n",
    " -   미세조정 (Fine tuning)\n",
    "     -   Pretrained 모델의 파라미터를 초기 파라미터로 사용하여 Custom Dataset(내가 사용하는 데이터 셋 -기존에 학습하지 않은)으로 학습을 진행하여 모델의 모든 파라미터를 업데이트 시킨다.\n",
    "\n",
    "> **Custom dataset:** 모델을 학습 시킬 dataset으로 pretrained model이 학습한 dataset과 구별하여 부르는 용어."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43544138",
   "metadata": {},
   "source": [
    "## **Transfer learning(전이학습)**\n",
    "- 기존에 학습된 모델(또는 그 일부)을 새로운 문제에 재사용하는 학습 방법\n",
    "\n",
    "### **Transfer learning 장점**\n",
    "| 장점          | 설명                        |\n",
    "| ----------- | ------------------------- |\n",
    "|  빠른 학습    | 이미 대부분의 학습이 되어 있어서 속도가 빠름 |\n",
    "|  적은 데이터   | 적은 데이터만으로도 좋은 성능을 낼 수 있음  |\n",
    "|  자원 절약    | GPU/메모리/시간 등을 크게 줄일 수 있음  |\n",
    "|  좋은 초기 성능 | 기존 모델의 일반화된 지식을 활용 가능     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de4da5",
   "metadata": {},
   "source": [
    "### **Transfer Learning의 주요 방식**\n",
    "- 1. Feature Extraction (특징 추출)\n",
    "    - 기존 모델의 앞쪽 레이어는 그대로 사용 (고정)\n",
    "    - 마지막 출력 레이어만 새 작업에 맞게 교체 후 학습\n",
    "    - 빠르고 간단\n",
    "- 2. Fine-tuning (미세조정)\n",
    "    - 기존 모델 전체 또는 일부 레이어를 새 데이터로 다시 학습\n",
    "    - 더 좋은 성능을 낼 수 있지만 학습 비용이 큼\n",
    "\n",
    "![transfer_learning](figures/transfer_learning_1.png)\n",
    "\n",
    "\n",
    "![transfer02](figures/transfer_learning_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5fe52",
   "metadata": {},
   "source": [
    "### **Transfer Learning의 단계**\n",
    "- Pretraining (사전학습): 대규모 데이터로 일반적인 지식을 학습\n",
    "\n",
    "- Task 정의: 새로운 문제와 데이터 준비\n",
    "\n",
    "- Feature Extract 또는 Fine-tuning 선택\n",
    "\n",
    "- Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b9522",
   "metadata": {},
   "source": [
    "> ## Backbone, Head\n",
    "> -  Backbone: 전체 모델 구조에서 Feature Extraction를 역할을 담당하는 부분.\n",
    "> -  Head: 추론을 담당하는 layer block을 Head (Network) 이라고 한다.\n",
    "\n",
    "\n",
    "| 항목    | Backbone               | Head               |\n",
    "| ----- | ---------------------- | ------------------ |\n",
    "| 역할    | 특징 추출기                 | Task 수행기           |\n",
    "| 학습 여부 | 보통 Freeze 또는 Fine-tune | 새로 학습              |\n",
    "| 예시    | ResNet, BERT, ViT      | Linear, MLP, CRF 등 |\n",
    "| 위치    | 입력 직후                  | 출력 직전              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee304da",
   "metadata": {},
   "source": [
    "## **Fine tuning(미세조정)**\n",
    "\n",
    "-   Pretrained 모델을 내가 학습시켜야 하는 데이터셋(Custom Dataset)으로 **추가 학습**시키는 것을 fine tuning 이라고 한다.\n",
    "-   주어진 문제에 더 적합하도록 Feature Extractor의 가중치들 추가로 학습하여 조정 한다.\n",
    "\n",
    "### **Fine tuning 전략**\n",
    "\n",
    "![fine tuning](figures/finetuning_1.png)\n",
    "-   **세 전략 모두 추론기는 trainable로 한다.**\n",
    "   \n",
    "**<span style='font-size:1.2em'>1. 전체 모델을 전부 학습시킨다.(1번)</span>**\n",
    "\n",
    "-   Pretrained 모델의 weight는 Feature extraction 의 초기 weight 역할을 한다.\n",
    "-   **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **낮은 경우** 적용.\n",
    "-   학습에 시간이 많이 걸린다.\n",
    "\n",
    "**<span style='font-size:1.2em'>2. Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습시킨다.(2번)<span>**\n",
    "\n",
    "-   **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
    "-   **Train dataset의 양이 적고** Pretained 모델이 학습했던 dataset과 custom dataset의 class간의 유사성이 **낮은 경우** 적용\n",
    "\n",
    "**<span style='font-size:1.2em'>3. Pretrained 모델 전체를 고정시키고 classifier layer들만 학습시킨다.(3번)</span>**\n",
    "\n",
    "-   **Train dataset의 양이 적고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
    "\n",
    "> 1번 2번 전략을 Fine tuning 이라고 한다.\n",
    "\n",
    "![fine tuning](figures/finetuning_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb524715",
   "metadata": {},
   "source": [
    "### **Hugging Face**\n",
    "<br>= AI 모델의 GitHub + Python 라이브러리 생태계\n",
    "\n",
    "- 연구자, 개발자, 기업 모두가 AI 모델을 쉽게 찾고, 쓰고, 공유할 수 있게 해주는 플랫폼\n",
    "\n",
    "- 특히 자연어 처리 분야에서는 사실상 표준 도구로 자리 잡고 있음"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
