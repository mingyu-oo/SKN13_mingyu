{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c5635f",
   "metadata": {},
   "source": [
    "# Seq2Seq 모델 만들기\n",
    "### Teacher Forcing 기법을 이용합니다.\n",
    "### ?를 채우기\n",
    "\n",
    "problem으로는 seq2seq 모델만드는 코드만 짜보고, 나머지 내용 전반에 대해서는 answer를 참고하시면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4aa18b-985a-47fd-868a-47a570dc1545",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1748842114333,
     "user": {
      "displayName": "남궁건우",
      "userId": "00560730431062529338"
     },
     "user_tz": -540
    },
    "id": "bd4aa18b-985a-47fd-868a-47a570dc1545",
    "outputId": "969cb9c0-6e0f-459e-83aa-0d9095d0b260"
   },
   "outputs": [],
   "source": [
    "# 1. 디코딩 시작토큰 변수에 담기\n",
    "? = tokenizer.token_to_id(\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1296c6",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1748842114334,
     "user": {
      "displayName": "남궁건우",
      "userId": "00560730431062529338"
     },
     "user_tz": -540
    },
    "id": "fa1296c6"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, outputs, teacher_forcing_rate=0.99):\n",
    "        \"\"\"\n",
    "        parameter\n",
    "            inputs: 질문 - (batch, seq_length)\n",
    "            outputs: 답변(정답) - (batch, seq_length)\n",
    "            teacher_forcing_rate: teacher_forcing 적용 확률.\n",
    "        \"\"\"\n",
    "        #################################\n",
    "        # 1번. 차원 맞추기\n",
    "        #################################\n",
    "\n",
    "\t\t# 1. input/output의 tensor가 1차원인 경우 해줘야할 조치\n",
    "        if inputs.dim() == ?: \n",
    "            inputs = ?\n",
    "        if outputs.dim() == ?:\n",
    "            outputs = ?\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # 기본 변수 설정\n",
    "        #################################\n",
    "\n",
    "        # 1.출력 문장의 길이와 배치 크기 추출\n",
    "        batch_size, output_length = outputs.shape\n",
    "\t\t# 2.디코더가 생성할 수 있는 전체 단어 개수\n",
    "        output_vocab_size = self.decoder.vocab_size  # 어휘사전 토큰 총 개수.\n",
    "        # 3. 출력 결과(모든 시점의 예측 결과)를 저장할 텐서\n",
    "        predicted_outputs = torch.zeros(output_length, batch_size, output_vocab_size).to(self.device)\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # 2번. 인코더 실행\n",
    "        #################################\n",
    "\n",
    "        # 1.encoder를 이용해서 질문 문장의 context vector(출력 및 hidden state) 추출.\n",
    "        ?, encoder_hidden = self.encoder(inputs)\n",
    "        # 2.Decoder의 첫번째 hidden state로써 인코더의 마지막 출력인 Context Vector 사용\n",
    "\t\t## hint: time dimension\n",
    "        decoder_hidden = ?[?].?\n",
    "        # 3.Decoder에 넣을 첫번째 time step의 값: [SOS] -> [batch_size] 형태\n",
    "        decoder_input = torch.full((batch_size, ), fill_value=?, device=self.device)\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # 디코더 반복 생성\n",
    "        #################################\n",
    "\n",
    "        # 순회(반복) 하면서 단어들을 하나씩 생성.\n",
    "        for t in range(?): # max_length 만큼 생성.\n",
    "\n",
    "\t\t\t# 1.디코더에 현재 입력 토큰과 hidden state를 넣어 출력 단어 확률 분포를 받음\n",
    "            decoder_out, decoder_hidden = self.decoder(?, ?)\n",
    "\n",
    "\t\t\t# 2.예측된 단어의 확률 분포를 t번째 시점에 저장\n",
    "            predicted_outputs[?] = decoder_out # t번째 예측 단어를 저장.\n",
    "\n",
    "\n",
    "            #################################\n",
    "            # Teacher Forcing 적용 여부 결정\n",
    "            #################################\n",
    "\n",
    "            # 1.teacher_forcing_rate 확률에 따라 정답을 줄지 예측을 줄지 결정(True면 정답)\n",
    "            teacher_forcing = teacher_forcing_rate > random.random() # TeacherForcing 적용여부(bool)\n",
    "\n",
    "\t\t\t# 2.반복할 수록 teacher forcing 비율 감소(모델 자율성 증가)\n",
    "            teacher_forcing_rate = teacher_forcing_rate * 0.99\n",
    "\n",
    "            #################################\n",
    "            # 다음 디코더 입력 준비\n",
    "            #################################\n",
    "\n",
    "            # 1.모델이 추론한 단어(예측된 단어)중 가장 확률이 높은(TOP-1) 단어의 인덱스를 추출\n",
    "            top1 = decoder_out.?\n",
    "\n",
    "            # 2.teacher forcing이면 정답 사용, 아니면 top1 예측 사용\n",
    "            decoder_input = outputs[:, ?] if teacher_forcing else top1\n",
    "\n",
    "        #################################\n",
    "        # 마무리: 출력 포맷 변경\n",
    "        #################################\n",
    "        return predicted_outputs.transpose(1, 0)\n",
    "\t\t    #>>>> (seq_len, batch, vocab_size) -> (batch, seq_length, vocab_size) 변환."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
