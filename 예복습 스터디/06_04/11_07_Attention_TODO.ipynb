{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d25496",
   "metadata": {},
   "source": [
    "[ 💡  ] 에 답을 넣어주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism \n",
    "\n",
    "Seq2Seq 모델의 문제점 :\n",
    "Seq2Seq 모델은 Encoder에서 입력 시퀀스에 대한 특성을 [💡     ] 에 압축하여 Decoder로 전달 한다.  \n",
    "하나의 고정된 크기의 vector에 모든 입력 시퀀스의 정보를 넣다보니 [ 💡   ] 이 발생한다. \n",
    "seq2seq는 encoder의 마지막 hidden state를 context로 받은 뒤 그것을 이용해 모든 출력 단어들을 생성하므로 그 중요도에 대한 반영이 안된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7056340",
   "metadata": {},
   "source": [
    "## 기존의 Seq2seq의 특징 \n",
    " - 짧은 문장의 경우 : \"오늘 저녁에 뭐할거야?\" \n",
    " - Encoder는 질문 하나에 벡터 하나씩 \n",
    " - Decoder는 이 벡터 하나씩에 대해 답을 만듬 (\"친구 만나러 가요\")\n",
    "\n",
    "### 여기서 문제는? \n",
    "- 질문이 길수록 벡터가 너무 많아져서 Decoder가 답을 만들기가 빡세다. \n",
    "\n",
    "### 그래서 등장한 Attention / Attention의 필요성\n",
    "- 벡터 하나에 모든 걸 다 넣지 말고, 필요할 때 마다 주어진 문장(문서)를 다시 참고하자. \n",
    "- 출력하는 단어마다(매 시점(time step)마다) 입력된 문장, 문서(context vector)에서 어디를 봐야할지(집중(attention)) 설정하는 것! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/refs/heads/master/ChatbotData.csv\"\n",
    "res = requests.get(url)\n",
    "if res.status_code == 200:\n",
    "    with open(\"data/chatbot_data.csv\", \"wt\", encoding=\"utf-8\") as fw:\n",
    "        fw.write(res.text)\n",
    "else:\n",
    "    print(f\"불러오지 못함: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chatbot_data.csv')\n",
    "df.drop(columns='label', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d23c37-f609-411b-aba2-17b081259cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_texts = df['Q'] # 질문 모음 \n",
    "answer_texts = df['A']   # 답변 모음\n",
    "all_texts = list(question_texts + \" \"+answer_texts) # Q + A : vocab 생성\n",
    "# all_texts에 대해 토큰화를 진행하면, 모델이 학습할 수 있는 형태로 바뀌게 된다.\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## Tokenizer 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe0018",
   "metadata": {},
   "source": [
    "1. BPE 기반 토크나이저 만들고\n",
    "2. 공백 단위로 단어를 자르게 하고\n",
    "3. 학습 조건을 설정한 뒤\n",
    "4. 실제로 학습시킨다 (내 문장들을 넣어서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704dca6-ec9e-48a7-8ad2-495c08aa9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 10000  # 뜻 : <최대 단어 사전 크기> 10,000개로 제한\n",
    "min_frequency = 5   # 뜻 : <최소 5번 이상 > 단어만 포함\n",
    "\n",
    "tokenizer = Tokenizer(BPE(<unk_token=\"[UNK]\")>) # 모르는 단어 일 때 [UNK]를 넣는다. \n",
    "tokenizer.pre_tokenizer = [💡  ] # 공백기준으로 문장을 쪼개겠다. \n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    continuing_subword_prefix='##',\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"] \n",
    "    # [SOS]: <문장의 시작을 의미>하는 토큰. [EOS]: <문장이 끝난 것을 의미>하는 토큰.\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0097d1d-7d62-4c81-9c09-db61741ffefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"총 어휘수:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac5977-edbe-4c64-b0dc-0c1ee0bb4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"saved_model/chatbot_attn\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "vocab_path = os.path.join(dir_path, \"chatbot_attn_bpe.json\")\n",
    "tokenizer.save(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cf727-4a41-4e09-8bf2-74f599d86ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset 생성\n",
    "- 한문장 단위로 학습시킬 것이므로 DataLoader를 생성하지 않고 Dataset에서 index로 조회한 질문-답변을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "여기서 질문!\n",
    "Q: 여기서 data loader를 생성하지않고 dataset에서 index로 조회한 질문 답변을 학습시킨다는 의미는 무엇일까요 ?\n",
    "\n",
    "A: 직접 인덱스로 조회 - [ 💡  ] \n",
    "    데이터 로더 사용 - [ 💡  ]\n",
    "\n",
    "# 실제 학습에서는 data loader로 배치 단위로 처리하는것이 일반적이라고 합니다.! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizer에 등록된 총 어휘수\n",
    "        SOS: int - [SOS] 문장의 시작 토큰 id\n",
    "        EOS: int = [EOS] 문장의 끝 토큰 id\n",
    "        question_squences: list - 모든 질문 str을 token_id_list(token sequence) 로 변환하여 저장한 list \n",
    "        answser_sequences: list - 모든 답변 str을 token_id_list(token sequence) 로 변환하여 저장한 list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=2, max_length=20):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\",   \"답2\",   ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=2: int - 최소 토큰 개수. 질문과 답변의 token수가 min_length 이상인 것만 학습한다.\n",
    "        max_length=20:int 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        \"\"\"\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]')\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]')\n",
    "\n",
    "        self.question_sequences = []\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            q_token = [💡  ](q) # 각 문장을 숫자리스트로 바꿈 \n",
    "            a_token = [💡  ](a) # 각 문장을 숫자리스트로 바꿈 \n",
    "            if len(q_token) > min_length and len(a_token) > min_length:\n",
    "                self.question_sequences.[💡  ](q_token)\n",
    "                self.answer_sequences.[💡  ] (a_token) # 길이가 충분한 질문, 답변만 최종 학습용 리스트에 저장 \n",
    "\n",
    "    def __add_special_tokens(self, token_sequence):\n",
    "        \"\"\"\n",
    "        질문/답변 토큰 리스트 맨 뒤에 문장의 끝을 표시하는 [EOS] 토큰 추가. \n",
    "        [EOS] Token을 붙이고 max_length 보다 토큰수가 많으면 안된다.\n",
    "        Args:\n",
    "            token_sequence (list[str]) - EOS 토큰을 추가할 문서 token sequence\n",
    "        \"\"\"\n",
    "        token_id_list = token_sequence [💡 ]# 문장 끝을 표시하는 [EOS] 를 추가하기 위해 마지막 하나 잘라낸다.         token_id_list.append(self.EOS)\n",
    "\n",
    "        return token_id_list\n",
    "\n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장 string을 받아서 encoding 한 뒤 [EOS] token을 추가한 token_id 리스트(list)를 생성 해서 반환한다.\n",
    "        Args:\n",
    "            text (str) - token id 리스트로 변환할 대상 String.\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_sequences)\n",
    "\n",
    "    def __getitem__(self, index):   \n",
    "        # embedding 입력 -> int 64\n",
    "        # unsqueeze(1) - [1, 2, 3, 4] -> [[1], [2], [3], [4]] # 차원을 추가하는 함수 \n",
    "        q = torch.tensor(self.question_sequences[index], dtype=torch.int64).[💡  ] # 차원을 추가하는 함수 \n",
    "        a = torch.tensor(self.answer_sequences[index], dtype=torch.int64).[💡   ] # 차원을 추가하는 함수 \n",
    "        return q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277360c-705b-42b1-8ea2-e9f9aaca4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "MIN_LENGTH = 2\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdd1b5",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "GRU를 통해 문맥 정보를 압축해주는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_vocabs: int - 총 어휘수 \n",
    "            hidden_size: int - GRU의 hidden size\n",
    "            embedding_dim: int - Embedding vector의 차원수 \n",
    "            num_layers: int - GRU의 layer수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 임베딩 레이어, 단어를 벡터로 변환\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # GRU 생성, 단어 순서에 따라 문맥을 이해 \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers\n",
    "            )\n",
    "\n",
    "\n",
    "    def [ 💡 ]:\n",
    "        \"\"\"\n",
    "        질문의 token한개의 토큰 id를 입력받아 hidden state를 출력\n",
    "        \n",
    "        Args:\n",
    "            x: 한개 토큰. shape-[1]\n",
    "            hidden: hidden state (이전 처리결과). shape: [1, 1, hidden_size]\n",
    "        Returns\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        # x shape : [batch: 1, 1]\n",
    "        embedded = self.embedding(x).unsqueeze(0) # (1: batch, embedding_dim) -> (1: batch, 1: seq_len, embedding_dim)\n",
    "        out, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        return out, hidden \n",
    "    \n",
    "    \n",
    "    def [  💡 ]: #처음 timestep에서 입력할 hidden_state.\n",
    "        \"\"\"\n",
    "        (왜냐면 첨 스텝에는 넣어야할 hidden state 가 없기 때문에 직접 만들어서 넣어주는 것)\n",
    "        값: < 0 >\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53312985",
   "metadata": {},
   "source": [
    "## Attention 적용 Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attention은 Decoder 네트워크가 순차적으로 다음 단어를 생성하는 자기 출력의 모든 단계에서 인코더 출력 중 연관있는 부분에 **집중(attention)** 할 수 있게 한다. \n",
    "- 다양한 어텐션 기법중에 **Luong attention** 방법은 다음과 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [💡  ]\n",
    "- Decoder가 현재 timestep의 단어(token)을 생성할 때 Encoder의 output 들 중 어떤 단어에 좀더 집중해야 하는지 계산하기 위한 가중치값.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [💡  ]\n",
    "- Decoder에서 현재 timestep의 단어를 추출할 때 사용할 Context Vector.\n",
    "    - Encoder의 output 들에 Attention Weight를 곱한다.\n",
    "    - Attention Value는 Decoder에서 단어를 생성할 때 encoder output의 어떤 단어에 더 집중하고 덜 집중할지를 가지는 값이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Extraction\n",
    "- Decoder의 embedding vector와 Attention Value 를 합쳐 RNN(GRU)의 입력을 만든다.\n",
    "    - **단어를 생성하기 위해 이전 timestep에서 추론한 단어 현재 timestep의 input 와 Encoder output에 attention이 적용된 값 이 둘을 합쳐 입력한다.\n",
    "    - 이 값을 [💡  ]를 이용해 RNN input_size에 맞춰 준다. (어떻게 input_size에 맞출지도 학습시키기 위해 Linear Layer이용)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### 단어 예측(생성)\n",
    "- RNN에서 찾은 Feature를 총 단어개수의 units을 출력하는 Linear에 입력해 **다음 단어를 추론한다.**\n",
    "- 추론한 단어는 다음 timestep의 입력($X_t$)으로 RNN의 hidden은 다음 timestep 의 hidden state ($h_{t-1}$) 로 입력된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, dropout_p, max_length):\n",
    "        # num_vocabs : 총 어휘수\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # attention weight를 계산하는 Linear\n",
    "        # 이전 단어의 hidden state(prev_hidden)와 현재 단어의 embedding vector에\n",
    "        #   가중합을 계산해서 attention weight를 계산\n",
    "        # in_features: hidden_size + embedding_dim \n",
    "        # out_features: Encoder의 hidden_state의 개수 (max_length)\n",
    "        self.attn = nn.Linear(hidden_size+embedding_dim, max_length)\n",
    "\n",
    "        # 가정 : hidden_size = 200, max_length (토큰 수) = 20, \n",
    "        # attention value: attention- weight @ encoder의 hidden state들 (out)\n",
    "        # shape: 1 x 20 @ 20 x 200 = 1 x 200 \n",
    "\n",
    "        # 현재 단어 embedding vector + attention balue를 입력받아 가중합을 계산해서 \n",
    "            # GRU(RNN)에 입력할 입력값을 계산 \n",
    "            # in_features : embedding_dim + encoder의 hidden_size\n",
    "        self.attn_combine = nn.Linear(embedding_dim+hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        # 분류기 \n",
    "        self.classifier = nn.Linear(hidden_size, num_vocabs)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: 현재 timestep의 입력 토큰(단어) id\n",
    "            hidden: 이전 timestep 처리결과 hidden state\n",
    "            encoder_outputs: Encoder output들. \n",
    "        Return\n",
    "            tupe: (output, hidden, attention_weight)\n",
    "                output: 단어별 다음 단어일 확률.  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                atttention_weight: Encoder output 중 어느 단어에 집중해야하는 지 가중치값. shape: [1, max_length]\n",
    "        \n",
    "        현재 timestep 입력과 이전 timestep 처리결과를 기준으로 encoder_output와 계산해서  encoder_output에서 집중(attention)해야할 attention value를 계산한다.\n",
    "        attention value와 현재 timestep 입력을 기준으로 단어를 추론(생성) 한다.\n",
    "        \"\"\"\n",
    "        \n",
    "        #  embedding \n",
    "        embedding = self.embedding(x).unsqueeze(0) # [1:batch] -> [1:batch, 1:seq_len]\n",
    "        embedding = self.dropout(embedding)\n",
    "\n",
    "        # attetion weight 계산 \n",
    "        attn_in = torch.[💡  ]((embedding[0], hidden[0]),dim=1 ) #현재 단어 임베딩, 이전 히든 이어붙여 각 단어별 중요도 점수 생성\n",
    "        # attn_in shape : [1, embedding_dim+hidden_size]\n",
    "        attn_score = self.attn(attn_in) # logit\n",
    "        # shape : < [1, max_length]> \n",
    "        # attn_score shape: 1 x embeddign_dim+hidden_size @ embedding_dim+hidden_size x max_length\n",
    "\n",
    "        \n",
    "        attn_weight = nn.Softmax[💡    ](attn_score) # 전체 문장에서 어떤 단어에 집중할지 확률로 변환\n",
    "        # 마지막 차원을 기준으로 softmax 적용해라 - **문장 안의 단어 위치별 집중 정도(확률)**를 구하는 데 쓰입니다.\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd602c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # attetion value 계산 (attn_applied) \n",
    "        #  attn_weight @ encoder_hiddenstate\n",
    "        ##  1 x max_length  @  max_length x hidden_size \n",
    "\n",
    "        # torch.bmm() - batch-wise matrix multiplication(배치단위 행렬곱) - 3차원 텐서만 받음! \n",
    "        ##  3차원 배열을 받아서 1, 2 축 기준으로 행렬곱 계산. \n",
    "        ### (5, 2, 3) @ (5, 3, 5) -> 2 x 3 @ 3 x 5 5개를 행렬곱 => (5, 2, 5)\n",
    "        attn_value = torch.bmm(\n",
    "            attn_weight.[💡   ], # (1, 1, max_length) # 배치차원 추가\n",
    "            encoder_outputs.[💡    ]>, # (1, max_length, hidden_size)\n",
    "        )\n",
    "        # attn_value 결과: attn_weight @ encoder_outputs (1:batch, 1, hidden_size)\n",
    "        # 각 단어 벡터에 집중정도 attn_weight를 곱해서 문장의 대표 의미 attention value를 뽑아냄 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a2875",
   "metadata": {},
   "source": [
    "### torch.bmm()\n",
    "- 요약 : attention 가중치를 각 encoder hidden에 곱해 **중요한 단어에 집중한 문맥 벡터(attn_value)**를 계산하는 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # attn_combine: gru의 input 값을 생성 \n",
    "        ## attn_value + embedding(concat) => Linear => ReLU\n",
    "\n",
    "\n",
    "        attn_combine_in = torch.concat([\n",
    "            [ 💡   ]  , ### 지금까지 입력 문장을 압축한 문맥 벡터(1 x hidden_size)\n",
    "            [ 💡    ]       ### 현재 디코더에 입력된 단어의 임베딩 벡터 (1 x embedding_dim)\n",
    "        ], dim = 1)\n",
    "        gru_in = self.attn_combine(attn_combine_in) # 합쳐진 벡터를 Linear 레이어에 통과시켜서 hidden size로 변환 # 출력 (1, hidden_size)\n",
    "        gru_in = gru_in.<unsqueeze(0)> ## # 여기서 왜함? ****\n",
    "        gru_in = nn.ReLU()(gru_in) # 비선형성 추가 \n",
    "\n",
    "        # gru에 입력해서 다음 단어를 찾기 위한 hidden state(feature)를 계산.\n",
    "        out, hidden_state = self.gru(gru_in, hidden) # (seq, batch, hidden_size) (1, 1, hidden_size) # hidden_size가 만약 200개면, [[[0, 1, 2, 3, 4 .... 199]]]\n",
    "\n",
    "        # classification 에 out 을 입력해서 다음 단어를 예측\n",
    "        last_out = self.classifier(out[0]) # 분류기는 2차원 자료구조, linear할 떄 생각하면... [0] 면, [[0, 1, 2, 3, 4 .... 199]] 괄호하나 지운것. \n",
    "        # gru에서 빠져나온 마지막 단어에 대한 확률 \n",
    "        # last_out shape: [  [1, num_vocabs]  ] 0번째 인덱스로 조회해야 우리가 알고싶은 값이 나온다. \n",
    "        # decode 에서 빠져 나온 값을 다음 gru, 다음 gru에서 쓰도록 0 인덱스에서 사이즈를 맞춰줌 \n",
    "\n",
    "\n",
    "        return last_out[0], hidden_state, attn_weight\n",
    "                #  last_out[0]의 값은 [0, 1, 2, 3, 4 .... 199] 괄호하나 더 지운것. \n",
    "                #  last_out : 실제 vocab 사이즈만큼의 확률 리스트 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52df84",
   "metadata": {},
   "source": [
    "*** \n",
    "GRU는 입력 shape을 **(seq_len, batch_size, hidden_size)**로 받습니다.\n",
    "\n",
    "현재 gru_in의 shape은 (1, hidden_size)로 2D입니다.\n",
    "\n",
    "→ unsqueeze(0)으로 맨 앞에 sequence 길이 차원을 추가해서 (1, 1, hidden_size)로 만들어줘야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b88850",
   "metadata": {},
   "source": [
    "###  Encoder와 AttentionDecoder가 잘 작동하는지 확인하기 위해 dummy 데이터를 통해 모델을 직접 생성하고, 구조를 시험해보는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fda460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/ Decoder 를 dummy_data로 확인\n",
    "dummy_encoder = Encoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(), # Tokenizer가 학습한 전체 단어 개수\n",
    "    hidden_size=256,                       # RNN의 hidden state\n",
    "    embedding_dim=200,                     # 단어를 표현할 embedding vector의 차원\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "dummy_encoder = dummy_encoder.to(device)\n",
    "\n",
    "dummy_decoder = AttentionDecoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=200,\n",
    "    [💡   ] =0.3,                          # 훈련 중 일부 노드를 랜덤으로 꺼서 과대적합 방지\n",
    "    [💡   ] =20                             # 입력 시퀀스의 최대 길이로 attention weight의 크기 결정에 사용됨\n",
    ")\n",
    "\n",
    "dummy_decoder = dummy_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0] # 첫번째 (Q, A)\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# 첫번째 질문의 첫번째 토큰을 입력\n",
    "encoder_out, encoder_hidden = dummy_encoder([💡    ], dummy_encoder.init_hidden(device))\n",
    "# x[0] 토큰은 ? [SOS] 토큰이다! \n",
    "\n",
    "encoder_out.shape, encoder_hidden.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 질문의 첫번째 토큰을 입력. y[0]\n",
    "encoder_outputs = torch.randn(20, 256, device=device) # 20: seq_len, 256: hidden_size\n",
    "next_token, hidden_state, attn_weight = dummy_decoder([💡    ], encoder_out, encoder_outputs) # 현재 단어(y[0])와 이전 hidden state를 입력받음\n",
    "# 결과적으로 다음 단어가 무엇인지 확률 분포(next_token)로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b9536",
   "metadata": {},
   "source": [
    "next_token        # 예측한 다음 단어의 확률 분포 (shape: [vocab_size])\n",
    "hidden_state      # 업데이트된 hidden state (shape: [1, 1, hidden_size])\n",
    "attn_weight       # 어떤 encoder 단어에 집중했는지 (shape: [1, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_token.shape)\n",
    "print(next_token.argmax(-1), tokenizer.id_to_token(next_token.argmax(-1).item()))\n",
    "print(hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ee446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_weight.shape)\n",
    "attn_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
