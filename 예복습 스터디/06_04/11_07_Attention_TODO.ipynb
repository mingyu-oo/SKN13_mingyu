{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d25496",
   "metadata": {},
   "source": [
    "[ ğŸ’¡  ] ì— ë‹µì„ ë„£ì–´ì£¼ì„¸ìš”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism \n",
    "\n",
    "Seq2Seq ëª¨ë¸ì˜ ë¬¸ì œì  :\n",
    "Seq2Seq ëª¨ë¸ì€ Encoderì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ íŠ¹ì„±ì„ [ğŸ’¡     ] ì— ì••ì¶•í•˜ì—¬ Decoderë¡œ ì „ë‹¬ í•œë‹¤.  \n",
    "í•˜ë‚˜ì˜ ê³ ì •ëœ í¬ê¸°ì˜ vectorì— ëª¨ë“  ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì •ë³´ë¥¼ ë„£ë‹¤ë³´ë‹ˆ [ ğŸ’¡   ] ì´ ë°œìƒí•œë‹¤. \n",
    "seq2seqëŠ” encoderì˜ ë§ˆì§€ë§‰ hidden stateë¥¼ contextë¡œ ë°›ì€ ë’¤ ê·¸ê²ƒì„ ì´ìš©í•´ ëª¨ë“  ì¶œë ¥ ë‹¨ì–´ë“¤ì„ ìƒì„±í•˜ë¯€ë¡œ ê·¸ ì¤‘ìš”ë„ì— ëŒ€í•œ ë°˜ì˜ì´ ì•ˆëœë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7056340",
   "metadata": {},
   "source": [
    "## ê¸°ì¡´ì˜ Seq2seqì˜ íŠ¹ì§• \n",
    " - ì§§ì€ ë¬¸ì¥ì˜ ê²½ìš° : \"ì˜¤ëŠ˜ ì €ë…ì— ë­í• ê±°ì•¼?\" \n",
    " - EncoderëŠ” ì§ˆë¬¸ í•˜ë‚˜ì— ë²¡í„° í•˜ë‚˜ì”© \n",
    " - DecoderëŠ” ì´ ë²¡í„° í•˜ë‚˜ì”©ì— ëŒ€í•´ ë‹µì„ ë§Œë“¬ (\"ì¹œêµ¬ ë§Œë‚˜ëŸ¬ ê°€ìš”\")\n",
    "\n",
    "### ì—¬ê¸°ì„œ ë¬¸ì œëŠ”? \n",
    "- ì§ˆë¬¸ì´ ê¸¸ìˆ˜ë¡ ë²¡í„°ê°€ ë„ˆë¬´ ë§ì•„ì ¸ì„œ Decoderê°€ ë‹µì„ ë§Œë“¤ê¸°ê°€ ë¹¡ì„¸ë‹¤. \n",
    "\n",
    "### ê·¸ë˜ì„œ ë“±ì¥í•œ Attention / Attentionì˜ í•„ìš”ì„±\n",
    "- ë²¡í„° í•˜ë‚˜ì— ëª¨ë“  ê±¸ ë‹¤ ë„£ì§€ ë§ê³ , í•„ìš”í•  ë•Œ ë§ˆë‹¤ ì£¼ì–´ì§„ ë¬¸ì¥(ë¬¸ì„œ)ë¥¼ ë‹¤ì‹œ ì°¸ê³ í•˜ì. \n",
    "- ì¶œë ¥í•˜ëŠ” ë‹¨ì–´ë§ˆë‹¤(ë§¤ ì‹œì (time step)ë§ˆë‹¤) ì…ë ¥ëœ ë¬¸ì¥, ë¬¸ì„œ(context vector)ì—ì„œ ì–´ë””ë¥¼ ë´ì•¼í• ì§€(ì§‘ì¤‘(attention)) ì„¤ì •í•˜ëŠ” ê²ƒ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/refs/heads/master/ChatbotData.csv\"\n",
    "res = requests.get(url)\n",
    "if res.status_code == 200:\n",
    "    with open(\"data/chatbot_data.csv\", \"wt\", encoding=\"utf-8\") as fw:\n",
    "        fw.write(res.text)\n",
    "else:\n",
    "    print(f\"ë¶ˆëŸ¬ì˜¤ì§€ ëª»í•¨: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chatbot_data.csv')\n",
    "df.drop(columns='label', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d23c37-f609-411b-aba2-17b081259cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_texts = df['Q'] # ì§ˆë¬¸ ëª¨ìŒ \n",
    "answer_texts = df['A']   # ë‹µë³€ ëª¨ìŒ\n",
    "all_texts = list(question_texts + \" \"+answer_texts) # Q + A : vocab ìƒì„±\n",
    "# all_textsì— ëŒ€í•´ í† í°í™”ë¥¼ ì§„í–‰í•˜ë©´, ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°”ë€Œê²Œ ëœë‹¤.\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## Tokenizer í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe0018",
   "metadata": {},
   "source": [
    "1. BPE ê¸°ë°˜ í† í¬ë‚˜ì´ì € ë§Œë“¤ê³ \n",
    "2. ê³µë°± ë‹¨ìœ„ë¡œ ë‹¨ì–´ë¥¼ ìë¥´ê²Œ í•˜ê³ \n",
    "3. í•™ìŠµ ì¡°ê±´ì„ ì„¤ì •í•œ ë’¤\n",
    "4. ì‹¤ì œë¡œ í•™ìŠµì‹œí‚¨ë‹¤ (ë‚´ ë¬¸ì¥ë“¤ì„ ë„£ì–´ì„œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704dca6-ec9e-48a7-8ad2-495c08aa9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 10000  # ëœ» : <ìµœëŒ€ ë‹¨ì–´ ì‚¬ì „ í¬ê¸°> 10,000ê°œë¡œ ì œí•œ\n",
    "min_frequency = 5   # ëœ» : <ìµœì†Œ 5ë²ˆ ì´ìƒ > ë‹¨ì–´ë§Œ í¬í•¨\n",
    "\n",
    "tokenizer = Tokenizer(BPE(<unk_token=\"[UNK]\")>) # ëª¨ë¥´ëŠ” ë‹¨ì–´ ì¼ ë•Œ [UNK]ë¥¼ ë„£ëŠ”ë‹¤. \n",
    "tokenizer.pre_tokenizer = [ğŸ’¡  ] # ê³µë°±ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ì„ ìª¼ê°œê² ë‹¤. \n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    continuing_subword_prefix='##',\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"] \n",
    "    # [SOS]: <ë¬¸ì¥ì˜ ì‹œì‘ì„ ì˜ë¯¸>í•˜ëŠ” í† í°. [EOS]: <ë¬¸ì¥ì´ ëë‚œ ê²ƒì„ ì˜ë¯¸>í•˜ëŠ” í† í°.\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0097d1d-7d62-4c81-9c09-db61741ffefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ì´ ì–´íœ˜ìˆ˜:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac5977-edbe-4c64-b0dc-0c1ee0bb4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"saved_model/chatbot_attn\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "vocab_path = os.path.join(dir_path, \"chatbot_attn_bpe.json\")\n",
    "tokenizer.save(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cf727-4a41-4e09-8bf2-74f599d86ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset ìƒì„±\n",
    "- í•œë¬¸ì¥ ë‹¨ìœ„ë¡œ í•™ìŠµì‹œí‚¬ ê²ƒì´ë¯€ë¡œ DataLoaderë¥¼ ìƒì„±í•˜ì§€ ì•Šê³  Datasetì—ì„œ indexë¡œ ì¡°íšŒí•œ ì§ˆë¬¸-ë‹µë³€ì„ í•™ìŠµì‹œí‚¨ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ì—¬ê¸°ì„œ ì§ˆë¬¸!\n",
    "Q: ì—¬ê¸°ì„œ data loaderë¥¼ ìƒì„±í•˜ì§€ì•Šê³  datasetì—ì„œ indexë¡œ ì¡°íšŒí•œ ì§ˆë¬¸ ë‹µë³€ì„ í•™ìŠµì‹œí‚¨ë‹¤ëŠ” ì˜ë¯¸ëŠ” ë¬´ì—‡ì¼ê¹Œìš” ?\n",
    "\n",
    "A: ì§ì ‘ ì¸ë±ìŠ¤ë¡œ ì¡°íšŒ - [ ğŸ’¡  ] \n",
    "    ë°ì´í„° ë¡œë” ì‚¬ìš© - [ ğŸ’¡  ]\n",
    "\n",
    "# ì‹¤ì œ í•™ìŠµì—ì„œëŠ” data loaderë¡œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ëŠ”ê²ƒì´ ì¼ë°˜ì ì´ë¼ê³  í•©ë‹ˆë‹¤.! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizerì— ë“±ë¡ëœ ì´ ì–´íœ˜ìˆ˜\n",
    "        SOS: int - [SOS] ë¬¸ì¥ì˜ ì‹œì‘ í† í° id\n",
    "        EOS: int = [EOS] ë¬¸ì¥ì˜ ë í† í° id\n",
    "        question_squences: list - ëª¨ë“  ì§ˆë¬¸ strì„ token_id_list(token sequence) ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•œ list \n",
    "        answser_sequences: list - ëª¨ë“  ë‹µë³€ strì„ token_id_list(token sequence) ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•œ list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=2, max_length=20):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - ì§ˆë¬¸ texts ëª©ë¡. ë¦¬ìŠ¤íŠ¸ì— ì§ˆë¬¸ë“¤ì„ ë‹´ì•„ì„œ ë°›ëŠ”ë‹¤. [\"ì§ˆë¬¸1\", \"ì§ˆë¬¸2\", ...]\n",
    "        answer_texts: list[str] - ë‹µ texts ëª©ë¡. ë¦¬ìŠ¤íŠ¸ì— ë‹µë³€ë“¤ì„ ë‹´ì•„ì„œ ë°›ëŠ”ë‹¤.     [\"ë‹µ1\",   \"ë‹µ2\",   ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=2: int - ìµœì†Œ í† í° ê°œìˆ˜. ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ tokenìˆ˜ê°€ min_length ì´ìƒì¸ ê²ƒë§Œ í•™ìŠµí•œë‹¤.\n",
    "        max_length=20:int ê°œë³„ ëŒ“ê¸€ì˜ token ê°œìˆ˜. ëª¨ë“  ëŒ“ê¸€ì˜ í† í°ìˆ˜ë¥¼ max_lengthì— ë§ì¶˜ë‹¤.\n",
    "        \"\"\"\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]')\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]')\n",
    "\n",
    "        self.question_sequences = []\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            q_token = [ğŸ’¡  ](q) # ê° ë¬¸ì¥ì„ ìˆ«ìë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿ˆ \n",
    "            a_token = [ğŸ’¡  ](a) # ê° ë¬¸ì¥ì„ ìˆ«ìë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿ˆ \n",
    "            if len(q_token) > min_length and len(a_token) > min_length:\n",
    "                self.question_sequences.[ğŸ’¡  ](q_token)\n",
    "                self.answer_sequences.[ğŸ’¡  ] (a_token) # ê¸¸ì´ê°€ ì¶©ë¶„í•œ ì§ˆë¬¸, ë‹µë³€ë§Œ ìµœì¢… í•™ìŠµìš© ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ \n",
    "\n",
    "    def __add_special_tokens(self, token_sequence):\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸/ë‹µë³€ í† í° ë¦¬ìŠ¤íŠ¸ ë§¨ ë’¤ì— ë¬¸ì¥ì˜ ëì„ í‘œì‹œí•˜ëŠ” [EOS] í† í° ì¶”ê°€. \n",
    "        [EOS] Tokenì„ ë¶™ì´ê³  max_length ë³´ë‹¤ í† í°ìˆ˜ê°€ ë§ìœ¼ë©´ ì•ˆëœë‹¤.\n",
    "        Args:\n",
    "            token_sequence (list[str]) - EOS í† í°ì„ ì¶”ê°€í•  ë¬¸ì„œ token sequence\n",
    "        \"\"\"\n",
    "        token_id_list = token_sequence [ğŸ’¡ ]# ë¬¸ì¥ ëì„ í‘œì‹œí•˜ëŠ” [EOS] ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ í•˜ë‚˜ ì˜ë¼ë‚¸ë‹¤.         token_id_list.append(self.EOS)\n",
    "\n",
    "        return token_id_list\n",
    "\n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        í•œ ë¬¸ì¥ stringì„ ë°›ì•„ì„œ encoding í•œ ë’¤ [EOS] tokenì„ ì¶”ê°€í•œ token_id ë¦¬ìŠ¤íŠ¸(list)ë¥¼ ìƒì„± í•´ì„œ ë°˜í™˜í•œë‹¤.\n",
    "        Args:\n",
    "            text (str) - token id ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•  ëŒ€ìƒ String.\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_sequences)\n",
    "\n",
    "    def __getitem__(self, index):   \n",
    "        # embedding ì…ë ¥ -> int 64\n",
    "        # unsqueeze(1) - [1, 2, 3, 4] -> [[1], [2], [3], [4]] # ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ \n",
    "        q = torch.tensor(self.question_sequences[index], dtype=torch.int64).[ğŸ’¡  ] # ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ \n",
    "        a = torch.tensor(self.answer_sequences[index], dtype=torch.int64).[ğŸ’¡   ] # ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ \n",
    "        return q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset ê°ì²´ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277360c-705b-42b1-8ea2-e9f9aaca4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "MIN_LENGTH = 2\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# ëª¨ë¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdd1b5",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "GRUë¥¼ í†µí•´ ë¬¸ë§¥ ì •ë³´ë¥¼ ì••ì¶•í•´ì£¼ëŠ” ì—­í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_vocabs: int - ì´ ì–´íœ˜ìˆ˜ \n",
    "            hidden_size: int - GRUì˜ hidden size\n",
    "            embedding_dim: int - Embedding vectorì˜ ì°¨ì›ìˆ˜ \n",
    "            num_layers: int - GRUì˜ layerìˆ˜\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # ì„ë² ë”© ë ˆì´ì–´, ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # GRU ìƒì„±, ë‹¨ì–´ ìˆœì„œì— ë”°ë¼ ë¬¸ë§¥ì„ ì´í•´ \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers\n",
    "            )\n",
    "\n",
    "\n",
    "    def [ ğŸ’¡ ]:\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸ì˜ tokení•œê°œì˜ í† í° idë¥¼ ì…ë ¥ë°›ì•„ hidden stateë¥¼ ì¶œë ¥\n",
    "        \n",
    "        Args:\n",
    "            x: í•œê°œ í† í°. shape-[1]\n",
    "            hidden: hidden state (ì´ì „ ì²˜ë¦¬ê²°ê³¼). shape: [1, 1, hidden_size]\n",
    "        Returns\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        # x shape : [batch: 1, 1]\n",
    "        embedded = self.embedding(x).unsqueeze(0) # (1: batch, embedding_dim) -> (1: batch, 1: seq_len, embedding_dim)\n",
    "        out, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        return out, hidden \n",
    "    \n",
    "    \n",
    "    def [  ğŸ’¡ ]: #ì²˜ìŒ timestepì—ì„œ ì…ë ¥í•  hidden_state.\n",
    "        \"\"\"\n",
    "        (ì™œëƒë©´ ì²¨ ìŠ¤í…ì—ëŠ” ë„£ì–´ì•¼í•  hidden state ê°€ ì—†ê¸° ë•Œë¬¸ì— ì§ì ‘ ë§Œë“¤ì–´ì„œ ë„£ì–´ì£¼ëŠ” ê²ƒ)\n",
    "        ê°’: < 0 >\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53312985",
   "metadata": {},
   "source": [
    "## Attention ì ìš© Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attentionì€ Decoder ë„¤íŠ¸ì›Œí¬ê°€ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ëŠ” ìê¸° ì¶œë ¥ì˜ ëª¨ë“  ë‹¨ê³„ì—ì„œ ì¸ì½”ë” ì¶œë ¥ ì¤‘ ì—°ê´€ìˆëŠ” ë¶€ë¶„ì— **ì§‘ì¤‘(attention)** í•  ìˆ˜ ìˆê²Œ í•œë‹¤. \n",
    "- ë‹¤ì–‘í•œ ì–´í…ì…˜ ê¸°ë²•ì¤‘ì— **Luong attention** ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [ğŸ’¡  ]\n",
    "- Decoderê°€ í˜„ì¬ timestepì˜ ë‹¨ì–´(token)ì„ ìƒì„±í•  ë•Œ Encoderì˜ output ë“¤ ì¤‘ ì–´ë–¤ ë‹¨ì–´ì— ì¢€ë” ì§‘ì¤‘í•´ì•¼ í•˜ëŠ”ì§€ ê³„ì‚°í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ê°’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [ğŸ’¡  ]\n",
    "- Decoderì—ì„œ í˜„ì¬ timestepì˜ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•  ë•Œ ì‚¬ìš©í•  Context Vector.\n",
    "    - Encoderì˜ output ë“¤ì— Attention Weightë¥¼ ê³±í•œë‹¤.\n",
    "    - Attention ValueëŠ” Decoderì—ì„œ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œ encoder outputì˜ ì–´ë–¤ ë‹¨ì–´ì— ë” ì§‘ì¤‘í•˜ê³  ëœ ì§‘ì¤‘í• ì§€ë¥¼ ê°€ì§€ëŠ” ê°’ì´ë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Extraction\n",
    "- Decoderì˜ embedding vectorì™€ Attention Value ë¥¼ í•©ì³ RNN(GRU)ì˜ ì…ë ¥ì„ ë§Œë“ ë‹¤.\n",
    "    - **ë‹¨ì–´ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì´ì „ timestepì—ì„œ ì¶”ë¡ í•œ ë‹¨ì–´ í˜„ì¬ timestepì˜ input ì™€ Encoder outputì— attentionì´ ì ìš©ëœ ê°’ ì´ ë‘˜ì„ í•©ì³ ì…ë ¥í•œë‹¤.\n",
    "    - ì´ ê°’ì„ [ğŸ’¡  ]ë¥¼ ì´ìš©í•´ RNN input_sizeì— ë§ì¶° ì¤€ë‹¤. (ì–´ë–»ê²Œ input_sizeì— ë§ì¶œì§€ë„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ Linear Layerì´ìš©)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### ë‹¨ì–´ ì˜ˆì¸¡(ìƒì„±)\n",
    "- RNNì—ì„œ ì°¾ì€ Featureë¥¼ ì´ ë‹¨ì–´ê°œìˆ˜ì˜ unitsì„ ì¶œë ¥í•˜ëŠ” Linearì— ì…ë ¥í•´ **ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì¶”ë¡ í•œë‹¤.**\n",
    "- ì¶”ë¡ í•œ ë‹¨ì–´ëŠ” ë‹¤ìŒ timestepì˜ ì…ë ¥($X_t$)ìœ¼ë¡œ RNNì˜ hiddenì€ ë‹¤ìŒ timestep ì˜ hidden state ($h_{t-1}$) ë¡œ ì…ë ¥ëœë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, dropout_p, max_length):\n",
    "        # num_vocabs : ì´ ì–´íœ˜ìˆ˜\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # attention weightë¥¼ ê³„ì‚°í•˜ëŠ” Linear\n",
    "        # ì´ì „ ë‹¨ì–´ì˜ hidden state(prev_hidden)ì™€ í˜„ì¬ ë‹¨ì–´ì˜ embedding vectorì—\n",
    "        #   ê°€ì¤‘í•©ì„ ê³„ì‚°í•´ì„œ attention weightë¥¼ ê³„ì‚°\n",
    "        # in_features: hidden_size + embedding_dim \n",
    "        # out_features: Encoderì˜ hidden_stateì˜ ê°œìˆ˜ (max_length)\n",
    "        self.attn = nn.Linear(hidden_size+embedding_dim, max_length)\n",
    "\n",
    "        # ê°€ì • : hidden_size = 200, max_length (í† í° ìˆ˜) = 20, \n",
    "        # attention value: attention- weight @ encoderì˜ hidden stateë“¤ (out)\n",
    "        # shape: 1 x 20 @ 20 x 200 = 1 x 200 \n",
    "\n",
    "        # í˜„ì¬ ë‹¨ì–´ embedding vector + attention balueë¥¼ ì…ë ¥ë°›ì•„ ê°€ì¤‘í•©ì„ ê³„ì‚°í•´ì„œ \n",
    "            # GRU(RNN)ì— ì…ë ¥í•  ì…ë ¥ê°’ì„ ê³„ì‚° \n",
    "            # in_features : embedding_dim + encoderì˜ hidden_size\n",
    "        self.attn_combine = nn.Linear(embedding_dim+hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        # ë¶„ë¥˜ê¸° \n",
    "        self.classifier = nn.Linear(hidden_size, num_vocabs)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: í˜„ì¬ timestepì˜ ì…ë ¥ í† í°(ë‹¨ì–´) id\n",
    "            hidden: ì´ì „ timestep ì²˜ë¦¬ê²°ê³¼ hidden state\n",
    "            encoder_outputs: Encoder outputë“¤. \n",
    "        Return\n",
    "            tupe: (output, hidden, attention_weight)\n",
    "                output: ë‹¨ì–´ë³„ ë‹¤ìŒ ë‹¨ì–´ì¼ í™•ë¥ .  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                atttention_weight: Encoder output ì¤‘ ì–´ëŠ ë‹¨ì–´ì— ì§‘ì¤‘í•´ì•¼í•˜ëŠ” ì§€ ê°€ì¤‘ì¹˜ê°’. shape: [1, max_length]\n",
    "        \n",
    "        í˜„ì¬ timestep ì…ë ¥ê³¼ ì´ì „ timestep ì²˜ë¦¬ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ encoder_outputì™€ ê³„ì‚°í•´ì„œ  encoder_outputì—ì„œ ì§‘ì¤‘(attention)í•´ì•¼í•  attention valueë¥¼ ê³„ì‚°í•œë‹¤.\n",
    "        attention valueì™€ í˜„ì¬ timestep ì…ë ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì¶”ë¡ (ìƒì„±) í•œë‹¤.\n",
    "        \"\"\"\n",
    "        \n",
    "        #  embedding \n",
    "        embedding = self.embedding(x).unsqueeze(0) # [1:batch] -> [1:batch, 1:seq_len]\n",
    "        embedding = self.dropout(embedding)\n",
    "\n",
    "        # attetion weight ê³„ì‚° \n",
    "        attn_in = torch.[ğŸ’¡  ]((embedding[0], hidden[0]),dim=1 ) #í˜„ì¬ ë‹¨ì–´ ì„ë² ë”©, ì´ì „ íˆë“  ì´ì–´ë¶™ì—¬ ê° ë‹¨ì–´ë³„ ì¤‘ìš”ë„ ì ìˆ˜ ìƒì„±\n",
    "        # attn_in shape : [1, embedding_dim+hidden_size]\n",
    "        attn_score = self.attn(attn_in) # logit\n",
    "        # shape : < [1, max_length]> \n",
    "        # attn_score shape: 1 x embeddign_dim+hidden_size @ embedding_dim+hidden_size x max_length\n",
    "\n",
    "        \n",
    "        attn_weight = nn.Softmax[ğŸ’¡    ](attn_score) # ì „ì²´ ë¬¸ì¥ì—ì„œ ì–´ë–¤ ë‹¨ì–´ì— ì§‘ì¤‘í• ì§€ í™•ë¥ ë¡œ ë³€í™˜\n",
    "        # ë§ˆì§€ë§‰ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ softmax ì ìš©í•´ë¼ - **ë¬¸ì¥ ì•ˆì˜ ë‹¨ì–´ ìœ„ì¹˜ë³„ ì§‘ì¤‘ ì •ë„(í™•ë¥ )**ë¥¼ êµ¬í•˜ëŠ” ë° ì“°ì…ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd602c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # attetion value ê³„ì‚° (attn_applied) \n",
    "        #  attn_weight @ encoder_hiddenstate\n",
    "        ##  1 x max_length  @  max_length x hidden_size \n",
    "\n",
    "        # torch.bmm() - batch-wise matrix multiplication(ë°°ì¹˜ë‹¨ìœ„ í–‰ë ¬ê³±) - 3ì°¨ì› í…ì„œë§Œ ë°›ìŒ! \n",
    "        ##  3ì°¨ì› ë°°ì—´ì„ ë°›ì•„ì„œ 1, 2 ì¶• ê¸°ì¤€ìœ¼ë¡œ í–‰ë ¬ê³± ê³„ì‚°. \n",
    "        ### (5, 2, 3) @ (5, 3, 5) -> 2 x 3 @ 3 x 5 5ê°œë¥¼ í–‰ë ¬ê³± => (5, 2, 5)\n",
    "        attn_value = torch.bmm(\n",
    "            attn_weight.[ğŸ’¡   ], # (1, 1, max_length) # ë°°ì¹˜ì°¨ì› ì¶”ê°€\n",
    "            encoder_outputs.[ğŸ’¡    ]>, # (1, max_length, hidden_size)\n",
    "        )\n",
    "        # attn_value ê²°ê³¼: attn_weight @ encoder_outputs (1:batch, 1, hidden_size)\n",
    "        # ê° ë‹¨ì–´ ë²¡í„°ì— ì§‘ì¤‘ì •ë„ attn_weightë¥¼ ê³±í•´ì„œ ë¬¸ì¥ì˜ ëŒ€í‘œ ì˜ë¯¸ attention valueë¥¼ ë½‘ì•„ëƒ„ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a2875",
   "metadata": {},
   "source": [
    "### torch.bmm()\n",
    "- ìš”ì•½ : attention ê°€ì¤‘ì¹˜ë¥¼ ê° encoder hiddenì— ê³±í•´ **ì¤‘ìš”í•œ ë‹¨ì–´ì— ì§‘ì¤‘í•œ ë¬¸ë§¥ ë²¡í„°(attn_value)**ë¥¼ ê³„ì‚°í•˜ëŠ” ì—°ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # attn_combine: gruì˜ input ê°’ì„ ìƒì„± \n",
    "        ## attn_value + embedding(concat) => Linear => ReLU\n",
    "\n",
    "\n",
    "        attn_combine_in = torch.concat([\n",
    "            [ ğŸ’¡   ]  , ### ì§€ê¸ˆê¹Œì§€ ì…ë ¥ ë¬¸ì¥ì„ ì••ì¶•í•œ ë¬¸ë§¥ ë²¡í„°(1 x hidden_size)\n",
    "            [ ğŸ’¡    ]       ### í˜„ì¬ ë””ì½”ë”ì— ì…ë ¥ëœ ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„° (1 x embedding_dim)\n",
    "        ], dim = 1)\n",
    "        gru_in = self.attn_combine(attn_combine_in) # í•©ì³ì§„ ë²¡í„°ë¥¼ Linear ë ˆì´ì–´ì— í†µê³¼ì‹œì¼œì„œ hidden sizeë¡œ ë³€í™˜ # ì¶œë ¥ (1, hidden_size)\n",
    "        gru_in = gru_in.<unsqueeze(0)> ## # ì—¬ê¸°ì„œ ì™œí•¨? ****\n",
    "        gru_in = nn.ReLU()(gru_in) # ë¹„ì„ í˜•ì„± ì¶”ê°€ \n",
    "\n",
    "        # gruì— ì…ë ¥í•´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì°¾ê¸° ìœ„í•œ hidden state(feature)ë¥¼ ê³„ì‚°.\n",
    "        out, hidden_state = self.gru(gru_in, hidden) # (seq, batch, hidden_size) (1, 1, hidden_size) # hidden_sizeê°€ ë§Œì•½ 200ê°œë©´, [[[0, 1, 2, 3, 4 .... 199]]]\n",
    "\n",
    "        # classification ì— out ì„ ì…ë ¥í•´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡\n",
    "        last_out = self.classifier(out[0]) # ë¶„ë¥˜ê¸°ëŠ” 2ì°¨ì› ìë£Œêµ¬ì¡°, linearí•  ë–„ ìƒê°í•˜ë©´... [0] ë©´, [[0, 1, 2, 3, 4 .... 199]] ê´„í˜¸í•˜ë‚˜ ì§€ìš´ê²ƒ. \n",
    "        # gruì—ì„œ ë¹ ì ¸ë‚˜ì˜¨ ë§ˆì§€ë§‰ ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥  \n",
    "        # last_out shape: [  [1, num_vocabs]  ] 0ë²ˆì§¸ ì¸ë±ìŠ¤ë¡œ ì¡°íšŒí•´ì•¼ ìš°ë¦¬ê°€ ì•Œê³ ì‹¶ì€ ê°’ì´ ë‚˜ì˜¨ë‹¤. \n",
    "        # decode ì—ì„œ ë¹ ì ¸ ë‚˜ì˜¨ ê°’ì„ ë‹¤ìŒ gru, ë‹¤ìŒ gruì—ì„œ ì“°ë„ë¡ 0 ì¸ë±ìŠ¤ì—ì„œ ì‚¬ì´ì¦ˆë¥¼ ë§ì¶°ì¤Œ \n",
    "\n",
    "\n",
    "        return last_out[0], hidden_state, attn_weight\n",
    "                #  last_out[0]ì˜ ê°’ì€ [0, 1, 2, 3, 4 .... 199] ê´„í˜¸í•˜ë‚˜ ë” ì§€ìš´ê²ƒ. \n",
    "                #  last_out : ì‹¤ì œ vocab ì‚¬ì´ì¦ˆë§Œí¼ì˜ í™•ë¥  ë¦¬ìŠ¤íŠ¸ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52df84",
   "metadata": {},
   "source": [
    "*** \n",
    "GRUëŠ” ì…ë ¥ shapeì„ **(seq_len, batch_size, hidden_size)**ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "í˜„ì¬ gru_inì˜ shapeì€ (1, hidden_size)ë¡œ 2Dì…ë‹ˆë‹¤.\n",
    "\n",
    "â†’ unsqueeze(0)ìœ¼ë¡œ ë§¨ ì•ì— sequence ê¸¸ì´ ì°¨ì›ì„ ì¶”ê°€í•´ì„œ (1, 1, hidden_size)ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b88850",
   "metadata": {},
   "source": [
    "###  Encoderì™€ AttentionDecoderê°€ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ dummy ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ ì§ì ‘ ìƒì„±í•˜ê³ , êµ¬ì¡°ë¥¼ ì‹œí—˜í•´ë³´ëŠ” ê³¼ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fda460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/ Decoder ë¥¼ dummy_dataë¡œ í™•ì¸\n",
    "dummy_encoder = Encoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(), # Tokenizerê°€ í•™ìŠµí•œ ì „ì²´ ë‹¨ì–´ ê°œìˆ˜\n",
    "    hidden_size=256,                       # RNNì˜ hidden state\n",
    "    embedding_dim=200,                     # ë‹¨ì–´ë¥¼ í‘œí˜„í•  embedding vectorì˜ ì°¨ì›\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "dummy_encoder = dummy_encoder.to(device)\n",
    "\n",
    "dummy_decoder = AttentionDecoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=200,\n",
    "    [ğŸ’¡   ] =0.3,                          # í›ˆë ¨ ì¤‘ ì¼ë¶€ ë…¸ë“œë¥¼ ëœë¤ìœ¼ë¡œ êº¼ì„œ ê³¼ëŒ€ì í•© ë°©ì§€\n",
    "    [ğŸ’¡   ] =20                             # ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ë¡œ attention weightì˜ í¬ê¸° ê²°ì •ì— ì‚¬ìš©ë¨\n",
    ")\n",
    "\n",
    "dummy_decoder = dummy_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0] # ì²«ë²ˆì§¸ (Q, A)\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# ì²«ë²ˆì§¸ ì§ˆë¬¸ì˜ ì²«ë²ˆì§¸ í† í°ì„ ì…ë ¥\n",
    "encoder_out, encoder_hidden = dummy_encoder([ğŸ’¡    ], dummy_encoder.init_hidden(device))\n",
    "# x[0] í† í°ì€ ? [SOS] í† í°ì´ë‹¤! \n",
    "\n",
    "encoder_out.shape, encoder_hidden.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²«ë²ˆì§¸ ì§ˆë¬¸ì˜ ì²«ë²ˆì§¸ í† í°ì„ ì…ë ¥. y[0]\n",
    "encoder_outputs = torch.randn(20, 256, device=device) # 20: seq_len, 256: hidden_size\n",
    "next_token, hidden_state, attn_weight = dummy_decoder([ğŸ’¡    ], encoder_out, encoder_outputs) # í˜„ì¬ ë‹¨ì–´(y[0])ì™€ ì´ì „ hidden stateë¥¼ ì…ë ¥ë°›ìŒ\n",
    "# ê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ í™•ë¥  ë¶„í¬(next_token)ë¡œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b9536",
   "metadata": {},
   "source": [
    "next_token        # ì˜ˆì¸¡í•œ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ (shape: [vocab_size])\n",
    "hidden_state      # ì—…ë°ì´íŠ¸ëœ hidden state (shape: [1, 1, hidden_size])\n",
    "attn_weight       # ì–´ë–¤ encoder ë‹¨ì–´ì— ì§‘ì¤‘í–ˆëŠ”ì§€ (shape: [1, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_token.shape)\n",
    "print(next_token.argmax(-1), tokenizer.id_to_token(next_token.argmax(-1).item()))\n",
    "print(hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ee446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_weight.shape)\n",
    "attn_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
